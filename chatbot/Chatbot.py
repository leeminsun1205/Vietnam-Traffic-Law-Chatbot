# app.py
import streamlit as st
import time
import config
import utils 
from model_loader import load_embedding_model, load_gemini_model, load_reranker_model
from data_loader import load_or_create_rag_components
from reranker import rerank_documents
from generation import generate_answer_with_gemini

# @st.cache_resource
def cached_load_or_create_components(_embedding_model):
    vector_db, hybrid_retriever = load_or_create_rag_components(_embedding_model)
    return vector_db, hybrid_retriever

# --- C·∫§U H√åNH TRANG STREAMLIT ---
st.set_page_config(page_title="Chatbot Lu·∫≠t GTƒêB", layout="wide", initial_sidebar_state="auto")

# --- Kh·ªüi t·∫°o Session State cho L·ªãch s·ª≠ Chat v√† C·∫•u h√¨nh ---

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ch√†o b·∫°n, t√¥i l√† chatbot Lu·∫≠t Giao th√¥ng ƒê∆∞·ªùng b·ªô. B·∫°n c·∫ßn h·ªó tr·ª£ g√¨?"}]

if "selected_gemini_model" not in st.session_state:
    st.session_state.selected_gemini_model = config.DEFAULT_GEMINI_MODEL

if "answer_mode" not in st.session_state:
    st.session_state.answer_mode = 'Ng·∫Øn g·ªçn'

if "retrieval_query_mode" not in st.session_state:
    st.session_state.retrieval_query_mode = 'M·ªü r·ªông' 

if "retrieval_method" not in st.session_state:
    st.session_state.retrieval_method = 'hybrid'

if "selected_reranker_model" not in st.session_state:
    st.session_state.selected_reranker_model = config.DEFAULT_RERANKER_MODEL

# --- Sidebar ---
with st.sidebar:
    st.title("T√πy ch·ªçn")

    st.header("M√¥ h√¨nh & Tr·∫£ l·ªùi")

    selected_model = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh Gemini:",
        options=config.AVAILABLE_GEMINI_MODELS,
        index=config.AVAILABLE_GEMINI_MODELS.index(st.session_state.selected_gemini_model),
        key="selected_gemini_model",
        help="Ch·ªçn m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn ƒë·ªÉ x·ª≠ l√Ω y√™u c·∫ßu."
    )

    answer_mode_choice = st.radio(
        "Ch·ªçn ch·∫ø ƒë·ªô tr·∫£ l·ªùi:",
        options=['Ng·∫Øn g·ªçn', 'ƒê·∫ßy ƒë·ªß'],
        key="answer_mode",
        horizontal=True,
        help="M·ª©c ƒë·ªô chi ti·∫øt c·ªßa c√¢u tr·∫£ l·ªùi."
    )

    st.header("C·∫•u h√¨nh truy v·∫•n")

    retrieval_query_mode_choice = st.radio(
        "Ngu·ªìn c√¢u h·ªèi cho Retrieval:",
        options=['ƒê∆°n gi·∫£n', 'M·ªü r·ªông', 'ƒêa d·∫°ng'],
        key="retrieval_query_mode", 
        horizontal=True,
        help=(
            "**ƒê∆°n gi·∫£n:** Ch·ªâ d√πng c√¢u h·ªèi g·ªëc.\n"
            "**M·ªü r·ªông:** Ch·ªâ d√πng c√¢u h·ªèi m·ªü r·ªông t·ª´ c√¢u h·ªèi g·ªëc (do AI t·∫°o).\n"
            "**ƒêa d·∫°ng:** D√πng c·∫£ c√¢u h·ªèi g·ªëc v√† c√°c bi·∫øn th·ªÉ t·ª´ c√¢u h·ªèi g·ªëc(do AI t·∫°o)."
        )
    )

    retrieval_method_choice = st.radio(
        "Ph∆∞∆°ng th·ª©c Retrieval:",
        options=['dense', 'sparse', 'hybrid'],
        index=['dense', 'sparse', 'hybrid'].index(st.session_state.retrieval_method), 
        key="retrieval_method",
        horizontal=True,
        help=(
            "**dense:** T√¨m ki·∫øm d·ª±a tr√™n vector ng·ªØ nghƒ©a (nhanh, hi·ªÉu ng·ªØ c·∫£nh).\n"
            "**sparse:** T√¨m ki·∫øm d·ª±a tr√™n t·ª´ kh√≥a (BM25) (nhanh, ch√≠nh x√°c t·ª´ kh√≥a).\n"
            "**hybrid:** K·∫øt h·ª£p c·∫£ dense v√† sparse (c√¢n b·∫±ng, c√≥ th·ªÉ t·ªët nh·∫•t)."
        )
    )

    selected_reranker = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh Reranker:",
        options=config.AVAILABLE_RERANKER_MODELS,
        index=config.AVAILABLE_RERANKER_MODELS.index(st.session_state.selected_reranker_model),
        key="selected_reranker_model",
        help="Ch·ªçn m√¥ h√¨nh ƒë·ªÉ x·∫øp h·∫°ng l·∫°i k·∫øt qu·∫£ t√¨m ki·∫øm. Ch·ªçn 'Kh√¥ng s·ª≠ d·ª•ng' ƒë·ªÉ t·∫Øt."
    )

    st.markdown("---") 

    st.header("Qu·∫£n l√Ω H·ªôi tho·∫°i")
    if st.button("‚ö†Ô∏è X√≥a L·ªãch S·ª≠ Chat"):
        st.session_state.messages = []
        st.success("ƒê√£ x√≥a l·ªãch s·ª≠ chat!")
        time.sleep(1)
        st.rerun()
    st.markdown("---")

# --- Giao di·ªán ch√≠nh c·ªßa ·ª®ng d·ª•ng ---
st.title("‚öñÔ∏è Chatbot H·ªèi ƒê√°p Lu·∫≠t Giao Th√¥ng ƒê∆∞·ªùng B·ªô VN")
st.caption(f"D·ª±a tr√™n c√°c vƒÉn b·∫£n Lu·∫≠t, Ngh·ªã ƒê·ªãnh, Th√¥ng t∆∞ v·ªÅ Lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô Vi·ªát Nam.")

# --- C·∫≠p nh·∫≠t Caption hi·ªÉn th·ªã c·∫•u h√¨nh ---
reranker_status_display = st.session_state.selected_reranker_model
if reranker_status_display == 'Kh√¥ng s·ª≠ d·ª•ng':
    reranker_status_display = "T·∫Øt"
else:
    # L·∫•y t√™n model ng·∫Øn g·ªçn h∆°n ƒë·ªÉ hi·ªÉn th·ªã n·∫øu c·∫ßn
    reranker_status_display = reranker_status_display.split('/')[-1]
st.caption(f"M√¥ h√¨nh: `{st.session_state.selected_gemini_model}` | Tr·∫£ l·ªùi: `{st.session_state.answer_mode}` | Ngu·ªìn Query: `{st.session_state.retrieval_query_mode}` | Retrieval: `{st.session_state.retrieval_method}` | Reranker: `{reranker_status_display}`")

# --- Hi·ªÉn th·ªã L·ªãch s·ª≠ Chat ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        content_to_display = message["content"]
        if message["role"] == "assistant":
            # L·∫•y relevant_docs t·ª´ message, n·∫øu kh√¥ng c√≥ th√¨ d√πng list r·ªóng
            docs_for_this_message = message.get("relevant_docs_for_display", [])
            content_to_display = utils.render_html_for_assistant_message(content_to_display, docs_for_this_message)
        st.markdown(content_to_display, unsafe_allow_html=True)

# --- Kh·ªüi t·∫°o h·ªá th·ªëng ---
init_ok = False
reranking_model_loaded = None

with st.status("ƒêang kh·ªüi t·∫°o h·ªá th·ªëng...", expanded=True) as status:
    embedding_model = load_embedding_model(config.embedding_model_name)
    if st.session_state.selected_reranker_model != 'Kh√¥ng s·ª≠ d·ª•ng':
        reranking_model_loaded = load_reranker_model(st.session_state.selected_reranker_model)
    else:
        reranking_model_loaded = None
    models_loaded = all([embedding_model])
    vector_db, hybrid_retriever = cached_load_or_create_components(embedding_model)
    retriever_ready = hybrid_retriever is not None

    status_label = "‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!"
    status_state = "complete"
    init_ok = True

    if not models_loaded: 
        status_label = "‚ö†Ô∏è L·ªói t·∫£i Embedding model!"
        status_state = "error"
        init_ok = False
    elif not retriever_ready:
        status_label = "‚ö†Ô∏è L·ªói kh·ªüi t·∫°o VectorDB ho·∫∑c Retriever!"
        status_state = "error"
        init_ok = False
    elif st.session_state.selected_reranker_model != 'Kh√¥ng s·ª≠ d·ª•ng' and not reranking_model_loaded:
        status_label = f"‚ö†Ô∏è L·ªói t·∫£i Reranker model ({st.session_state.selected_reranker_model}). Reranking s·∫Ω b·ªã t·∫Øt."
        status_state = "warning" # C√≥ th·ªÉ coi l√† warning, h·ªá th·ªëng v·∫´n ch·∫°y ƒë∆∞·ª£c nh∆∞ng kh√¥ng c√≥ rerank
        # init_ok v·∫´n c√≥ th·ªÉ l√† True, nh∆∞ng reranking_model_loaded s·∫Ω l√† None
        init_ok = False

    if init_ok:
        status.update(label=status_label, state=status_state, expanded=False)
    else:
        status.update(label=status_label, state=status_state, expanded=True)

# --- Input v√† X·ª≠ l√Ω ---
if init_ok:
    if user_query := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ Lu·∫≠t GTƒêB..."):
        # 1. Th√™m v√† hi·ªÉn th·ªã tin nh·∫Øn c·ªßa ng∆∞·ªùi d√πng
        st.session_state.messages.append({"role": "user", "content": user_query})
        with st.chat_message("user"):
            st.markdown(user_query)

        # 2. X·ª≠ l√Ω v√† t·∫°o ph·∫£n h·ªìi t·ª´ bot
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            raw_llm_output = ""
            processing_log = []
            final_relevant_documents = [] 
            relevance_status = 'valid'

            try:
                start_time = time.time()
                processing_log.append(f"[{time.time() - start_time:.2f}s] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
                message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                # --- T·∫£i model Gemini ƒë√£ ch·ªçn ---
                selected_model_name = st.session_state.selected_gemini_model
                selected_gemini_llm = load_gemini_model(selected_model_name)
                if not selected_gemini_llm:
                     raise ValueError(f"Kh√¥ng th·ªÉ t·∫£i model Gemini: {selected_model_name}")
                processing_log.append(f"[{time.time() - start_time:.2f}s]: Model '{selected_model_name}' ƒë√£ s·∫µn s√†ng.")
                message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                # L·∫•y reranker model ƒë√£ t·∫£i (c√≥ th·ªÉ l√† None)
                current_reranker_model = reranking_model_loaded
                use_reranker_flag = current_reranker_model is not None and st.session_state.selected_reranker_model != 'Kh√¥ng s·ª≠ d·ª•ng'

                if use_reranker_flag:
                     processing_log.append(f"[{time.time() - start_time:.2f}s]: Reranker model '{st.session_state.selected_reranker_model}' ƒë√£ s·∫µn s√†ng.")
                else:
                     processing_log.append(f"[{time.time() - start_time:.2f}s]: Reranker kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng ho·∫∑c kh√¥ng t·∫£i ƒë∆∞·ª£c.")
                message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                history_for_llm1 = st.session_state.messages[-(config.MAX_HISTORY_TURNS * 2):-1]
                log_hist_llm1 = "(c√≥ d√πng l·ªãch s·ª≠)"
                processing_log.append(f"[{time.time() - start_time:.2f}s] Ph√¢n t√≠ch c√¢u h·ªèi {log_hist_llm1}...")
                message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                # --- B∆∞·ªõc A: Ph√¢n lo·∫°i relevancy v√† t·∫°o bi·∫øn th·ªÉ/t√≥m t·∫Øt ---
                relevance_status, direct_answer, all_queries, summarizing_q = utils.generate_query_variations(
                    original_query=user_query,
                    gemini_model=selected_gemini_llm,
                    chat_history=history_for_llm1,
                    num_variations=config.NUM_QUERY_VARIATIONS
                )
                st.write(summarizing_q)
                # --- Ki·ªÉm tra m·ª©c ƒë·ªô li√™n quan ---
                if relevance_status == 'invalid':
                    if direct_answer and direct_answer.strip():
                        full_response = direct_answer
                    else:
                        full_response = "‚ö†Ô∏è C√¢u h·ªèi c·ªßa b·∫°n c√≥ v·∫ª kh√¥ng li√™n quan ƒë·∫øn Lu·∫≠t Giao th√¥ng ƒê∆∞·ªùng b·ªô Vi·ªát Nam."
                    processing_log.append(f"[{time.time() - start_time:.2f}s] Ho√†n t·∫•t (C√¢u h·ªèi kh√¥ng li√™n quan).")

                # --- N·∫øu c√¢u h·ªèi h·ª£p l·ªá, ti·∫øp t·ª•c x·ª≠ l√Ω RAG ---
                else:
                    # --- L·∫•y l·ªãch s·ª≠ g·∫ßn ƒë√¢y cho LLM th·ª© 2 (t·∫°o c√¢u tr·∫£ l·ªùi) ---
                    recent_chat_history = st.session_state.messages[-(config.MAX_HISTORY_TURNS * 2):-1]

                    # --- X√°c ƒë·ªãnh query(s) ƒë·ªÉ t√¨m ki·∫øm d·ª±a tr√™n retrieval_query_mode ---
                    queries_to_search = []
                    query_source_log = ""
                    retrieval_query_mode = st.session_state.retrieval_query_mode
                    if retrieval_query_mode == 'ƒê∆°n gi·∫£n':
                        queries_to_search = [user_query]
                        query_source_log = "c√¢u h·ªèi g·ªëc"
                    elif retrieval_query_mode == 'M·ªü r·ªông':
                        queries_to_search = [summarizing_q]
                        query_source_log = "c√¢u h·ªèi t√≥m t·∫Øt"
                    elif retrieval_query_mode == 'ƒêa d·∫°ng':
                        queries_to_search = all_queries # all_queries ƒë√£ bao g·ªìm user_query
                        query_source_log = f"c√¢u h·ªèi g·ªëc v√† {len(all_queries)-1} bi·∫øn th·ªÉ"

                    # --- L·∫•y c·∫•u h√¨nh retrieval v√† rerank ---
                    retrieval_method = st.session_state.retrieval_method

                    processing_log.append(f"[{time.time() - start_time:.2f}s]: B·∫Øt ƒë·∫ßu Retrieval (Ngu·ªìn: {query_source_log}, Ph∆∞∆°ng th·ª©c: {retrieval_method})...")
                    message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                    # --- Th·ª±c hi·ªán Retrieval ---
                    collected_docs_data = {} # Dict l∆∞u k·∫øt qu·∫£ {index: {'doc': ..., 'score': ...}}
                    retrieval_start_time = time.time()
        
                    for q_idx, current_query in enumerate(queries_to_search):
                        # G·ªçi ph∆∞∆°ng th·ª©c search m·ªõi c·ªßa retriever
                        search_results = hybrid_retriever.search(
                            current_query,
                            embedding_model,
                            method=retrieval_method,
                            k=config.VECTOR_K_PER_QUERY # L·∫•y nhi·ªÅu h∆°n ƒë·ªÉ c√≥ ƒë·ªß cho rerank/fusion
                        )
                        # T·ªïng h·ª£p k·∫øt qu·∫£, tr√°nh tr√πng l·∫∑p index
                        for item in search_results:
                            doc_index = item['index']
                            if doc_index not in collected_docs_data:
                                collected_docs_data[doc_index] = item # L∆∞u c·∫£ score t·ª´ retrieval
                            # Optional: N·∫øu mu·ªën c·∫≠p nh·∫≠t score (v√≠ d·ª•: l·∫•y score cao nh·∫•t n·∫øu tr√πng) - ph·ª©c t·∫°p h∆°n
                    retrieval_time = time.time() - retrieval_start_time
                    num_unique_docs = len(collected_docs_data)
                    processing_log.append(f"[{time.time() - start_time:.2f}s]: Retrieval ({retrieval_time:.2f}s) t√¨m th·∫•y {num_unique_docs} t√†i li·ªáu ·ª©ng vi√™n.")
                    message_placeholder.markdown(" ".join(processing_log) + "‚è≥")


                    # --- Chu·∫©n b·ªã d·ªØ li·ªáu cho b∆∞·ªõc ti·∫øp theo (Rerank ho·∫∑c l·∫•y tr·ª±c ti·∫øp) ---
                    # Chuy·ªÉn dict th√†nh list v√† s·∫Øp x·∫øp theo score (cao xu·ªëng th·∫•p cho sparse/hybrid, th·∫•p l√™n cao cho dense)
                    retrieved_docs_list = list(collected_docs_data.values())
                    sort_reverse = (retrieval_method != 'dense') # Dense s·∫Øp x·∫øp ng∆∞·ª£c l·∫°i
                    retrieved_docs_list.sort(key=lambda x: x.get('score', 0 if sort_reverse else float('inf')), reverse=sort_reverse)

                    # --- B∆∞·ªõc Rerank (N·∫øu ƒë∆∞·ª£c b·∫≠t) ---
                    final_relevant_documents = [] # List c√°c dict {'doc': ..., 'score': ..., 'original_index': ...}
                    rerank_time = 0.0
                    rerank_start_time = time.time()

                    if use_reranker_flag and num_unique_docs > 0:
                        # L·∫•y query ph√π h·ª£p ƒë·ªÉ rerank (th∆∞·ªùng l√† c√¢u t√≥m t·∫Øt ho·∫∑c c√¢u g·ªëc)
                        query_for_reranking = summarizing_q if summarizing_q else user_query
                        processing_log.append(f"[{time.time() - start_time:.2f}s]: X·∫øp h·∫°ng l·∫°i {min(num_unique_docs, config.MAX_DOCS_FOR_RERANK)} t√†i li·ªáu b·∫±ng '{st.session_state.selected_reranker_model}'...")
                        message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                        # Ch·ªçn top N docs ƒë·ªÉ rerank
                        docs_to_rerank = retrieved_docs_list[:config.MAX_DOCS_FOR_RERANK]
                        rerank_input = [{'doc': item['doc'], 'index': item['index']} for item in docs_to_rerank]
                        reranked_results = rerank_documents(
                            query_for_reranking,
                            rerank_input, # ƒê·∫£m b·∫£o ƒë√∫ng ƒë·ªãnh d·∫°ng ƒë·∫ßu v√†o
                            current_reranker_model
                        )
                        # L·∫•y top K k·∫øt qu·∫£ cu·ªëi c√πng sau rerank
                        final_relevant_documents = reranked_results[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
                        rerank_time = time.time() - rerank_start_time
                        processing_log.append(f"[{time.time() - start_time:.2f}s]: Rerank ({rerank_time:.2f}s) ho√†n t·∫•t, ch·ªçn top {len(final_relevant_documents)}.")
                        message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                    elif num_unique_docs > 0: # Kh√¥ng d√πng reranker nh∆∞ng c√≥ k·∫øt qu·∫£ retrieval
                        processing_log.append(f"[{time.time() - start_time:.2f}s]: B·ªè qua Rerank, l·∫•y tr·ª±c ti·∫øp top {config.FINAL_NUM_RESULTS_AFTER_RERANK} k·∫øt qu·∫£ Retrieval.")
                        final_relevant_documents = retrieved_docs_list[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
                        message_placeholder.markdown(" ".join(processing_log) + "‚è≥")
                    else: # Kh√¥ng c√≥ k·∫øt qu·∫£ retrieval
                         processing_log.append(f"[{time.time() - start_time:.2f}s]: Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan.")
                         message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                    # --- B∆∞·ªõc Generate Answer ---
                    answer_mode = st.session_state.answer_mode
                    processing_log.append(f"[{time.time() - start_time:.2f}s]: T·ªïng h·ª£p c√¢u tr·∫£ l·ªùi (ch·∫ø ƒë·ªô: {answer_mode})...")
                    message_placeholder.markdown(" ".join(processing_log))
                    raw_llm_output = generate_answer_with_gemini(
                        query_text=user_query,
                        relevant_documents=final_relevant_documents, 
                        gemini_model=selected_gemini_llm,
                        mode=answer_mode,
                        chat_history=recent_chat_history
                    )
                    processing_log.append(f"[{time.time() - start_time:.2f}s]: Ho√†n t·∫•t!")

                # Hi·ªÉn th·ªã log x·ª≠ l√Ω
                with st.expander("Xem chi ti·∫øt qu√° tr√¨nh x·ª≠ l√Ω", expanded=False):
                    log_content = "\n".join(processing_log)
                    st.markdown(f"```text\n{log_content}\n```")
                # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi cu·ªëi c√πng
                if raw_llm_output:
                    content_for_immediate_display = utils.render_html_for_assistant_message(raw_llm_output, final_relevant_documents)
                    message_placeholder.markdown(content_for_immediate_display, unsafe_allow_html=True)
                    full_response = raw_llm_output
                else:
                    message_placeholder.markdown(full_response, unsafe_allow_html=True)

            except Exception as e:
                st.error(f"üêû ƒê√£ x·∫£y ra l·ªói: {e}") 
                full_response = f"üêû Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh x·ª≠ l√Ω. Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c thay ƒë·ªïi c·∫•u h√¨nh."
                if message_placeholder:
                    message_placeholder.markdown(full_response, unsafe_allow_html=True)
                else:
                    st.markdown(full_response) 
            finally:
                if user_query and full_response:
                    utils.log_qa_to_json(user_query, full_response)
                # ƒê·∫£m b·∫£o tin nh·∫Øn c·ªßa assistant lu√¥n ƒë∆∞·ª£c th√™m v√†o history
                if full_response: 
                    assistant_message = {"role": "assistant", "content": full_response}
                    if relevance_status != 'invalid' and 'final_relevant_documents' in locals() and final_relevant_documents:
                        assistant_message["relevant_docs_for_display"] = final_relevant_documents
                    elif relevance_status == 'invalid': 
                        assistant_message["relevant_docs_for_display"] = [] 
                    else: 
                        assistant_message["relevant_docs_for_display"] = []

                    st.session_state.messages.append(assistant_message)

elif not init_ok:
    st.error("‚ö†Ô∏è H·ªá th·ªëng ch∆∞a th·ªÉ kh·ªüi ƒë·ªông do l·ªói t·∫£i m√¥ h√¨nh ho·∫∑c d·ªØ li·ªáu. Vui l√≤ng ki·ªÉm tra l·∫°i.")