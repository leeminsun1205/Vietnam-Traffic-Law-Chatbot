# app.py (Chatbot.py)
import streamlit as st
import time
import config 
import utils 
import traceback
from model_loader import load_gemini_model, initialize_app_resources
from reranker import rerank_documents 
from generation import generate_answer_with_gemini 

# --- Trang Streamlit cho Chatbot ---
st.set_page_config(page_title="Chatbot Lu·∫≠t GTƒêB", layout="wide", initial_sidebar_state="auto")

# --- Kh·ªüi t·∫°o session state ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ch√†o b·∫°n, t√¥i l√† chatbot Lu·∫≠t Giao th√¥ng ƒê∆∞·ªùng b·ªô. B·∫°n c·∫ßn h·ªó tr·ª£ g√¨?"}]

# Kh·ªüi t·∫°o session state cho m√¥ h√¨nh 
if "selected_emb_name" not in st.session_state:
    st.session_state.selected_emb_name = config.DEFAULT_EMBEDDING_MODEL 
if "selected_secondary_emb_name" not in st.session_state:
    secondary_default = None
    for model_name in config.AVAILABLE_EMBEDDING_MODELS:
        if model_name != config.DEFAULT_EMBEDDING_MODEL:
            secondary_default = model_name
            break
    st.session_state.selected_secondary_emb_name = secondary_default if secondary_default else (config.AVAILABLE_EMBEDDING_MODELS[1] if len(config.AVAILABLE_EMBEDDING_MODELS) > 1 else config.DEFAULT_EMBEDDING_MODEL)
if "hybrid_component_mode" not in st.session_state:
    st.session_state.hybrid_component_mode = "2 Dense + 1 Sparse"
if "selected_gem_name" not in st.session_state:
    st.session_state.selected_gem_name = config.DEFAULT_GEMINI_MODEL 
if "selected_reranker_name" not in st.session_state:
    st.session_state.selected_reranker_name = config.DEFAULT_RERANKER_MODEL 

# Kh·ªüi t·∫°o session state cho ch·∫ø ƒë·ªô
if "answer_mode" not in st.session_state: 
    st.session_state.answer_mode = 'Ng·∫Øn g·ªçn'
if "retrieval_query_mode" not in st.session_state: 
    st.session_state.retrieval_query_mode = 'M·ªü r·ªông'
if "retrieval_method" not in st.session_state: 
    st.session_state.retrieval_method = 'K·∫øt h·ª£p'

# T·∫£i tr∆∞·ªõc TO√ÄN B·ªò models v√† RAG components 
if "app_loaded_embedding_models" not in st.session_state:
    st.session_state.app_loaded_embedding_models = {}
if "app_loaded_reranker_models" not in st.session_state:
    st.session_state.app_loaded_reranker_models = {}
if "app_rag_components_per_embedding_model" not in st.session_state:
    st.session_state.app_rag_components_per_embedding_model = {}

# --- Sidebar cho trang Chatbot---
with st.sidebar:
    st.title("T√πy ch·ªçn C·∫•u h√¨nh")

    current_emb_name_sb = st.session_state.selected_emb_name
    current_secondary_emb_name_sb = st.session_state.selected_secondary_emb_name
    current_hybrid_component_mode = st.session_state.hybrid_component_mode
    current_gem_name_sb = st.session_state.selected_gem_name
    current_reranker_name_sb = st.session_state.selected_reranker_name
    current_answer_mode = st.session_state.answer_mode
    current_retrieval_query_mode = st.session_state.retrieval_query_mode
    current_retrieval_method = st.session_state.retrieval_method

    # Mode radio
    answer_mode_choice = st.radio(
        "Ch·ªçn ch·∫ø ƒë·ªô tr·∫£ l·ªùi:", 
        options=['Ng·∫Øn g·ªçn', 'ƒê·∫ßy ƒë·ªß'],
        key="answer_mode", 
        index=['Ng·∫Øn g·ªçn', 'ƒê·∫ßy ƒë·ªß'].index(current_answer_mode),
        horizontal=True, 
        help="M·ª©c ƒë·ªô chi ti·∫øt c·ªßa c√¢u tr·∫£ l·ªùi."
    )

    st.header("C·∫•u h√¨nh truy v·∫•n")

    retrieval_query_mode_choice = st.radio(
        "Ngu·ªìn c√¢u h·ªèi cho truy v·∫•n:", 
        options=['ƒê∆°n gi·∫£n', 'M·ªü r·ªông', 'ƒêa d·∫°ng'],
        key="retrieval_query_mode",
        index = ['ƒê∆°n gi·∫£n', 'M·ªü r·ªông', 'ƒêa d·∫°ng'].index(current_retrieval_query_mode), 
        horizontal=True, 
        help=(
            "**ƒê∆°n gi·∫£n:** Ch·ªâ d√πng c√¢u h·ªèi g·ªëc.\n"
            "**M·ªü r·ªông:** Ch·ªâ d√πng c√¢u h·ªèi m·ªü r·ªông t·ª´ c√¢u h·ªèi g·ªëc (do AI t·∫°o).\n"
            "**ƒêa d·∫°ng:** D√πng c·∫£ c√¢u h·ªèi g·ªëc v√† c√°c bi·∫øn th·ªÉ t·ª´ c√¢u h·ªèi g·ªëc(do AI t·∫°o)."
        )
    )

    retrieval_method_choice = st.radio(
        "Ph∆∞∆°ng th·ª©c truy v·∫•n:", 
        options=['Ng·ªØ nghƒ©a', 'T·ª´ kh√≥a', 'K·∫øt h·ª£p'],
        key="retrieval_method",
        index=['Ng·ªØ nghƒ©a', 'T·ª´ kh√≥a', 'K·∫øt h·ª£p'].index(current_retrieval_method), 
        horizontal=True, 
        help=(
            "**Dense:** T√¨m ki·∫øm d·ª±a tr√™n vector ng·ªØ nghƒ©a (nhanh, hi·ªÉu ng·ªØ c·∫£nh).\n"
            "**Sparse:** T√¨m ki·∫øm d·ª±a tr√™n t·ª´ kh√≥a (BM25) (nhanh, ch√≠nh x√°c t·ª´ kh√≥a).\n"
            "**Hybrid:** K·∫øt h·ª£p c·∫£ Dense v√† Sparse (c√¢n b·∫±ng, c√≥ th·ªÉ t·ªët nh·∫•t)."
        )
    )

    if current_retrieval_method == 'K·∫øt h·ª£p':
        hybrid_component_mode_choice = st.radio(
            "C·∫•u h√¨nh th√†nh ph·∫ßn Hybrid:",
            options=["1 Dense + 1 Sparse", "2 Dense + 1 Sparse"],
            key="hybrid_component_mode",
            index=["1 Dense + 1 Sparse", "2 Dense + 1 Sparse"].index(current_hybrid_component_mode),
            horizontal=True,
            help="Ch·ªçn s·ªë l∆∞·ª£ng Dense encoders s·ª≠ d·ª•ng trong ph∆∞∆°ng th·ª©c K·∫øt h·ª£p."
        )

    st.header("M√¥ h√¨nh")

    # Model selectbox
    avail_emb_names = list(st.session_state.get("app_loaded_embedding_models", {}).keys())
    if not avail_emb_names:
        avail_emb_names = config.AVAILABLE_EMBEDDING_MODELS 
    # Selectbox cho Embedding Model
    selected_emb_name_ui = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh Embedding:",
        options=avail_emb_names,
        key = "selected_emb_name",
        index=avail_emb_names.index(current_emb_name_sb)
            if current_emb_name_sb in avail_emb_names else 0,
        help="Ch·ªçn m√¥ h√¨nh ƒë·ªÉ vector h√≥a t√†i li·ªáu v√† c√¢u h·ªèi."
    )
    
    if current_retrieval_method == 'K·∫øt h·ª£p' and st.session_state.hybrid_component_mode == "2 Dense + 1 Sparse": 
        options_for_secondary = [
            name for name in avail_emb_names 
            if name != st.session_state.selected_emb_name
        ]

        current_secondary_val = st.session_state.selected_secondary_emb_name
        
        if not options_for_secondary: 
            st.warning("C·∫ßn √≠t nh·∫•t 2 embedding models kh√°c nhau ƒë·ªÉ s·ª≠ d·ª•ng ch·∫ø ƒë·ªô Hybrid 2-Dense.")
            st.session_state.selected_secondary_emb_name = None
        elif current_secondary_val == st.session_state.selected_emb_name or current_secondary_val not in options_for_secondary:
            st.session_state.selected_secondary_emb_name = options_for_secondary[0]
            current_secondary_val = options_for_secondary[0]

        idx_secondary = 0
        if current_secondary_val and options_for_secondary:
            try:
                idx_secondary = options_for_secondary.index(current_secondary_val)
            except ValueError: 
                st.session_state.selected_secondary_emb_name = options_for_secondary[0]
                idx_secondary = 0
        elif not options_for_secondary:
             st.session_state.selected_secondary_emb_name = None 

        if options_for_secondary: 
            selected_secondary_emb_name_ui = st.selectbox(
                "Ch·ªçn m√¥ h√¨nh Embedding Ph·ª• (cho Hybrid 2-Dense):",
                options=options_for_secondary,
                key="selected_secondary_emb_name",
                index=idx_secondary,
                help="Ch·ªçn m√¥ h√¨nh embedding th·ª© hai. Danh s√°ch n√†y ƒë√£ lo·∫°i tr·ª´ m√¥ h√¨nh Embedding Ch√≠nh."
            )

    # Selectbox cho Gemini Model
    selected_gem_name_ui = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh Gemini:",
        options=config.AVAILABLE_GEMINI_MODELS, 
        key = "selected_gem_name",
        index=config.AVAILABLE_GEMINI_MODELS.index(current_gem_name_sb) 
            if current_gem_name_sb in config.AVAILABLE_GEMINI_MODELS else 0, 
        help="Ch·ªçn m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn ƒë·ªÉ x·ª≠ l√Ω y√™u c·∫ßu."
    )

    avail_reranker_names = list(st.session_state.get("app_loaded_reranker_models", {}).keys())
    if not avail_reranker_names: 
        avail_reranker_names = config.AVAILABLE_RERANKER_MODELS 
    # Selectbox cho Reranker Model
    selected_reranker_name_ui = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh Reranker:",
        options=avail_reranker_names,
        key = "selected_reranker_name",
        index=avail_reranker_names.index(current_reranker_name_sb)
            if current_reranker_name_sb in avail_reranker_names else 0,
        help="Ch·ªçn m√¥ h√¨nh ƒë·ªÉ x·∫øp h·∫°ng l·∫°i k·∫øt qu·∫£ t√¨m ki·∫øm. 'Kh√¥ng s·ª≠ d·ª•ng' ƒë·ªÉ t·∫Øt."
    )

    st.markdown("---")
    st.header("Qu·∫£n l√Ω H·ªôi tho·∫°i")
    if st.button("‚ö†Ô∏è X√≥a L·ªãch S·ª≠ Chat"):
        st.session_state.messages = [{"role": "assistant", "content": "Ch√†o b·∫°n, t√¥i l√† chatbot Lu·∫≠t Giao th√¥ng ƒê∆∞·ªùng b·ªô. B·∫°n c·∫ßn h·ªó tr·ª£ g√¨?"}]
        st.success("ƒê√£ x√≥a l·ªãch s·ª≠ chat!")
        time.sleep(1)
        st.rerun()
    st.markdown("---")

# --- Giao di·ªán ch√≠nh c·ªßa ·ª®ng d·ª•ng ---
st.title("‚öñÔ∏è Chatbot H·ªèi ƒê√°p Lu·∫≠t Giao Th√¥ng ƒê∆∞·ªùng B·ªô VN")
st.caption(f"D·ª±a tr√™n c√°c vƒÉn b·∫£n Lu·∫≠t, Ngh·ªã ƒê·ªãnh, Th√¥ng t∆∞ v·ªÅ Lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô Vi·ªát Nam.")

# --- Kh·ªüi t·∫°o t√†i nguy√™n cho trang Chatbot ---
app_init_status_placeholder = st.empty() 
if "app_resources_initialized" not in st.session_state:
    st.session_state.app_resources_initialized = False

if not st.session_state.app_resources_initialized:
    with st.spinner("ƒêang kh·ªüi t·∫°o to√†n b·ªô h·ªá th·ªëng v√† c√°c m√¥ h√¨nh... Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t."):
        system_fully_ready = initialize_app_resources()
        st.session_state.app_resources_initialized = system_fully_ready 

# Ki·ªÉm tra sau khi ƒë√£ kh·ªüi t·∫°o
if st.session_state.app_resources_initialized:
    app_init_status_placeholder.success("‚úÖ H·ªá th·ªëng v√† t·∫•t c·∫£ m√¥ h√¨nh ƒë√£ s·∫µn s√†ng!")

    current_selected_emb_name = st.session_state.selected_emb_name
    current_selected_reranker_name = st.session_state.selected_reranker_name
    current_selected_gem_name = st.session_state.selected_gem_name

    active_emb_obj = st.session_state.app_loaded_embedding_models.get(current_selected_emb_name)
    active_rag_comps = st.session_state.app_rag_components_per_embedding_model.get(current_selected_emb_name)
    active_retriever = active_rag_comps[1] if active_rag_comps else None
    active_reranker_obj = st.session_state.app_loaded_reranker_models.get(current_selected_reranker_name)
    active_gem_obj = load_gemini_model(current_selected_gem_name)

    # --- Ki·ªÉm tra l·∫°i c√°c active components ---
    proceed_with_chat = True
    if not active_emb_obj:
        st.error(f"L·ªói nghi√™m tr·ªçng: Kh√¥ng t√¨m th·∫•y Embedding model '{current_selected_emb_name.split('/')[-1]}' ƒë√£ t·∫£i.")
        proceed_with_chat = False
    if not active_retriever:
        st.error(f"L·ªói nghi√™m tr·ªçng: Kh√¥ng t√¨m th·∫•y Retriever cho '{current_selected_emb_name.split('/')[-1]}'.")
        proceed_with_chat = False
    if not active_gem_obj:
        st.error(f"L·ªói nghi√™m tr·ªçng: Kh√¥ng t·∫£i ƒë∆∞·ª£c Gemini model '{current_selected_gem_name}'.")
        proceed_with_chat = False

    # --- Input v√† X·ª≠ l√Ω ---
    if proceed_with_chat:
        # --- C·∫≠p nh·∫≠t Caption hi·ªÉn th·ªã c·∫•u h√¨nh ---

        caption_text = (
            f"Embedding Ch√≠nh: `{current_selected_emb_name.split('/')[-1]}` | "
            f"M√¥ h√¨nh: `{current_selected_gem_name}` | "
            f"Tr·∫£ l·ªùi: `{current_answer_mode}` | "
            f"Ngu·ªìn c√¢u h·ªèi: `{current_retrieval_query_mode}` | "
            f"Lo·∫°i truy v·∫•n: `{current_retrieval_method}` | "
            f"Reranker: `{current_selected_reranker_name.split('/')[-1] if current_selected_reranker_name != 'Kh√¥ng s·ª≠ d·ª•ng' else 'T·∫Øt'}`"
        )
        if st.session_state.retrieval_method == 'K·∫øt h·ª£p':
            caption_text += f" | C·∫•u h√¨nh Hybrid: `{st.session_state.hybrid_component_mode}`"
            if st.session_state.hybrid_component_mode == "2 Dense + 1 Sparse" and st.session_state.selected_secondary_emb_name:
                caption_text += f" | Embedding Ph·ª•: `{st.session_state.selected_secondary_emb_name.split('/')[-1]}`"
        st.caption(caption_text)

        # --- Hi·ªÉn th·ªã L·ªãch s·ª≠ Chat ---
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                content_to_display = message["content"]
                if message["role"] == "assistant":
                    docs_for_this_message = message.get("relevant_docs_for_display", [])
                    content_to_display = utils.render_html_for_assistant_message(content_to_display, docs_for_this_message) #
                st.markdown(content_to_display, unsafe_allow_html=True)

        if user_query := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ Lu·∫≠t GTƒêB..."):
            st.session_state.messages.append({"role": "user", "content": user_query})
            with st.chat_message("user"):
                st.markdown(user_query)
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                raw_llm_output = ""
                processing_log = []
                final_relevant_documents_for_display_main = []
                relevance_status = 'valid'
                try:
                    start_time = time.time()
                    processing_log.append(f"[{time.time() - start_time:.2f}s] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
                    message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                    use_reranker_flag_main = active_reranker_obj is not None and current_selected_reranker_name != 'Kh√¥ng s·ª≠ d·ª•ng'

                    if use_reranker_flag_main:
                        processing_log.append(f"[{time.time() - start_time:.2f}s]: Reranker '{current_selected_reranker_name.split('/')[-1]}' ƒëang ho·∫°t ƒë·ªông.")
                    else:
                        processing_log.append(f"[{time.time() - start_time:.2f}s]: Reranker kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng.")
                    message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                    history_for_llm1_main = st.session_state.messages[-(config.MAX_HISTORY_TURNS * 2):-1] 
                    processing_log.append(f"[{time.time() - start_time:.2f}s] Ph√¢n t√≠ch c√¢u h·ªèi \"{user_query}\"...")
                    message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                    use_two_dense_hybrid_from_session = (st.session_state.hybrid_component_mode == "2 Dense + 1 Sparse")
                    secondary_embedding_model_object_main = None
                    secondary_vector_db_main = None

                    if st.session_state.retrieval_method == 'K·∫øt h·ª£p':
                        selected_secondary_emb_name = st.session_state.get("selected_secondary_emb_name")
                        st.write(selected_secondary_emb_name)
                        if use_two_dense_hybrid_from_session: 
                            if selected_secondary_emb_name: 
                                secondary_embedding_model_object_main = st.session_state.app_loaded_embedding_models.get(selected_secondary_emb_name)
                                secondary_rag_components = st.session_state.app_rag_components_per_embedding_model.get(selected_secondary_emb_name)
                                if secondary_rag_components:
                                    secondary_vector_db_main = secondary_rag_components[0]

                                if secondary_embedding_model_object_main and secondary_vector_db_main:
                                    if secondary_embedding_model_object_main != active_emb_obj or \
                                    (secondary_embedding_model_object_main == active_emb_obj and selected_secondary_emb_name != st.session_state.selected_emb_name): # ƒê·∫£m b·∫£o kh√°c bi·ªát th·ª±c s·ª±
                                        processing_log.append(f"[{time.time() - start_time:.2f}s]: Hybrid mode (2-Dense) v·ªõi Embedding Ph·ª•: {selected_secondary_emb_name.split('/')[-1]}.")
                                    else:
                                        processing_log.append(f"[{time.time() - start_time:.2f}s]: Embedding Ph·ª• gi·ªëng Embedding Ch√≠nh. Chuy·ªÉn sang Hybrid mode (1-Dense).")
                                        use_two_dense_hybrid_from_session = False # Ghi ƒë√® n·∫øu kh√¥ng h·ª£p l·ªá
                                else:
                                    processing_log.append(f"[{time.time() - start_time:.2f}s]: Kh√¥ng t√¨m th·∫•y component cho Embedding Ph·ª•. Chuy·ªÉn sang Hybrid mode (1-Dense).")
                                    use_two_dense_hybrid_from_session = False # Ghi ƒë√®
                            else:
                                processing_log.append(f"[{time.time() - start_time:.2f}s]: Ch∆∞a ch·ªçn Embedding Ph·ª• cho ch·∫ø ƒë·ªô 2-Dense. Chuy·ªÉn sang Hybrid mode (1-Dense).")
                                use_two_dense_hybrid_from_session = False # Ghi ƒë√®
                        else: # Tr∆∞·ªùng h·ª£p "1 Dense + 1 Sparse" ƒë∆∞·ª£c ch·ªçn
                            processing_log.append(f"[{time.time() - start_time:.2f}s]: Hybrid mode (1-Dense) ƒë√£ ƒë∆∞·ª£c ch·ªçn.")

                    relevance_status, direct_answer, all_queries, summarizing_q = utils.generate_query_variations(
                        original_query=user_query,
                        gemini_model=active_gem_obj,
                        chat_history=history_for_llm1_main,
                        num_variations=config.NUM_QUERY_VARIATIONS 
                    )
                    
                    if relevance_status == 'invalid':
                        full_response = direct_answer if direct_answer and direct_answer.strip() else "‚ö†Ô∏è C√¢u h·ªèi c·ªßa b·∫°n c√≥ v·∫ª kh√¥ng li√™n quan ƒë·∫øn Lu·∫≠t Giao th√¥ng ƒê∆∞·ªùng b·ªô Vi·ªát Nam."
                        processing_log.append(f"[{time.time() - start_time:.2f}s] Ho√†n t·∫•t (C√¢u h·ªèi kh√¥ng li√™n quan).")
                    else:
                        recent_chat_history_main = st.session_state.messages[-(config.MAX_HISTORY_TURNS * 2):-1] 
                        queries_to_search_main = []
                        query_source_log_main = ""
                        retrieval_query_mode_main = st.session_state.retrieval_query_mode
                        if retrieval_query_mode_main == 'ƒê∆°n gi·∫£n':
                            queries_to_search_main = [user_query]
                            query_source_log_main = "c√¢u h·ªèi g·ªëc"
                        elif retrieval_query_mode_main == 'M·ªü r·ªông':
                            queries_to_search_main = [summarizing_q] if summarizing_q else [user_query]
                            query_source_log_main = "c√¢u h·ªèi m·ªü r·ªông t·ª´ c√¢u g·ªëc"
                        elif retrieval_query_mode_main == 'ƒêa d·∫°ng':
                            queries_to_search_main = all_queries if all_queries else [user_query]
                            query_source_log_main = f"c√¢u h·ªèi g·ªëc v√† {max(0, len(all_queries)-1)} bi·∫øn th·ªÉ"

                        retrieval_method_main = st.session_state.retrieval_method
                        processing_log.append(f"[{time.time() - start_time:.2f}s]: B·∫Øt ƒë·∫ßu Truy v·∫•n (Ngu·ªìn: {query_source_log_main}, Ph∆∞∆°ng th·ª©c: {retrieval_method_main})...")
                        message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                        collected_docs_data_main = {}
                        retrieval_start_time = time.time()
                        
                        for q_variant in queries_to_search_main:
                            if not q_variant: continue
                            
                            search_results = active_retriever.search(
                                q_variant,
                                active_emb_obj,
                                method=st.session_state.retrieval_method,
                                k=config.VECTOR_K_PER_QUERY if st.session_state.retrieval_method != 'K·∫øt h·ª£p' else config.HYBRID_K_PER_QUERY,
                                secondary_embedding_model=secondary_embedding_model_object_main if use_two_dense_hybrid_from_session else None,
                                secondary_vector_db=secondary_vector_db_main if use_two_dense_hybrid_from_session else None,
                                use_two_dense_if_hybrid=use_two_dense_hybrid_from_session
                            )

                            for item_res in search_results:
                                doc_idx = item_res['index']
                                if doc_idx not in collected_docs_data_main:
                                    collected_docs_data_main[doc_idx] = item_res
                        retrieval_time = time.time() - retrieval_start_time
                        num_unique_docs_main = len(collected_docs_data_main)
                        processing_log.append(f"[{time.time() - start_time:.2f}s]: Truy v·∫•n ({retrieval_time:.2f}s) t√¨m th·∫•y {num_unique_docs_main} t√†i li·ªáu ·ª©ng vi√™n.")
                        message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                        retrieved_docs_list_main = list(collected_docs_data_main.values())
                        sort_reverse_main = (retrieval_method_main != 'Ng·ªØ nghƒ©a')
                        retrieved_docs_list_main.sort(key=lambda x: x.get('score', 0 if sort_reverse_main else float('inf')), reverse=sort_reverse_main)

                        reranked_documents_for_llm_main = []
                        rerank_time = 0.0

                        if use_reranker_flag_main and num_unique_docs_main > 0:
                            rerank_start_time = time.time()
                            query_for_reranking_main = summarizing_q if summarizing_q else user_query
                            processing_log.append(f"[{time.time() - start_time:.2f}s]: X·∫øp h·∫°ng l·∫°i {min(num_unique_docs_main, config.MAX_DOCS_FOR_RERANK)} t√†i li·ªáu b·∫±ng '{current_selected_reranker_name.split('/')[-1]}'...") #
                            message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                            docs_to_rerank_input_main = retrieved_docs_list_main[:config.MAX_DOCS_FOR_RERANK] #
                            rerank_input_formatted_main = [{'doc': item['doc'], 'index': item['index']} for item in docs_to_rerank_input_main]

                            reranked_results_list_main = rerank_documents( #
                                query_for_reranking_main,
                                rerank_input_formatted_main,
                                active_reranker_obj
                            )
                            reranked_documents_for_llm_main = reranked_results_list_main[:config.FINAL_NUM_RESULTS_AFTER_RERANK] #
                            rerank_time = time.time() - rerank_start_time
                            processing_log.append(f"[{time.time() - start_time:.2f}s]: X·∫øp h·∫°ng ({rerank_time:.2f}s) ho√†n t·∫•t, ch·ªçn top {len(reranked_documents_for_llm_main)}.")
                            message_placeholder.markdown(" ".join(processing_log) + "‚è≥")
                        elif num_unique_docs_main > 0:
                            processing_log.append(f"[{time.time() - start_time:.2f}s]: B·ªè qua X·∫øp h·∫°ng, l·∫•y top {config.FINAL_NUM_RESULTS_AFTER_RERANK} k·∫øt qu·∫£ Retrieval.") 
                            temp_docs_main = retrieved_docs_list_main[:config.FINAL_NUM_RESULTS_AFTER_RERANK] 
                            reranked_documents_for_llm_main = [
                                {'doc': item['doc'], 'score': item.get('score'), 'original_index': item['index']}
                                for item in temp_docs_main
                            ]
                            message_placeholder.markdown(" ".join(processing_log) + "‚è≥")
                        else:
                            processing_log.append(f"[{time.time() - start_time:.2f}s]: Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan.")
                            message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                        final_relevant_documents_for_display_main = reranked_documents_for_llm_main

                        answer_mode_main = st.session_state.answer_mode
                        processing_log.append(f"[{time.time() - start_time:.2f}s]: T·ªïng h·ª£p c√¢u tr·∫£ l·ªùi (ch·∫ø ƒë·ªô: {answer_mode_main})...")
                        message_placeholder.markdown(" ".join(processing_log))

                        raw_llm_output = generate_answer_with_gemini( 
                            query_text=user_query,
                            relevant_documents=reranked_documents_for_llm_main,
                            gemini_model=active_gem_obj,
                            mode=answer_mode_main,
                            chat_history=recent_chat_history_main
                        )
                        processing_log.append(f"[{time.time() - start_time:.2f}s]: Ho√†n t·∫•t!")

                    with st.expander("Xem chi ti·∫øt qu√° tr√¨nh x·ª≠ l√Ω", expanded=False):
                        log_content_main = "\n".join(processing_log)
                        st.markdown(f"```text\n{log_content_main}\n```")

                    if raw_llm_output:
                        content_for_display_main = utils.render_html_for_assistant_message(raw_llm_output, final_relevant_documents_for_display_main) #
                        message_placeholder.markdown(content_for_display_main, unsafe_allow_html=True)
                        full_response = raw_llm_output
                    else:
                        message_placeholder.markdown(full_response, unsafe_allow_html=True)

                except Exception as e_main:
                    st.error(f"üêû ƒê√£ x·∫£y ra l·ªói: {e_main}")
                    st.expander("Xem Traceback L·ªói").code(traceback.format_exc())
                    full_response = f"üêû Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh x·ª≠ l√Ω. Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c thay ƒë·ªïi c·∫•u h√¨nh."
                    message_placeholder.markdown(full_response, unsafe_allow_html=True)
                finally:
                    if user_query and full_response:
                        utils.log_qa_to_json(user_query, full_response) #

                    if full_response:
                        assistant_message_main = {"role": "assistant", "content": full_response}
                        if relevance_status != 'invalid' and final_relevant_documents_for_display_main:
                            assistant_message_main["relevant_docs_for_display"] = final_relevant_documents_for_display_main
                        else:
                            assistant_message_main["relevant_docs_for_display"] = []
                        st.session_state.messages.append(assistant_message_main)
    else:
        st.error("‚ö†Ô∏è Chatbot kh√¥ng th·ªÉ ho·∫°t ƒë·ªông do thi·∫øu c√°c th√†nh ph·∫ßn c·∫ßn thi·∫øt. Vui l√≤ng ki·ªÉm tra th√¥ng b√°o l·ªói ·ªü tr√™n.")

elif not st.session_state.app_resources_initialized:
    app_init_status_placeholder.error("‚ö†Ô∏è H·ªá th·ªëng CH∆ØA S·∫¥N S√ÄNG. L·ªói trong qu√° tr√¨nh t·∫£i model ho·∫∑c t·∫°o RAG. Vui l√≤ng ki·ªÉm tra log chi ti·∫øt trong c√°c kh·ªëi 'status' ·ªü tr√™n (n·∫øu c√≥) ho·∫∑c l√†m m·ªõi trang ƒë·ªÉ th·ª≠ l·∫°i.")