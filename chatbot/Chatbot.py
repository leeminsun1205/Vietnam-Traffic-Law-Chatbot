# app.py
import streamlit as st
import time
import config
import utils
import data_loader

# --- H√†m Cache ƒë·ªÉ Kh·ªüi t·∫°o DB v√† Retriever ---
@st.cache_resource
def cached_load_or_create_components(_embedding_model):
    vector_db, hybrid_retriever = data_loader.load_or_create_rag_components(_embedding_model)
    return vector_db, hybrid_retriever

# --- C·∫•u h√¨nh Trang Streamlit ---
st.set_page_config(page_title="Chatbot Lu·∫≠t GTƒêB", layout="wide", initial_sidebar_state="auto")

# --- Kh·ªüi t·∫°o Session State cho L·ªãch s·ª≠ Chat v√† C·∫•u h√¨nh ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# C·∫•u h√¨nh m√¥ h√¨nh v√† tr·∫£ l·ªùi
if "selected_gemini_model" not in st.session_state:
    st.session_state.selected_gemini_model = config.DEFAULT_GEMINI_MODEL
if "answer_mode" not in st.session_state:
    st.session_state.answer_mode = 'Ng·∫Øn g·ªçn'

# C·∫•u h√¨nh truy v·∫•n (query variation)
if "retrieval_query_mode" not in st.session_state:
    st.session_state.retrieval_query_mode = 'T·ªïng qu√°t' 
if "use_history_for_llm1" not in st.session_state:
    st.session_state.use_history_for_llm1 = True

# --- C·∫•u h√¨nh ph∆∞∆°ng th·ª©c Retrieval v√† Reranker ---
if "retrieval_method" not in st.session_state:
    st.session_state.retrieval_method = 'hybrid' 
if "use_reranker" not in st.session_state:
    st.session_state.use_reranker = True 

# --- Sidebar ---
with st.sidebar:
    st.title("T√πy ch·ªçn")

    st.header("M√¥ h√¨nh & Tr·∫£ l·ªùi")

    selected_model = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh Gemini:",
        options=config.AVAILABLE_GEMINI_MODELS,
        index=config.AVAILABLE_GEMINI_MODELS.index(st.session_state.selected_gemini_model),
        key="selected_gemini_model",
        help="Ch·ªçn m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn ƒë·ªÉ x·ª≠ l√Ω y√™u c·∫ßu."
    )

    answer_mode_choice = st.radio(
        "Ch·ªçn ch·∫ø ƒë·ªô tr·∫£ l·ªùi:",
        options=['Ng·∫Øn g·ªçn', 'ƒê·∫ßy ƒë·ªß'],
        key="answer_mode",
        horizontal=True,
        help="M·ª©c ƒë·ªô chi ti·∫øt c·ªßa c√¢u tr·∫£ l·ªùi."
    )

    st.header("C·∫•u h√¨nh truy v·∫•n")

    retrieval_query_mode_choice = st.radio(
        "Ngu·ªìn c√¢u h·ªèi cho Retrieval:",
        options=['ƒê∆°n gi·∫£n', 'T·ªïng qu√°t', 'S√¢u'],
        key="retrieval_query_mode", 
        horizontal=True,
        help=(
            "**ƒê∆°n gi·∫£n:** Ch·ªâ d√πng c√¢u h·ªèi g·ªëc.\n"
            "**T·ªïng qu√°t:** Ch·ªâ d√πng c√¢u h·ªèi t√≥m t·∫Øt (do AI t·∫°o).\n"
            "**S√¢u:** D√πng c·∫£ c√¢u h·ªèi g·ªëc v√† c√°c bi·∫øn th·ªÉ (do AI t·∫°o)."
        )
    )
    # Ch·ªçn ph∆∞∆°ng th·ª©c Retrieval
    retrieval_method_choice = st.radio(
        "Ph∆∞∆°ng th·ª©c Retrieval:",
        options=['dense', 'sparse', 'hybrid'],
        index=['dense', 'sparse', 'hybrid'].index(st.session_state.retrieval_method), 
        key="retrieval_method",
        horizontal=True,
        help=(
            "**dense:** T√¨m ki·∫øm d·ª±a tr√™n vector ng·ªØ nghƒ©a (nhanh, hi·ªÉu ng·ªØ c·∫£nh).\n"
            "**sparse:** T√¨m ki·∫øm d·ª±a tr√™n t·ª´ kh√≥a (BM25) (nhanh, ch√≠nh x√°c t·ª´ kh√≥a).\n"
            "**hybrid:** K·∫øt h·ª£p c·∫£ dense v√† sparse (c√¢n b·∫±ng, c√≥ th·ªÉ t·ªët nh·∫•t)."
        )
    )
    # B·∫≠t/t·∫Øt Reranker
    use_rerank_toggle = st.toggle(
        "S·ª≠ d·ª•ng Reranker",
        key="use_reranker",
        value=st.session_state.use_reranker,
        help="B·∫≠t ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh CrossEncoder x·∫øp h·∫°ng l·∫°i k·∫øt qu·∫£ t√¨m ki·∫øm (tƒÉng ƒë·ªô ch√≠nh x√°c nh∆∞ng ch·∫≠m h∆°n)."
    )

    use_hist_llm1 = st.toggle(
        "D√πng l·ªãch s·ª≠ cho ph√¢n t√≠ch c√¢u h·ªèi",
        key="use_history_for_llm1",
        value=st.session_state.use_history_for_llm1,
        help="Cho ph√©p LLM xem x√©t ng·ªØ c·∫£nh h·ªôi tho·∫°i khi ph√¢n t√≠ch c√¢u h·ªèi ƒë·∫ßu v√†o."
    )

    st.markdown("---") 

    st.header("Qu·∫£n l√Ω H·ªôi tho·∫°i")
    if st.button("‚ö†Ô∏è X√≥a L·ªãch S·ª≠ Chat"):
        st.session_state.messages = []
        st.success("ƒê√£ x√≥a l·ªãch s·ª≠ chat!")
        time.sleep(1)
        st.rerun()
    st.markdown("---")


# --- Giao di·ªán ch√≠nh c·ªßa ·ª®ng d·ª•ng ---
st.title("‚öñÔ∏è Chatbot H·ªèi ƒê√°p Lu·∫≠t Giao Th√¥ng ƒê∆∞·ªùng B·ªô VN")
st.caption(f"D·ª±a tr√™n c√°c vƒÉn b·∫£n Lu·∫≠t, Ngh·ªã ƒê·ªãnh, Th√¥ng t∆∞ v·ªÅ Lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô Vi·ªát Nam.")

# --- C·∫≠p nh·∫≠t Caption hi·ªÉn th·ªã c·∫•u h√¨nh ---
hist_llm1_status = "B·∫≠t" if st.session_state.use_history_for_llm1 else "T·∫Øt"
reranker_status = "B·∫≠t" if st.session_state.use_reranker else "T·∫Øt"
st.caption(f"Model: `{st.session_state.selected_gemini_model}` | Tr·∫£ l·ªùi: `{st.session_state.answer_mode}` | Ngu·ªìn Query: `{st.session_state.retrieval_query_mode}` | Retrieval: `{st.session_state.retrieval_method}` | Reranker: `{reranker_status}` | History LLM1: `{hist_llm1_status}`")

# --- Hi·ªÉn th·ªã L·ªãch s·ª≠ Chat ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Kh·ªüi t·∫°o h·ªá th·ªëng ---
init_ok = False
with st.status("ƒêang kh·ªüi t·∫°o h·ªá th·ªëng...", expanded=True) as status:
    g_embedding_model = utils.load_embedding_model(config.embedding_model_name)
    g_reranking_model = utils.load_reranker_model(config.reranking_model_name)
    models_loaded = all([g_embedding_model, g_reranking_model])
    # T·∫£i retriever (c·∫ßn embedding model)
    g_vector_db, g_hybrid_retriever = cached_load_or_create_components(g_embedding_model)
    retriever_ready = g_hybrid_retriever is not None

    if not models_loaded:
         status.update(label="‚ö†Ô∏è L·ªói t·∫£i Embedding ho·∫∑c Reranker model!", state="error", expanded=True)
    elif not retriever_ready:
        status.update(label="‚ö†Ô∏è L·ªói kh·ªüi t·∫°o VectorDB ho·∫∑c Retriever!", state="error", expanded=True)
    else:
        status.update(label="‚úÖ H·ªá th·ªëng c∆° b·∫£n ƒë√£ s·∫µn s√†ng!", state="complete", expanded=False)
        init_ok = True

# --- Input v√† X·ª≠ l√Ω ---
if init_ok:
    if user_query := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ Lu·∫≠t GTƒêB..."):
        # 1. Th√™m v√† hi·ªÉn th·ªã tin nh·∫Øn c·ªßa ng∆∞·ªùi d√πng
        st.session_state.messages.append({"role": "user", "content": user_query})
        with st.chat_message("user"):
            st.markdown(user_query)

        # 2. X·ª≠ l√Ω v√† t·∫°o ph·∫£n h·ªìi t·ª´ bot
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            processing_log = []
            try:
                start_time = time.time()
                processing_log.append(f"[{time.time() - start_time:.2f}s] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
                message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                # --- T·∫£i model Gemini ƒë√£ ch·ªçn ---
                selected_model_name = st.session_state.selected_gemini_model
                selected_gemini_llm = utils.load_gemini_model(selected_model_name)
                if not selected_gemini_llm:
                     raise ValueError(f"Kh√¥ng th·ªÉ t·∫£i model Gemini: {selected_model_name}")
                processing_log.append(f"[{time.time() - start_time:.2f}s]: Model '{selected_model_name}' ƒë√£ s·∫µn s√†ng.")
                message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                history_for_llm1 = None
                log_hist_llm1 = "(kh√¥ng d√πng l·ªãch s·ª≠)"
                if st.session_state.use_history_for_llm1:
                    history_for_llm1 = st.session_state.messages[-(config.MAX_HISTORY_TURNS * 2):-1]
                    log_hist_llm1 = "(c√≥ d√πng l·ªãch s·ª≠)"
                processing_log.append(f"[{time.time() - start_time:.2f}s] Ph√¢n t√≠ch c√¢u h·ªèi {log_hist_llm1}...")
                message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                # --- B∆∞·ªõc A: Ph√¢n lo·∫°i relevancy v√† t·∫°o bi·∫øn th·ªÉ/t√≥m t·∫Øt ---
                relevance_status, direct_answer, all_queries, summarizing_q = utils.generate_query_variations(
                    original_query=user_query,
                    gemini_model=selected_gemini_llm,
                    chat_history=history_for_llm1,
                    num_variations=config.NUM_QUERY_VARIATIONS
                )

                # --- Ki·ªÉm tra m·ª©c ƒë·ªô li√™n quan ---
                if relevance_status == 'invalid':
                    if direct_answer and direct_answer.strip():
                        full_response = direct_answer
                    else:
                        full_response = "‚ö†Ô∏è C√¢u h·ªèi c·ªßa b·∫°n c√≥ v·∫ª kh√¥ng li√™n quan ƒë·∫øn Lu·∫≠t Giao th√¥ng ƒê∆∞·ªùng b·ªô Vi·ªát Nam."
                    processing_log.append(f"[{time.time() - start_time:.2f}s] Ho√†n t·∫•t (C√¢u h·ªèi kh√¥ng li√™n quan).")

                # --- N·∫øu c√¢u h·ªèi h·ª£p l·ªá, ti·∫øp t·ª•c x·ª≠ l√Ω RAG ---
                else:
                    # --- L·∫•y l·ªãch s·ª≠ g·∫ßn ƒë√¢y cho LLM th·ª© 2 (t·∫°o c√¢u tr·∫£ l·ªùi) ---
                    recent_chat_history = st.session_state.messages[-(config.MAX_HISTORY_TURNS * 2):-1]

                    # --- X√°c ƒë·ªãnh query(s) ƒë·ªÉ t√¨m ki·∫øm d·ª±a tr√™n retrieval_query_mode ---
                    queries_to_search = []
                    query_source_log = ""
                    retrieval_query_mode = st.session_state.retrieval_query_mode
                    if retrieval_query_mode == 'ƒê∆°n gi·∫£n':
                        queries_to_search = [user_query]
                        query_source_log = "c√¢u h·ªèi g·ªëc"
                    elif retrieval_query_mode == 'T·ªïng qu√°t':
                        queries_to_search = [summarizing_q]
                        query_source_log = "c√¢u h·ªèi t√≥m t·∫Øt"
                    elif retrieval_query_mode == 'S√¢u':
                        queries_to_search = all_queries # all_queries ƒë√£ bao g·ªìm user_query
                        query_source_log = f"c√¢u h·ªèi g·ªëc v√† {len(all_queries)-1} bi·∫øn th·ªÉ"

                    # --- L·∫•y c·∫•u h√¨nh retrieval v√† rerank ---
                    retrieval_method = st.session_state.retrieval_method
                    use_reranker = st.session_state.use_reranker

                    processing_log.append(f"[{time.time() - start_time:.2f}s]: B·∫Øt ƒë·∫ßu Retrieval (Ngu·ªìn: {query_source_log}, Ph∆∞∆°ng th·ª©c: {retrieval_method})...")
                    message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                    # --- Th·ª±c hi·ªán Retrieval ---
                    collected_docs_data = {} # Dict l∆∞u k·∫øt qu·∫£ {index: {'doc': ..., 'score': ...}}
                    retrieval_start_time = time.time()
                    st.write('HAHHAHAHA')
                    for q_idx, current_query in enumerate(queries_to_search):
                        # G·ªçi ph∆∞∆°ng th·ª©c search m·ªõi c·ªßa retriever
                        search_results = g_hybrid_retriever.search(
                            current_query,
                            g_embedding_model,
                            method=retrieval_method,
                            k=config.VECTOR_K_PER_QUERY # L·∫•y nhi·ªÅu h∆°n ƒë·ªÉ c√≥ ƒë·ªß cho rerank/fusion
                        )
                        # T·ªïng h·ª£p k·∫øt qu·∫£, tr√°nh tr√πng l·∫∑p index
                        for item in search_results:
                            doc_index = item['index']
                            if doc_index not in collected_docs_data:
                                collected_docs_data[doc_index] = item # L∆∞u c·∫£ score t·ª´ retrieval
                            # Optional: N·∫øu mu·ªën c·∫≠p nh·∫≠t score (v√≠ d·ª•: l·∫•y score cao nh·∫•t n·∫øu tr√πng) - ph·ª©c t·∫°p h∆°n
                    retrieval_time = time.time() - retrieval_start_time
                    st.write(search_results)
                    st.write('kkkkkk')
                    num_unique_docs = len(collected_docs_data)
                    st.write('kkkkkk')
                    processing_log.append(f"[{time.time() - start_time:.2f}s]: Retrieval ({retrieval_time:.2f}s) t√¨m th·∫•y {num_unique_docs} t√†i li·ªáu ·ª©ng vi√™n.")
                    message_placeholder.markdown(" ".join(processing_log) + "‚è≥")


                    # --- Chu·∫©n b·ªã d·ªØ li·ªáu cho b∆∞·ªõc ti·∫øp theo (Rerank ho·∫∑c l·∫•y tr·ª±c ti·∫øp) ---
                    # Chuy·ªÉn dict th√†nh list v√† s·∫Øp x·∫øp theo score (cao xu·ªëng th·∫•p cho sparse/hybrid, th·∫•p l√™n cao cho dense)
                    retrieved_docs_list = list(collected_docs_data.values())
                    sort_reverse = (retrieval_method != 'dense') # Dense s·∫Øp x·∫øp ng∆∞·ª£c l·∫°i
                    retrieved_docs_list.sort(key=lambda x: x.get('score', 0 if sort_reverse else float('inf')), reverse=sort_reverse)

                    # --- B∆∞·ªõc Rerank (N·∫øu ƒë∆∞·ª£c b·∫≠t) ---
                    final_relevant_documents = [] # List c√°c dict {'doc': ..., 'score': ..., 'original_index': ...}
                    rerank_time = 0.0
                    rerank_start_time = time.time()

                    if use_reranker and num_unique_docs > 0:
                        # L·∫•y query ph√π h·ª£p ƒë·ªÉ rerank (th∆∞·ªùng l√† c√¢u t√≥m t·∫Øt ho·∫∑c c√¢u g·ªëc)
                        query_for_reranking = summarizing_q if summarizing_q else user_query
                        processing_log.append(f"[{time.time() - start_time:.2f}s]: X·∫øp h·∫°ng l·∫°i {min(num_unique_docs, config.MAX_DOCS_FOR_RERANK)} t√†i li·ªáu...")
                        message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                        # Ch·ªçn top N docs ƒë·ªÉ rerank
                        docs_to_rerank = retrieved_docs_list[:config.MAX_DOCS_FOR_RERANK]
                        # Chuy·ªÉn ƒë·ªïi ƒë·ªãnh d·∫°ng ƒë·∫ßu v√†o cho rerank_documents n·∫øu c·∫ßn
                        # H√†m rerank_documents hi·ªán t·∫°i nh·∫≠n list [{'doc': ..., 'index': ...}]
                        # N·∫øu retrieved_docs_list ƒë√£ ƒë√∫ng ƒë·ªãnh d·∫°ng th√¨ kh√¥ng c·∫ßn chuy·ªÉn
                        rerank_input = [{'doc': item['doc'], 'index': item['index']} for item in docs_to_rerank]

                        reranked_results = utils.rerank_documents(
                            query_for_reranking,
                            rerank_input, # ƒê·∫£m b·∫£o ƒë√∫ng ƒë·ªãnh d·∫°ng ƒë·∫ßu v√†o
                            g_reranking_model
                        )
                        # L·∫•y top K k·∫øt qu·∫£ cu·ªëi c√πng sau rerank
                        final_relevant_documents = reranked_results[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
                        rerank_time = time.time() - rerank_start_time
                        processing_log.append(f"[{time.time() - start_time:.2f}s]: Rerank ({rerank_time:.2f}s) ho√†n t·∫•t, ch·ªçn top {len(final_relevant_documents)}.")
                        message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                    elif num_unique_docs > 0: # Kh√¥ng d√πng reranker nh∆∞ng c√≥ k·∫øt qu·∫£ retrieval
                        processing_log.append(f"[{time.time() - start_time:.2f}s]: B·ªè qua Rerank, l·∫•y tr·ª±c ti·∫øp top {config.FINAL_NUM_RESULTS_AFTER_RERANK} k·∫øt qu·∫£ Retrieval.")
                        final_relevant_documents = retrieved_docs_list[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
                        message_placeholder.markdown(" ".join(processing_log) + "‚è≥")
                    else: # Kh√¥ng c√≥ k·∫øt qu·∫£ retrieval
                         processing_log.append(f"[{time.time() - start_time:.2f}s]: Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan.")
                         message_placeholder.markdown(" ".join(processing_log) + "‚è≥")


                    # --- B∆∞·ªõc Generate Answer ---
                    answer_mode = st.session_state.answer_mode
                    processing_log.append(f"[{time.time() - start_time:.2f}s]: T·ªïng h·ª£p c√¢u tr·∫£ l·ªùi (ch·∫ø ƒë·ªô: {answer_mode})...")
                    message_placeholder.markdown(" ".join(processing_log))

                    full_response = utils.generate_answer_with_gemini(
                        query_text=user_query, # V·∫´n d√πng c√¢u h·ªèi g·ªëc c·ªßa user ƒë·ªÉ LLM tr·∫£ l·ªùi
                        relevant_documents=final_relevant_documents, # List c√°c dict {'doc': ...}
                        gemini_model=selected_gemini_llm,
                        mode=answer_mode,
                        chat_history=recent_chat_history
                    )

                    processing_log.append(f"[{time.time() - start_time:.2f}s]: Ho√†n t·∫•t!")

                # Hi·ªÉn th·ªã log x·ª≠ l√Ω
                with st.expander("Xem chi ti·∫øt qu√° tr√¨nh x·ª≠ l√Ω", expanded=False):
                    log_content = "\n".join(processing_log)
                    st.markdown(f"```text\n{log_content}\n```")
                # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi cu·ªëi c√πng
                message_placeholder.markdown(full_response)

            except Exception as e:
                st.error(f"üêû ƒê√£ x·∫£y ra l·ªói: {e}") # Hi·ªÉn th·ªã l·ªói r√µ r√†ng h∆°n
                full_response = f"üêû Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh x·ª≠ l√Ω. Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c thay ƒë·ªïi c·∫•u h√¨nh."
                if message_placeholder:
                    message_placeholder.markdown(full_response)
                else:
                    st.markdown(full_response) # Fallback n·∫øu placeholder l·ªói
            finally:
                # ƒê·∫£m b·∫£o tin nh·∫Øn c·ªßa assistant lu√¥n ƒë∆∞·ª£c th√™m v√†o history
                if full_response: # Ch·ªâ th√™m n·∫øu c√≥ n·ªôi dung
                    st.session_state.messages.append({"role": "assistant", "content": full_response})

elif not init_ok:
    st.error("‚ö†Ô∏è H·ªá th·ªëng ch∆∞a th·ªÉ kh·ªüi ƒë·ªông do l·ªói t·∫£i m√¥ h√¨nh ho·∫∑c d·ªØ li·ªáu. Vui l√≤ng ki·ªÉm tra l·∫°i.")