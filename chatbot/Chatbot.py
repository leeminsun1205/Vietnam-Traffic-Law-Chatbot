# app.py
import streamlit as st
import time
import config
import utils 
from model_loader import load_embedding_model, load_gemini_model, load_reranker_model
from data_loader import load_or_create_rag_components #ƒê·∫£m b·∫£o import ƒë√∫ng
from reranker import rerank_documents
from generation import generate_answer_with_gemini

# @st.cache_resource # S·ª≠ d·ª•ng cache c·ªßa Streamlit cho h√†m n√†y r·∫•t quan tr·ªçng
def cached_load_or_create_components(embedding_model_name: str, _embedding_model_object):
    # H√†m n√†y s·∫Ω t·∫°o ho·∫∑c t·∫£i VectorDB v√† Retriever cho m·ªôt embedding model c·ª• th·ªÉ.
    # _embedding_model_object ƒë∆∞·ª£c truy·ªÅn v√†o ƒë·ªÉ ƒë·∫£m b·∫£o cache ho·∫°t ƒë·ªông ƒë√∫ng khi object thay ƒë·ªïi.
    current_rag_data_prefix = config.get_rag_data_prefix(embedding_model_name)
    vector_db, retriever = load_or_create_rag_components(_embedding_model_object, current_rag_data_prefix)
    return vector_db, retriever

# --- C·∫§U H√åNH TRANG STREAMLIT ---
st.set_page_config(page_title="Chatbot Lu·∫≠t GTƒêB", layout="wide", initial_sidebar_state="auto")

# --- Kh·ªüi t·∫°o Session State cho L·ªãch s·ª≠ Chat v√† C·∫•u h√¨nh ---

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ch√†o b·∫°n, t√¥i l√† chatbot Lu·∫≠t Giao th√¥ng ƒê∆∞·ªùng b·ªô. B·∫°n c·∫ßn h·ªó tr·ª£ g√¨?"}]

# Model ch√≠nh
if "selected_embedding_model" not in st.session_state:
    st.session_state.selected_embedding_model = config.DEFAULT_EMBEDDING_MODEL
# Model ph·ª• cho hybrid
if "additional_hybrid_models" not in st.session_state:
    st.session_state.additional_hybrid_models = [] # Kh·ªüi t·∫°o l√† list r·ªóng

if "selected_gemini_model" not in st.session_state:
    st.session_state.selected_gemini_model = config.DEFAULT_GEMINI_MODEL
if "answer_mode" not in st.session_state:
    st.session_state.answer_mode = 'Ng·∫Øn g·ªçn'
if "retrieval_query_mode" not in st.session_state:
    st.session_state.retrieval_query_mode = 'M·ªü r·ªông' 
if "retrieval_method" not in st.session_state:
    st.session_state.retrieval_method = 'hybrid'
if "selected_reranker_model" not in st.session_state:
    st.session_state.selected_reranker_model = config.DEFAULT_RERANKER_MODEL

# --- Sidebar ---
with st.sidebar:
    st.title("T√πy ch·ªçn")
    st.header("M√¥ h√¨nh")

    # Ch·ªçn m√¥ h√¨nh Embedding ch√≠nh
    selected_embedding_model_name = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh Embedding ch√≠nh:",
        options=config.AVAILABLE_EMBEDDING_MODELS,
        index=config.AVAILABLE_EMBEDDING_MODELS.index(st.session_state.selected_embedding_model),
        key="selected_embedding_model", # Gi·ªØ key n√†y cho session state
        help="Ch·ªçn m√¥ h√¨nh embedding ch√≠nh. M√¥ h√¨nh n√†y s·∫Ω ƒë∆∞·ª£c d√πng cho t√¨m ki·∫øm 'dense' thu·∫ßn t√∫y, l√†m c∆° s·ªü cho BM25 (sparse) v√† l√† m·ªôt ph·∫ßn c·ªßa t√¨m ki·∫øm 'hybrid'."
    )

    # Ch·ªçn m√¥ h√¨nh Embedding ph·ª• (ch·ªâ hi·ªÉn th·ªã khi retrieval_method l√† 'hybrid')
    if st.session_state.retrieval_method == 'hybrid':
        available_for_additional = [
            m for m in config.AVAILABLE_EMBEDDING_MODELS 
            if m != st.session_state.selected_embedding_model # Lo·∫°i tr·ª´ model ch√≠nh ƒë√£ ch·ªçn
        ]
        
        # L·∫•y l·ª±a ch·ªçn hi·ªán t·∫°i t·ª´ session_state, ƒë·∫£m b·∫£o n√≥ l√† list
        current_additional_selection = st.session_state.get("additional_hybrid_models", [])
        if not isinstance(current_additional_selection, list):
            current_additional_selection = []
        
        # L·ªçc ra nh·ªØng l·ª±a ch·ªçn c≈© kh√¥ng c√≤n h·ª£p l·ªá (v√≠ d·ª• model ch√≠nh thay ƒë·ªïi)
        valid_current_additional_selection = [m for m in current_additional_selection if m in available_for_additional]

        if available_for_additional:
            selected_additional_models = st.multiselect(
                "Ch·ªçn th√™m m√¥ h√¨nh Embedding ph·ª• cho Hybrid (t·ªëi ƒëa 2):",
                options=available_for_additional,
                default=valid_current_additional_selection, # default l√† list ƒë√£ l·ªçc
                # key="additional_hybrid_models_multiselect", # S·ª≠ d·ª•ng key m·ªõi n·∫øu c·∫ßn ph√¢n bi·ªát, ho·∫∑c c·∫≠p nh·∫≠t session_state tr·ª±c ti·∫øp
                help="K·∫øt h·ª£p th√™m t·ªëi ƒëa 2 m√¥ h√¨nh embedding kh√°c. T·ªïng s·ªë m√¥ h√¨nh dense trong hybrid s·∫Ω l√† 1 (ch√≠nh) + s·ªë l∆∞·ª£ng ch·ªçn ·ªü ƒë√¢y."
            )
            if len(selected_additional_models) > 2:
                st.warning("B·∫°n ch·ªâ c√≥ th·ªÉ ch·ªçn t·ªëi ƒëa 2 m√¥ h√¨nh embedding ph·ª•. S·∫Ω ch·ªâ l·∫•y 2 m√¥ h√¨nh ƒë·∫ßu ti√™n ƒë∆∞·ª£c ch·ªçn.")
                st.session_state.additional_hybrid_models = selected_additional_models[:2]
            else:
                st.session_state.additional_hybrid_models = selected_additional_models
        else:
            st.markdown("<p style='font-size:0.9em; font-style:italic;'>Kh√¥ng c√≥ m√¥ h√¨nh embedding ph·ª• n√†o kh√°c ƒë·ªÉ ch·ªçn.</p>", unsafe_allow_html=True)
            st.session_state.additional_hybrid_models = [] # Reset n·∫øu kh√¥ng c√≥ l·ª±a ch·ªçn
    else:
        # N·∫øu kh√¥ng ph·∫£i hybrid, x√≥a l·ª±a ch·ªçn model ph·ª•
        st.session_state.additional_hybrid_models = []


    selected_model_llm = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh Gemini:",
        options=config.AVAILABLE_GEMINI_MODELS,
        index=config.AVAILABLE_GEMINI_MODELS.index(st.session_state.selected_gemini_model),
        key="selected_gemini_model",
        help="Ch·ªçn m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn ƒë·ªÉ x·ª≠ l√Ω y√™u c·∫ßu."
    )
    selected_reranker = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh Reranker:",
        options=config.AVAILABLE_RERANKER_MODELS,
        index=config.AVAILABLE_RERANKER_MODELS.index(st.session_state.selected_reranker_model),
        key="selected_reranker_model",
        help="Ch·ªçn m√¥ h√¨nh ƒë·ªÉ x·∫øp h·∫°ng l·∫°i k·∫øt qu·∫£ t√¨m ki·∫øm. Ch·ªçn 'Kh√¥ng s·ª≠ d·ª•ng' ƒë·ªÉ t·∫Øt."
    )
    answer_mode_choice = st.radio(
        "Ch·ªçn ch·∫ø ƒë·ªô tr·∫£ l·ªùi:", options=['Ng·∫Øn g·ªçn', 'ƒê·∫ßy ƒë·ªß'], key="answer_mode", horizontal=True,
        help="M·ª©c ƒë·ªô chi ti·∫øt c·ªßa c√¢u tr·∫£ l·ªùi."
    )

    st.header("C·∫•u h√¨nh truy v·∫•n")
    retrieval_query_mode_choice = st.radio(
        "Ngu·ªìn c√¢u h·ªèi cho Retrieval:", options=['ƒê∆°n gi·∫£n', 'M·ªü r·ªông', 'ƒêa d·∫°ng'], key="retrieval_query_mode", horizontal=True,
        help=(
            "**ƒê∆°n gi·∫£n:** Ch·ªâ d√πng c√¢u h·ªèi g·ªëc.\n"
            "**M·ªü r·ªông:** Ch·ªâ d√πng c√¢u h·ªèi m·ªü r·ªông t·ª´ c√¢u h·ªèi g·ªëc (do AI t·∫°o).\n"
            "**ƒêa d·∫°ng:** D√πng c·∫£ c√¢u h·ªèi g·ªëc v√† c√°c bi·∫øn th·ªÉ t·ª´ c√¢u h·ªèi g·ªëc(do AI t·∫°o)."
        )
    )
    # C·∫≠p nh·∫≠t index cho radio retrieval_method
    current_retrieval_method_index = ['dense', 'sparse', 'hybrid'].index(st.session_state.retrieval_method)
    retrieval_method_choice = st.radio(
        "Ph∆∞∆°ng th·ª©c Retrieval:", options=['dense', 'sparse', 'hybrid'], index=current_retrieval_method_index, key="retrieval_method", horizontal=True,
        help=(
            "**dense:** T√¨m ki·∫øm d·ª±a tr√™n vector ng·ªØ nghƒ©a (nhanh, hi·ªÉu ng·ªØ c·∫£nh).\n"
            "**sparse:** T√¨m ki·∫øm d·ª±a tr√™n t·ª´ kh√≥a (BM25) (nhanh, ch√≠nh x√°c t·ª´ kh√≥a).\n"
            "**hybrid:** K·∫øt h·ª£p c·∫£ dense v√† sparse, c√≥ th·ªÉ bao g·ªìm nhi·ªÅu ngu·ªìn dense (c√¢n b·∫±ng, c√≥ th·ªÉ t·ªët nh·∫•t)."
        )
    )

    st.markdown("---") 
    st.header("Qu·∫£n l√Ω H·ªôi tho·∫°i")
    if st.button("‚ö†Ô∏è X√≥a L·ªãch S·ª≠ Chat"):
        st.session_state.messages = []
        st.success("ƒê√£ x√≥a l·ªãch s·ª≠ chat!")
        time.sleep(1); st.rerun()
    st.markdown("---")

# --- Giao di·ªán ch√≠nh c·ªßa ·ª®ng d·ª•ng ---
st.title("‚öñÔ∏è Chatbot H·ªèi ƒê√°p Lu·∫≠t Giao Th√¥ng ƒê∆∞·ªùng B·ªô VN")
st.caption(f"D·ª±a tr√™n c√°c vƒÉn b·∫£n Lu·∫≠t, Ngh·ªã ƒê·ªãnh, Th√¥ng t∆∞ v·ªÅ Lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô Vi·ªát Nam.")

# --- C·∫≠p nh·∫≠t Caption hi·ªÉn th·ªã c·∫•u h√¨nh ---
reranker_status_display = st.session_state.selected_reranker_model
if reranker_status_display == 'Kh√¥ng s·ª≠ d·ª•ng': reranker_status_display = "T·∫Øt"
else: reranker_status_display = reranker_status_display.split('/')[-1]

# Hi·ªÉn th·ªã c√°c model ph·ª• n·∫øu c√≥
additional_models_display_caption = "Kh√¥ng"
if st.session_state.retrieval_method == 'hybrid' and st.session_state.additional_hybrid_models:
    additional_names = [name.split('/')[-1] for name in st.session_state.additional_hybrid_models]
    additional_models_display_caption = ", ".join(additional_names) if additional_names else "Kh√¥ng"

st.caption(
    f"Embedding ch√≠nh: `{st.session_state.selected_embedding_model.split('/')[-1]}` | "
    f"Embeddings ph·ª• (Hybrid): `{additional_models_display_caption}` | "
    f"M√¥ h√¨nh LLM: `{st.session_state.selected_gemini_model}` | Tr·∫£ l·ªùi: `{st.session_state.answer_mode}` | "
    f"Ngu·ªìn Query: `{st.session_state.retrieval_query_mode}` | Retrieval: `{st.session_state.retrieval_method}` | "
    f"Reranker: `{reranker_status_display}`"
)

# --- Hi·ªÉn th·ªã L·ªãch s·ª≠ Chat ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        content_to_display = message["content"]
        if message["role"] == "assistant":
            docs_for_this_message = message.get("relevant_docs_for_display", [])
            content_to_display = utils.render_html_for_assistant_message(content_to_display, docs_for_this_message)
        st.markdown(content_to_display, unsafe_allow_html=True)

# --- Kh·ªüi t·∫°o h·ªá th·ªëng ---
init_ok = False
reranking_model_loaded = None
embedding_model_object = None # Model ch√≠nh
vector_db = None # VectorDB ch√≠nh
retriever = None # Retriever ch√≠nh, s·∫Ω s·ª≠ d·ª•ng model ch√≠nh v√† c√°c model ph·ª•

with st.status("ƒêang kh·ªüi t·∫°o h·ªá th·ªëng...", expanded=True) as status:
    # 1. T·∫£i embedding model ch√≠nh
    embedding_model_object = load_embedding_model(st.session_state.selected_embedding_model)
    
    # 2. T·∫£i VectorDB v√† Retriever cho model ch√≠nh
    if embedding_model_object:
        vector_db, retriever = cached_load_or_create_components(
            st.session_state.selected_embedding_model, 
            embedding_model_object
        )
    
    models_loaded_ok = all([embedding_model_object])
    retriever_ready_ok = all([vector_db, retriever])
    
    status_label = "‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!"
    status_state = "complete"
    init_ok = True

    if not models_loaded_ok: 
        status_label = f"‚ö†Ô∏è L·ªói t·∫£i Embedding model ch√≠nh ({st.session_state.selected_embedding_model})!"
        status_state = "error"; init_ok = False
    elif not retriever_ready_ok:
        status_label = f"‚ö†Ô∏è L·ªói kh·ªüi t·∫°o VectorDB/Retriever cho model ch√≠nh!"
        status_state = "error"; init_ok = False

    # 3. T·∫£i c√°c embedding model ph·ª• v√† VDB c·ªßa ch√∫ng n·∫øu c·∫ßn (ch·ªâ th√¥ng b√°o, kh√¥ng ch·∫∑n init ch√≠nh)
    # C√°c object n√†y s·∫Ω ƒë∆∞·ª£c load l·∫°i l√∫c x·ª≠ l√Ω query ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n c·ªßa cache
    if init_ok and st.session_state.retrieval_method == 'hybrid' and st.session_state.additional_hybrid_models:
        status.write("ƒêang ki·ªÉm tra c√°c embedding model ph·ª•...")
        for model_name_add_init in st.session_state.additional_hybrid_models:
            emb_obj_add_init = load_embedding_model(model_name_add_init)
            if emb_obj_add_init:
                vdb_add_init, _ = cached_load_or_create_components(model_name_add_init, emb_obj_add_init)
                if not vdb_add_init:
                    status.write(f"L∆∞u √Ω: Kh√¥ng th·ªÉ t·∫£i VectorDB cho model ph·ª• '{model_name_add_init.split('/')[-1]}' l√∫c kh·ªüi t·∫°o.")
            else:
                status.write(f"L∆∞u √Ω: Kh√¥ng th·ªÉ t·∫£i Embedding Model ph·ª• '{model_name_add_init.split('/')[-1]}' l√∫c kh·ªüi t·∫°o.")
        status.write("Ki·ªÉm tra model ph·ª• ho√†n t·∫•t.")


    if init_ok:
        status.update(label=status_label, state=status_state, expanded=False)
    else:
        status.update(label=status_label, state=status_state, expanded=True)


# --- Input v√† X·ª≠ l√Ω ---
if init_ok and retriever: # ƒê·∫£m b·∫£o retriever ch√≠nh ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o
    if user_query := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ Lu·∫≠t GTƒêB..."):
        st.session_state.messages.append({"role": "user", "content": user_query})
        with st.chat_message("user"):
            st.markdown(user_query)

        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            raw_llm_output = ""
            processing_log = []
            final_relevant_documents = [] 
            relevance_status = 'valid'

            try:
                start_time = time.time()
                processing_log.append(f"[{time.time() - start_time:.2f}s] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
                message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                # T·∫£i reranker model
                if st.session_state.selected_reranker_model != 'Kh√¥ng s·ª≠ d·ª•ng':
                    reranking_model_loaded = load_reranker_model(st.session_state.selected_reranker_model)
                else: reranking_model_loaded = None
                
                # T·∫£i LLM (Gemini)
                selected_llm_name = st.session_state.selected_gemini_model
                selected_gemini_llm = load_gemini_model(selected_llm_name)
                if not selected_gemini_llm:
                     raise ValueError(f"Kh√¥ng th·ªÉ t·∫£i model Gemini: {selected_llm_name}")
                processing_log.append(f"[{time.time() - start_time:.2f}s]: Model LLM '{selected_llm_name}' ƒë√£ s·∫µn s√†ng.")
                message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                current_reranker_model_obj = reranking_model_loaded
                use_reranker_flag = current_reranker_model_obj is not None and st.session_state.selected_reranker_model != 'Kh√¥ng s·ª≠ d·ª•ng'
                if use_reranker_flag:
                     processing_log.append(f"[{time.time() - start_time:.2f}s]: Reranker '{st.session_state.selected_reranker_model.split('/')[-1]}' ƒë√£ s·∫µn s√†ng.")
                else:
                     processing_log.append(f"[{time.time() - start_time:.2f}s]: Reranker kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng.")
                message_placeholder.markdown(" ".join(processing_log) + "‚è≥")
                
                # --- B∆∞·ªõc A: Ph√¢n lo·∫°i relevancy v√† t·∫°o bi·∫øn th·ªÉ/t√≥m t·∫Øt ---
                history_for_llm1 = st.session_state.messages[-(config.MAX_HISTORY_TURNS * 2):-1]
                log_hist_llm1 = "(c√≥ d√πng l·ªãch s·ª≠)" if history_for_llm1 else "(kh√¥ng d√πng l·ªãch s·ª≠)"
                processing_log.append(f"[{time.time() - start_time:.2f}s] Ph√¢n t√≠ch c√¢u h·ªèi {log_hist_llm1}...")
                message_placeholder.markdown(" ".join(processing_log) + "‚è≥")
                
                relevance_status, direct_answer, all_queries, summarizing_q = utils.generate_query_variations(
                    original_query=user_query, gemini_model=selected_gemini_llm,
                    chat_history=history_for_llm1, num_variations=config.NUM_QUERY_VARIATIONS
                )
                
                if relevance_status == 'invalid':
                    full_response = direct_answer if direct_answer and direct_answer.strip() else "‚ö†Ô∏è C√¢u h·ªèi c·ªßa b·∫°n c√≥ v·∫ª kh√¥ng li√™n quan ƒë·∫øn Lu·∫≠t Giao th√¥ng ƒê∆∞·ªùng b·ªô Vi·ªát Nam."
                    processing_log.append(f"[{time.time() - start_time:.2f}s] Ho√†n t·∫•t (C√¢u h·ªèi kh√¥ng li√™n quan).")
                else: # C√¢u h·ªèi h·ª£p l·ªá, ti·∫øp t·ª•c RAG
                    recent_chat_history = st.session_state.messages[-(config.MAX_HISTORY_TURNS * 2):-1]
                    queries_to_search = []
                    query_source_log = ""
                    retrieval_query_mode = st.session_state.retrieval_query_mode
                    if retrieval_query_mode == 'ƒê∆°n gi·∫£n': queries_to_search = [user_query]; query_source_log = "c√¢u h·ªèi g·ªëc"
                    elif retrieval_query_mode == 'M·ªü r·ªông': queries_to_search = [summarizing_q]; query_source_log = "c√¢u h·ªèi t√≥m t·∫Øt/m·ªü r·ªông"
                    elif retrieval_query_mode == 'ƒêa d·∫°ng': queries_to_search = all_queries; query_source_log = f"c√¢u h·ªèi g·ªëc v√† {len(all_queries)-1} bi·∫øn th·ªÉ"
                    
                    retrieval_method = st.session_state.retrieval_method
                    
                    # --- Chu·∫©n b·ªã c√°c ngu·ªìn dense ph·ª• cho Retriever ---
                    additional_dense_sources_for_retriever_search = []
                    if retrieval_method == 'hybrid' and st.session_state.additional_hybrid_models:
                        processing_log.append(f"[{time.time() - start_time:.2f}s] ƒêang t·∫£i c√°c ngu·ªìn embedding ph·ª• cho hybrid search...")
                        message_placeholder.markdown(" ".join(processing_log) + "‚è≥")
                        for model_name_add in st.session_state.additional_hybrid_models:
                            add_emb_model_obj = load_embedding_model(model_name_add) # H√†m n√†y ƒë√£ c√≥ @st.cache_resource
                            if add_emb_model_obj:
                                # H√†m n√†y c≈©ng ƒë√£ c√≥ @st.cache_resource
                                add_vector_db, _ = cached_load_or_create_components(model_name_add, add_emb_model_obj)
                                if add_vector_db:
                                    additional_dense_sources_for_retriever_search.append((add_emb_model_obj, add_vector_db))
                                    processing_log.append(f"[{time.time() - start_time:.2f}s] Ngu·ªìn ph·ª• '{model_name_add.split('/')[-1]}' ƒë√£ ƒë∆∞·ª£c n·∫°p.")
                                else:
                                    processing_log.append(f"[{time.time() - start_time:.2f}s] L∆ØU √ù: Kh√¥ng t·∫£i ƒë∆∞·ª£c VectorDB cho ngu·ªìn ph·ª• '{model_name_add.split('/')[-1]}'. B·ªè qua.")
                            else:
                                processing_log.append(f"[{time.time() - start_time:.2f}s] L∆ØU √ù: Kh√¥ng t·∫£i ƒë∆∞·ª£c Embedding Model cho ngu·ªìn ph·ª• '{model_name_add.split('/')[-1]}'. B·ªè qua.")
                        message_placeholder.markdown(" ".join(processing_log) + "‚è≥")
                    
                    num_additional_sources = len(additional_dense_sources_for_retriever_search)
                    hybrid_info_log = ""
                    if retrieval_method == 'hybrid':
                        hybrid_info_log = f" (Ch√≠nh: {st.session_state.selected_embedding_model.split('/')[-1]}"
                        if num_additional_sources > 0:
                            add_names = [name.split('/')[-1] for name in st.session_state.additional_hybrid_models[:num_additional_sources]] # Ch·ªâ log c√°c model ƒë√£ load th√†nh c√¥ng
                            hybrid_info_log += f", Ph·ª•: {', '.join(add_names)}"
                        hybrid_info_log += ")"
                    
                    processing_log.append(f"[{time.time() - start_time:.2f}s]: B·∫Øt ƒë·∫ßu Retrieval (Ngu·ªìn Query: {query_source_log}, Ph∆∞∆°ng th·ª©c: {retrieval_method}{hybrid_info_log})...")
                    message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                    collected_docs_data = {} 
                    retrieval_start_time = time.time()
                    
                    for q_idx, current_query_variant in enumerate(queries_to_search):
                        if not current_query_variant: continue
                        # G·ªçi ph∆∞∆°ng th·ª©c search c·ªßa retriever ch√≠nh
                        # embedding_model_object l√† model ch√≠nh ƒë√£ ƒë∆∞·ª£c t·∫£i ·ªü ph·∫ßn init
                        search_results_variant = retriever.search(
                            current_query_variant,
                            embedding_model_object, # Model object cho VDB ch√≠nh c·ªßa retriever
                            method=retrieval_method,
                            k=config.VECTOR_K_PER_QUERY, 
                            additional_dense_sources=additional_dense_sources_for_retriever_search
                        )
                        for item_res in search_results_variant:
                            doc_index = item_res['index']
                            if doc_index not in collected_docs_data:
                                collected_docs_data[doc_index] = item_res
                            else: # N·∫øu tr√πng, c·∫≠p nh·∫≠t score n·∫øu score m·ªõi t·ªët h∆°n
                                # Score RRF/BM25: cao h∆°n t·ªët h∆°n. Score L2 distance (dense thu·∫ßn t√∫y): th·∫•p h∆°n t·ªët h∆°n.
                                if (retrieval_method == 'hybrid' or retrieval_method == 'sparse') and item_res['score'] > collected_docs_data[doc_index]['score']:
                                    collected_docs_data[doc_index] = item_res
                                elif retrieval_method == 'dense' and item_res['score'] < collected_docs_data[doc_index]['score']:
                                     collected_docs_data[doc_index] = item_res


                    retrieval_time = time.time() - retrieval_start_time
                    num_unique_docs = len(collected_docs_data)
                    processing_log.append(f"[{time.time() - start_time:.2f}s]: Retrieval ({retrieval_time:.2f}s) t√¨m th·∫•y {num_unique_docs} t√†i li·ªáu ·ª©ng vi√™n.")
                    message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                    retrieved_docs_list = list(collected_docs_data.values())
                    # S·∫Øp x·∫øp d·ª±a tr√™n ph∆∞∆°ng th·ª©c retrieval cu·ªëi c√πng ƒë√£ d√πng
                    # Hybrid v√† Sparse: score cao t·ªët h∆°n. Dense: score th·∫•p t·ªët h∆°n (distance).
                    sort_reverse_final = (retrieval_method == 'hybrid' or retrieval_method == 'sparse')
                    retrieved_docs_list.sort(key=lambda x: x.get('score', 0 if sort_reverse_final else float('inf')), reverse=sort_reverse_final)

                    final_relevant_documents = []
                    rerank_time_val = 0.0
                    
                    if use_reranker_flag and num_unique_docs > 0:
                        rerank_start_time_val = time.time()
                        query_for_reranking_step = summarizing_q if summarizing_q else user_query
                        processing_log.append(f"[{time.time() - start_time:.2f}s]: X·∫øp h·∫°ng l·∫°i {min(num_unique_docs, config.MAX_DOCS_FOR_RERANK)} t√†i li·ªáu b·∫±ng '{st.session_state.selected_reranker_model.split('/')[-1]}'...")
                        message_placeholder.markdown(" ".join(processing_log) + "‚è≥")
                        
                        docs_to_rerank_input = retrieved_docs_list[:config.MAX_DOCS_FOR_RERANK]
                        # rerank_documents_with_indices y√™u c·∫ßu input d·∫°ng [{'doc': ..., 'index': ...}]
                        rerank_input_formatted = [{'doc': item_rr['doc'], 'index': item_rr['index']} for item_rr in docs_to_rerank_input]

                        reranked_results = rerank_documents(
                            query_for_reranking_step,
                            rerank_input_formatted, 
                            current_reranker_model_obj 
                        )
                        final_relevant_documents = reranked_results[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
                        rerank_time_val = time.time() - rerank_start_time_val
                        processing_log.append(f"[{time.time() - start_time:.2f}s]: Rerank ({rerank_time_val:.2f}s) ho√†n t·∫•t, ch·ªçn top {len(final_relevant_documents)}.")
                        message_placeholder.markdown(" ".join(processing_log) + "‚è≥")
                    elif num_unique_docs > 0:
                        processing_log.append(f"[{time.time() - start_time:.2f}s]: B·ªè qua Rerank, l·∫•y tr·ª±c ti·∫øp top {config.FINAL_NUM_RESULTS_AFTER_RERANK} k·∫øt qu·∫£ Retrieval.")
                        final_relevant_documents = retrieved_docs_list[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
                        message_placeholder.markdown(" ".join(processing_log) + "‚è≥")
                    else:
                         processing_log.append(f"[{time.time() - start_time:.2f}s]: Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan t·ª´ Retrieval.")
                         message_placeholder.markdown(" ".join(processing_log) + "‚è≥")

                    answer_mode = st.session_state.answer_mode
                    processing_log.append(f"[{time.time() - start_time:.2f}s]: T·ªïng h·ª£p c√¢u tr·∫£ l·ªùi (ch·∫ø ƒë·ªô: {answer_mode})...")
                    message_placeholder.markdown(" ".join(processing_log)) # Kh√¥ng c√≥ icon ·ªü ƒë√¢y
                    
                    raw_llm_output = generate_answer_with_gemini(
                        query_text=user_query, relevant_documents=final_relevant_documents, 
                        gemini_model=selected_gemini_llm, mode=answer_mode,
                        chat_history=recent_chat_history
                    )
                    processing_log.append(f"[{time.time() - start_time:.2f}s]: Ho√†n t·∫•t!")

                with st.expander("Xem chi ti·∫øt qu√° tr√¨nh x·ª≠ l√Ω", expanded=False):
                    st.markdown(f"```text\n{'\n'.join(processing_log)}\n```")
                
                if raw_llm_output:
                    content_for_immediate_display = utils.render_html_for_assistant_message(raw_llm_output, final_relevant_documents)
                    message_placeholder.markdown(content_for_immediate_display, unsafe_allow_html=True)
                    full_response = raw_llm_output
                else: # Tr∆∞·ªùng h·ª£p full_response ƒë√£ ƒë∆∞·ª£c g√°n (v√≠ d·ª• c√¢u h·ªèi kh√¥ng li√™n quan)
                    message_placeholder.markdown(full_response, unsafe_allow_html=True)

            except Exception as e:
                st.error(f"üêû ƒê√£ x·∫£y ra l·ªói: {e}") 
                full_response = f"üêû Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh x·ª≠ l√Ω. Vui l√≤ng th·ª≠ l·∫°i ho·∫∑c thay ƒë·ªïi c·∫•u h√¨nh."
                if message_placeholder: message_placeholder.markdown(full_response, unsafe_allow_html=True)
                else: st.markdown(full_response, unsafe_allow_html=True) # Fallback
            finally:
                if user_query and full_response:
                    utils.log_qa_to_json(user_query, full_response)
                
                if full_response: 
                    assistant_message = {"role": "assistant", "content": full_response}
                    # Ch·ªâ th√™m relevant_docs n·∫øu c√¢u h·ªèi h·ª£p l·ªá v√† c√≥ t√†i li·ªáu
                    if relevance_status != 'invalid' and 'final_relevant_documents' in locals() and final_relevant_documents:
                        assistant_message["relevant_docs_for_display"] = final_relevant_documents
                    else: # Bao g·ªìm c·∫£ tr∆∞·ªùng h·ª£p invalid ho·∫∑c kh√¥ng c√≥ docs
                        assistant_message["relevant_docs_for_display"] = []
                    st.session_state.messages.append(assistant_message)

elif not init_ok:
    st.error("‚ö†Ô∏è H·ªá th·ªëng ch∆∞a th·ªÉ kh·ªüi ƒë·ªông do l·ªói t·∫£i m√¥ h√¨nh ch√≠nh ho·∫∑c d·ªØ li·ªáu VectorDB/Retriever ch√≠nh. Vui l√≤ng ki·ªÉm tra l·∫°i.")
else: # init_ok nh∆∞ng retriever ch∆∞a s·∫µn s√†ng (tr∆∞·ªùng h·ª£p hi·∫øm)
    st.error("‚ö†Ô∏è Retriever ch√≠nh ch∆∞a s·∫µn s√†ng. Vui l√≤ng ki·ªÉm tra c·∫•u h√¨nh v√† th·ª≠ l√†m m·ªõi trang.")