# app.py
import streamlit as st
import os
import time 
import config
import utils
import data_loader

MAX_HISTORY_TURNS = 5

# --- H√†m Cache ƒë·ªÉ Kh·ªüi t·∫°o DB v√† Retriever ---
@st.cache_resource
def cached_load_or_create_components(_embedding_model): 
    vector_db, hybrid_retriever = data_loader.load_or_create_rag_components(_embedding_model)
    return vector_db, hybrid_retriever

# --- C·∫•u h√¨nh Trang Streamlit ---
st.set_page_config(page_title="Chatbot Lu·∫≠t GTƒêB", layout="wide", initial_sidebar_state="collapsed")

# --- Kh·ªüi t·∫°o Session State cho L·ªãch s·ª≠ Chat ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if "selected_gemini_model" not in st.session_state:
    st.session_state.selected_gemini_model = config.DEFAULT_GEMINI_MODEL

if "answer_mode" not in st.session_state:
    st.session_state.answer_mode = 'Ng·∫Øn g·ªçn'

# --- Sidebar ---
with st.sidebar:
    st.title("T√πy ch·ªçn")
    # Widget ƒë·ªÉ ch·ªçn m√¥ h√¨nh Gemini
    selected_model = st.selectbox(
        "Ch·ªçn m√¥ h√¨nh Gemini:",
        options=config.AVAILABLE_GEMINI_MODELS,
        index=config.AVAILABLE_GEMINI_MODELS.index(st.session_state.selected_gemini_model), # ƒê·∫∑t gi√° tr·ªã hi·ªán t·∫°i
        key="selected_gemini_model", # L∆∞u l·ª±a ch·ªçn v√†o session state
        help="Ch·ªçn m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn ƒë·ªÉ x·ª≠ l√Ω y√™u c·∫ßu. C√°c m√¥ h√¨nh kh√°c nhau c√≥ th·ªÉ cho t·ªëc ƒë·ªô v√† ch·∫•t l∆∞·ª£ng tr·∫£ l·ªùi kh√°c nhau."
    )
    st.markdown("---")

    answer_mode_choice = st.radio(
        "Ch·ªçn ch·∫ø ƒë·ªô tr·∫£ l·ªùi:",
        options=['Ng·∫Øn g·ªçn', 'ƒê·∫ßy ƒë·ªß'],
        key="answer_mode", # L∆∞u v√†o session state
        # index=1 if st.session_state.answer_mode == 'ƒê·∫ßy ƒë·ªß' else 0, # B·ªè index n·∫øu d√πng key
        horizontal=True,
        help="Ch·ªçn m·ª©c ƒë·ªô chi ti·∫øt cho c√¢u tr·∫£ l·ªùi c·ªßa bot."
    )
    st.markdown("---")

    st.write("Qu·∫£n l√Ω h·ªôi tho·∫°i:")
    if st.button("‚ö†Ô∏è X√≥a L·ªãch S·ª≠ Chat"):
        st.session_state.messages = [] 
        st.success("ƒê√£ x√≥a l·ªãch s·ª≠ chat!") 
        time.sleep(1) 
        st.rerun() 
    st.markdown("---")

# --- Giao di·ªán ch√≠nh c·ªßa ·ª®ng d·ª•ng ---
st.title("‚öñÔ∏è Chatbot H·ªèi ƒê√°p Lu·∫≠t Giao Th√¥ng ƒê∆∞·ªùng B·ªô VN")
st.caption(f"D·ª±a tr√™n c√°c vƒÉn b·∫£n Lu·∫≠t, Ngh·ªã ƒê·ªãnh, Th√¥ng t∆∞ v·ªÅ Lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô Vi·ªát Nam.")
st.caption(f"Model: {st.session_state.selected_gemini_model} | Ch·∫ø ƒë·ªô: {st.session_state.answer_mode}")

# --- Hi·ªÉn th·ªã L·ªãch s·ª≠ Chat ---
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- Kh·ªüi t·∫°o h·ªá th·ªëng ---
init_ok = False
with st.status("ƒêang kh·ªüi t·∫°o h·ªá th·ªëng...", expanded=True) as status:
    g_embedding_model = utils.load_embedding_model(config.embedding_model_name)
    g_reranking_model = utils.load_reranker_model(config.reranking_model_name)
    # g_gemini_model = utils.load_gemini_model(config.gemini_model_name)
    models_loaded = all([g_embedding_model, g_reranking_model])
    g_vector_db, g_hybrid_retriever = cached_load_or_create_components(g_embedding_model)
    retriever_ready = g_hybrid_retriever is not None
    if not retriever_ready:
        raise ValueError("Kh√¥ng th·ªÉ chu·∫©n b·ªã c∆° s·ªü d·ªØ li·ªáu vector ho·∫∑c retriever.")

    status.update(label="‚úÖ H·ªá th·ªëng c∆° b·∫£n ƒë√£ s·∫µn s√†ng!", state="complete", expanded=False)
    init_ok = True

# --- Input v√† X·ª≠ l√Ω ---
if init_ok:
    if user_query := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ Lu·∫≠t GTƒêB..."):
        # 1. Th√™m v√† hi·ªÉn th·ªã tin nh·∫Øn c·ªßa ng∆∞·ªùi d√πng
        st.session_state.messages.append({"role": "user", "content": user_query})
        with st.chat_message("user"):
            st.markdown(user_query)

        # 2. X·ª≠ l√Ω v√† t·∫°o ph·∫£n h·ªìi t·ª´ bot
        with st.chat_message("assistant"):
            message_placeholder = st.empty() 
            full_response = ""
            processing_log = []
            try:
                start_time = time.time()
                processing_log.append(f"[{time.time() - start_time:.2f}s] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")
                message_placeholder.markdown(" ".join(processing_log) + "...")

                # --- T·∫£i model Gemini ƒë√£ ch·ªçn ---
                selected_model_name = st.session_state.selected_gemini_model
                selected_gemini_llm = utils.load_gemini_model(selected_model_name)
                processing_log.append(f"[{time.time() - start_time:.2f}s]: Model '{selected_model_name}' ƒë√£ s·∫µn s√†ng.*")
                message_placeholder.markdown(" ".join(processing_log) + "...")

                # --- B∆∞·ªõc A: Ph√¢n lo·∫°i relevancy ---
                processing_log.append(f"[{time.time() - start_time:.2f}s] Ph√¢n t√≠ch c√¢u h·ªèi...")
                message_placeholder.markdown(" ".join(processing_log) + "...")
                relevance_status, direct_answer, _, summarizing_q = utils.generate_query_variations(
                    user_query, 
                    selected_gemini_llm, 
                    num_variations=config.NUM_QUERY_VARIATIONS
                )

                # --- Ki·ªÉm tra m·ª©c ƒë·ªô li√™n quan ---
                if relevance_status == 'invalid':
                    if direct_answer and direct_answer.strip():
                        full_response = direct_answer
                    else:
                        full_response = "‚ö†Ô∏è C√¢u h·ªèi c·ªßa b·∫°n c√≥ v·∫ª kh√¥ng li√™n quan ƒë·∫øn Lu·∫≠t Giao th√¥ng ƒê∆∞·ªùng b·ªô Vi·ªát Nam."
                    processing_log.append(f"[{time.time() - start_time:.2f}s] Ho√†n t·∫•t (C√¢u h·ªèi kh√¥ng li√™n quan).")

                # --- N·∫øu c√¢u h·ªèi h·ª£p l·ªá, ti·∫øp t·ª•c x·ª≠ l√Ω RAG ---
                else:
                    # --- L·∫•y l·ªãch s·ª≠ g·∫ßn ƒë√¢y cho LLM th·ª© 2 ---
                    recent_chat_history = st.session_state.messages[-(MAX_HISTORY_TURNS * 2):-1] # B·ªè qua tin nh·∫Øn cu·ªëi c√πng c·ªßa user (ƒë√£ c√≥ trong query_text)

                    # 2a. Hybrid Search (D√πng summarizing_q)
                    processing_log.append(f"\n*{time.time() - start_time:.2f}s: T√¨m ki·∫øm t√†i li·ªáu...*")
                    message_placeholder.markdown(" ".join(processing_log) + "...")
                    variant_results = g_hybrid_retriever.hybrid_search(
                            summarizing_q, g_embedding_model, 
                            vector_search_k=config.VECTOR_K_PER_QUERY,
                            final_k=config.HYBRID_K_PER_QUERY
                    )
                    collected_docs_data = {}
                    for item in variant_results: 
                        doc_index = item['index']
                        if doc_index not in collected_docs_data:
                            collected_docs_data[doc_index] = {'doc': item['doc']}
                    num_unique_docs = len(collected_docs_data)
                    processing_log.append(f"\n*{time.time() - start_time:.2f}s: T√¨m th·∫•y {num_unique_docs} t√†i li·ªáu ·ª©ng vi√™n.*")
                    message_placeholder.markdown(" ".join(processing_log) + "...")

                    unique_docs_for_reranking_input = []
                    if num_unique_docs > 0:
                        unique_docs_for_reranking_input = [{'doc': data['doc'], 'index': idx}
                                                    for idx, data in collected_docs_data.items()]
                        if len(unique_docs_for_reranking_input) > config.MAX_DOCS_FOR_RERANK:
                            unique_docs_for_reranking_input = unique_docs_for_reranking_input[:config.MAX_DOCS_FOR_RERANK]


                    # 2b. Re-ranking (D√πng summarizing_q)
                    final_relevant_documents = []
                    if unique_docs_for_reranking_input:
                        processing_log.append(f"\n*{time.time() - start_time:.2f}s: X·∫øp h·∫°ng l·∫°i {len(unique_docs_for_reranking_input)} t√†i li·ªáu...*")
                        message_placeholder.markdown(" ".join(processing_log) + "...")
                        reranked_results = utils.rerank_documents(
                            summarizing_q, 
                            unique_docs_for_reranking_input,
                            g_reranking_model
                        )
                        final_relevant_documents = reranked_results[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
                        processing_log.append(f"\n*{time.time() - start_time:.2f}s: Ch·ªçn top {len(final_relevant_documents)} t√†i li·ªáu.*")
                        message_placeholder.markdown(" ".join(processing_log) + "...")

                    # 2c. Generate Answer (Truy·ªÅn history v√†o ƒë√¢y)
                    answer_mode = st.session_state.answer_mode
                    processing_log.append(f"\n*{time.time() - start_time:.2f}s: T·ªïng h·ª£p c√¢u tr·∫£ l·ªùi...")
                    message_placeholder.markdown(" ".join(processing_log))

                    full_response = utils.generate_answer_with_gemini(
                        query_text=user_query,
                        relevant_documents=final_relevant_documents,
                        gemini_model=selected_gemini_llm, 
                        mode=answer_mode,
                        chat_history=recent_chat_history 
                    )

                    # C·∫≠p nh·∫≠t placeholder v·ªõi c√¢u tr·∫£ l·ªùi cu·ªëi c√πng
                    processing_log.append(f"\n*{time.time() - start_time:.2f}s: Ho√†n t·∫•t!*")

                with st.expander("Xem chi ti·∫øt qu√° tr√¨nh x·ª≠ l√Ω", expanded=False):
                    log_content = "\n".join(processing_log)
                    st.markdown(f"```text\n{log_content}\n```") 
                # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi ch√≠nh
                message_placeholder.markdown(full_response)

                    # --- Hi·ªÉn th·ªã ·∫¢nh (n·∫øu c√≥) ---
                    # if final_relevant_documents:
                    #     st.markdown("---")
                    #     with st.expander("Xem H√¨nh ·∫¢nh Bi·ªÉn B√°o Li√™n Quan (N·∫øu c√≥)"):
                    #         displayed_images = set()
                    #         image_found_in_context = False
                    #         cols = st.columns(5) # Hi·ªÉn th·ªã t·ªëi ƒëa 5 ·∫£nh/h√†ng
                    #         col_idx = 0
                    #         for item in final_relevant_documents:
                    #             doc = item.get('doc')
                    #             if doc:
                    #                 metadata = doc.get('metadata', {})
                    #                 image_path = metadata.get('sign_image_path') 
                    #                 sign_code = metadata.get('sign_code')

                    #                 if image_path and image_path not in displayed_images:
                                        
                    #                     full_image_path = image_path 
                    #                     # Ho·∫∑c full_image_path = os.path.join("images", os.path.basename(image_path))

                    #                     if os.path.exists(full_image_path):
                    #                         with cols[col_idx % 5]:
                    #                             st.image(full_image_path, caption=f"{sign_code}" if sign_code else None, use_column_width=True)
                    #                         displayed_images.add(image_path)
                    #                         image_found_in_context = True
                    #                         col_idx += 1

                    #         if not image_found_in_context:
                    #             st.write("_Kh√¥ng t√¨m th·∫•y h√¨nh ·∫£nh bi·ªÉn b√°o trong c√°c t√†i li·ªáu tham kh·∫£o._")
            except Exception as e:
                full_response = f"üêû Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra trong qu√° tr√¨nh x·ª≠ l√Ω: {e}"
                if message_placeholder: 
                    message_placeholder.markdown(full_response)
                else:
                    st.markdown(full_response) 
            st.session_state.messages.append({"role": "assistant", "content": full_response})

elif not init_ok:
    st.error("‚ö†Ô∏è H·ªá th·ªëng ch∆∞a th·ªÉ kh·ªüi ƒë·ªông do l·ªói.")