# app.py
import streamlit as st
import os
import time 
import config
import utils
import data_loader

# --- H√†m Cache ƒë·ªÉ Kh·ªüi t·∫°o DB v√† Retriever ---
@st.cache_resource
def cached_load_or_create_components(_embedding_model): 
    vector_db, hybrid_retriever = data_loader.load_or_create_rag_components(_embedding_model)
    return vector_db, hybrid_retriever

# --- C·∫•u h√¨nh Trang Streamlit ---
st.set_page_config(page_title="Chatbot Lu·∫≠t GTƒêB", layout="wide", initial_sidebar_state="collapsed")

# --- Giao di·ªán ch√≠nh c·ªßa ·ª®ng d·ª•ng ---
st.title("‚öñÔ∏è Chatbot H·ªèi ƒê√°p Lu·∫≠t Giao Th√¥ng ƒê∆∞·ªùng B·ªô VN")
st.caption(f"D·ª±a tr√™n c√°c vƒÉn b·∫£n Lu·∫≠t, Ngh·ªã ƒê·ªãnh, Th√¥ng t∆∞ v·ªÅ Lu·∫≠t giao th√¥ng ƒë∆∞·ªùng b·ªô Vi·ªát Nam.")

# --- Kh·ªüi t·∫°o h·ªá th·ªëng ---
init_ok = False
with st.status("ƒêang kh·ªüi t·∫°o h·ªá th·ªëng...", expanded=True) as status:
    st.write(f"T·∫£i embedding model: {config.embedding_model_name}...")
    g_embedding_model = utils.load_embedding_model(config.embedding_model_name)

    st.write(f"T·∫£i reranker model: {config.reranking_model_name}...")
    g_reranking_model = utils.load_reranker_model(config.reranking_model_name)

    st.write(f"C·∫•u h√¨nh Gemini model: {config.gemini_model_name}...")
    g_gemini_model = utils.load_gemini_model(config.gemini_model_name)

    models_loaded = all([g_embedding_model, g_reranking_model, g_gemini_model])

    st.write("Chu·∫©n b·ªã c∆° s·ªü d·ªØ li·ªáu v√† retriever...")
    g_vector_db, g_hybrid_retriever = cached_load_or_create_components(g_embedding_model)
    retriever_ready = g_hybrid_retriever is not None
    if retriever_ready:
        status.update(label="‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!", state="complete", expanded=False)
        init_ok = True
    else:
        status.update(label=" L·ªói Kh·ªüi T·∫°o!", state="error", expanded=True)
        if not retriever_ready: st.error("Kh√¥ng th·ªÉ chu·∫©n b·ªã c∆° s·ªü d·ªØ li·ªáu/retriever.")

# --- Ph·∫ßn t∆∞∆°ng t√°c ---
if init_ok:
    with st.form("query_form"):
        user_query = st.text_area("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:", height=100, placeholder="V√≠ d·ª•: M·ª©c ph·∫°t khi kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm?")
        submitted = st.form_submit_button("Tra c·ª©u üöÄ")

    if submitted and user_query:
        st.markdown("---")
        with st.spinner("‚è≥ ƒêang x·ª≠ l√Ω..."):
            start_time = time.time()

            # --- 1. Query Augmentation, Relevance Check & Direct Answer ---
            st.write(f"*{time.time() - start_time:.2f}s: Ph√¢n t√≠ch c√¢u h·ªèi...*")
            relevance_status, direct_answer, all_queries, summarizing_q = utils.generate_query_variations(
                user_query, g_gemini_model, num_variations=config.NUM_QUERY_VARIATIONS
            )

            # --- Ki·ªÉm tra m·ª©c ƒë·ªô li√™n quan ---
            if relevance_status == 'invalid':
                st.markdown("---")
                st.header("üìñ C√¢u tr·∫£ l·ªùi:")
                if direct_answer and direct_answer.strip():
                    st.markdown(direct_answer) # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi tr·ª±c ti·∫øp t·ª´ LLM
                else:
                    # Fallback n·∫øu LLM kh√¥ng t·∫°o c√¢u tr·∫£ l·ªùi tr·ª±c ti·∫øp
                    st.warning("‚ö†Ô∏è C√¢u h·ªèi c·ªßa b·∫°n c√≥ v·∫ª kh√¥ng li√™n quan ƒë·∫øn Lu·∫≠t Giao th√¥ng ƒê∆∞·ªùng b·ªô Vi·ªát Nam.")
                end_time_invalid = time.time()
                st.write(f"*{end_time_invalid - start_time:.2f}s: Ho√†n t·∫•t!*")
                st.stop() # D·ª´ng x·ª≠ l√Ω t·∫°i ƒë√¢y

            # 2. Hybrid Search
            st.write(f"*{time.time() - start_time:.2f}s: T√¨m ki·∫øm t√†i li·ªáu li√™n quan...*")
            collected_docs_data = {}
            for q_idx, query_variant in enumerate(all_queries):
                variant_results = g_hybrid_retriever.hybrid_search(
                    query_variant, g_embedding_model,
                    vector_search_k=config.VECTOR_K_PER_QUERY,
                    final_k=config.HYBRID_K_PER_QUERY
                )
                for item in variant_results:
                    doc_index = item['index']
                    if doc_index not in collected_docs_data:
                        collected_docs_data[doc_index] = {'doc': item['doc']}
            num_unique_docs = len(collected_docs_data)
            st.write(f"*{time.time() - start_time:.2f}s: T√¨m th·∫•y {num_unique_docs} t√†i li·ªáu ·ª©ng vi√™n.*")

            unique_docs_for_reranking_input = []
            if num_unique_docs > 0:
                unique_docs_for_reranking_input = [{'doc': data['doc'], 'index': idx}
                                                  for idx, data in collected_docs_data.items()]
                if len(unique_docs_for_reranking_input) > config.MAX_DOCS_FOR_RERANK:
                    unique_docs_for_reranking_input = unique_docs_for_reranking_input[:config.MAX_DOCS_FOR_RERANK]

            # 3. Re-ranking
            final_relevant_documents = []
            if unique_docs_for_reranking_input:
                st.write(f"*{time.time() - start_time:.2f}s: ƒê√°nh gi√° v√† x·∫øp h·∫°ng l·∫°i {len(unique_docs_for_reranking_input)} t√†i li·ªáu...*")
                reranked_results = utils.rerank_documents(
                    summarizing_q,
                    unique_docs_for_reranking_input,
                    g_reranking_model
                )
                final_relevant_documents = reranked_results[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
                st.write(f"*{time.time() - start_time:.2f}s: Ch·ªçn l·ªçc top {len(final_relevant_documents)} t√†i li·ªáu.*")

            # 4. Generate Answer
            final_answer = "..."
            if final_relevant_documents:
                st.write(f"*{time.time() - start_time:.2f}s: T·ªïng h·ª£p c√¢u tr·∫£ l·ªùi...*")
                final_answer = utils.generate_answer_with_gemini(
                    user_query,
                    final_relevant_documents,
                    g_gemini_model
                )
            else:
                st.write(f"*{time.time() - start_time:.2f}s: Kh√¥ng ƒë·ªß ng·ªØ c·∫£nh, ƒëang t·∫°o c√¢u tr·∫£ l·ªùi chung...*")
                final_answer = utils.generate_answer_with_gemini(user_query, [], g_gemini_model)

            end_time = time.time()
            st.write(f"*{end_time - start_time:.2f}s: Ho√†n t·∫•t!*")

        # --- Hi·ªÉn th·ªã K·∫øt qu·∫£ ---
        st.markdown("---")
        st.header("üìñ C√¢u tr·∫£ l·ªùi:")
        st.markdown(final_answer) 

        # --- Hi·ªÉn th·ªã ·∫¢nh (n·∫øu c√≥) ---
        if final_relevant_documents:
            st.markdown("---")
            
            with st.expander("Xem H√¨nh ·∫¢nh Bi·ªÉn B√°o Li√™n Quan (N·∫øu c√≥)"):
                displayed_images = set()
                image_found_in_context = False
                cols = st.columns(5) # Hi·ªÉn th·ªã t·ªëi ƒëa 5 ·∫£nh/h√†ng
                col_idx = 0
                for item in final_relevant_documents:
                    doc = item.get('doc')
                    if doc:
                        metadata = doc.get('metadata', {})
                        image_path = metadata.get('sign_image_path') 
                        sign_code = metadata.get('sign_code')

                        if image_path and image_path not in displayed_images:
                            
                            full_image_path = image_path 
                            # Ho·∫∑c full_image_path = os.path.join("images", os.path.basename(image_path))

                            if os.path.exists(full_image_path):
                                with cols[col_idx % 5]:
                                    st.image(full_image_path, caption=f"{sign_code}" if sign_code else None, use_column_width=True)
                                displayed_images.add(image_path)
                                image_found_in_context = True
                                col_idx += 1

                if not image_found_in_context:
                    st.write("_Kh√¥ng t√¨m th·∫•y h√¨nh ·∫£nh bi·ªÉn b√°o trong c√°c t√†i li·ªáu tham kh·∫£o._")

    elif submitted and not user_query:
        st.warning("ü§î Vui l√≤ng nh·∫≠p c√¢u h·ªèi!")

elif not init_ok:
    st.error("‚ö†Ô∏è H·ªá th·ªëng ch∆∞a th·ªÉ kh·ªüi ƒë·ªông do l·ªói. Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u h√¨nh v√† ƒë·∫£m b·∫£o c√≥ k·∫øt n·ªëi m·∫°ng ƒë·ªÉ t·∫£i model l·∫ßn ƒë·∫ßu.")