# app.py
import streamlit as st
import os
import logging
import time # C√≥ th·ªÉ d√πng ƒë·ªÉ th√™m delay n·∫øu x·ª≠ l√Ω qu√° nhanh

# Import c√°c th√†nh ph·∫ßn ƒë√£ t√°ch file
import config
import utils
import data_loader
from sentence_transformers import SentenceTransformer, CrossEncoder
import google.generativeai as genai
from kaggle_secrets import UserSecretsClient # Ch·ªâ d√πng n·∫øu ch·∫°y tr√™n Kaggle

# C·∫•u h√¨nh logging (ghi ra console khi ch·∫°y streamlit)
logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')

# --- C·∫•u h√¨nh Trang Streamlit ---
st.set_page_config(page_title="Chatbot Lu·∫≠t GTƒêB", layout="wide", initial_sidebar_state="collapsed")

# --- H√†m Cache ƒë·ªÉ t·∫£i Model ---
@st.cache_resource
def load_embedding_model(model_name):
    logging.info(f"CACHE MISS: Loading embedding model: {model_name}")
    try:
        model = SentenceTransformer(model_name)
        logging.info("Embedding model loaded successfully.")
        return model
    except Exception as e:
        st.error(f"L·ªói t·∫£i Embedding Model ({model_name}): {e}")
        return None

@st.cache_resource
def load_reranker_model(model_name):
    logging.info(f"CACHE MISS: Loading reranker model: {model_name}")
    try:
        model = CrossEncoder(model_name)
        logging.info("Reranker model loaded successfully.")
        return model
    except Exception as e:
        st.error(f"L·ªói t·∫£i Reranker Model ({model_name}): {e}")
        return None

@st.cache_resource
def load_gemini_model(model_name):
    logging.info(f"CACHE MISS: Loading/Configuring Gemini model: {model_name}")
    
    try:
        user_secrets = UserSecretsClient()
        google_api_key = user_secrets.get_secret("GOOGLE_API_KEY")
        source = "Kaggle secrets"
    except Exception: # Ngo·∫°i l·ªá chung n·∫øu kh√¥ng ·ªü trong Kaggle
        google_api_key = None
        source = "Kh√¥ng t√¨m th·∫•y"

    if google_api_key:
        logging.info(f"T√¨m th·∫•y Google API Key t·ª´: {source}")
        genai.configure(api_key=google_api_key)
        model = genai.GenerativeModel(model_name)
        logging.info("Gemini model configured successfully.")
        return model
    else:
        st.error("Kh√¥ng t√¨m th·∫•y GOOGLE_API_KEY trong Streamlit secrets ho·∫∑c Kaggle secrets.")
        logging.error("GOOGLE_API_KEY not found.")
        return None

# --- H√†m Cache ƒë·ªÉ Kh·ªüi t·∫°o DB v√† Retriever ---
@st.cache_resource
def cached_load_or_create_components(_embedding_model): # Th√™m _ ƒë·ªÉ streamlit bi·∫øt n√≥ ph·ª• thu·ªôc v√†o embedding model
    """Wrapper cho data_loader ƒë·ªÉ d√πng v·ªõi cache c·ªßa Streamlit."""
    if _embedding_model is None:
         st.error("Kh√¥ng th·ªÉ kh·ªüi t·∫°o DB/Retriever v√¨ Embedding Model l·ªói.")
         return None, None
    # G·ªçi h√†m x·ª≠ l√Ω ch√≠nh t·ª´ data_loader.py
    print(_embedding_model is not None)
    vector_db, hybrid_retriever = data_loader.load_or_create_rag_components(_embedding_model)
    print('aaaa')
    return vector_db, hybrid_retriever

# --- Giao di·ªán ch√≠nh c·ªßa ·ª®ng d·ª•ng ---
st.title("‚öñÔ∏è Chatbot H·ªèi ƒê√°p Lu·∫≠t Giao Th√¥ng ƒê∆∞·ªùng B·ªô VN")
st.caption(f"D·ª±a tr√™n QC41, TT36 (2024) v√† c√°c VB li√™n quan (hi·ªáu l·ª±c 2025). Model: {os.path.basename(config.embedding_model_name)}, {os.path.basename(config.reranking_model_name)}")

# --- Kh·ªüi t·∫°o h·ªá th·ªëng ---
init_ok = False
with st.status("ƒêang kh·ªüi t·∫°o h·ªá th·ªëng...", expanded=True) as status:
    st.write(f"T·∫£i embedding model: {config.embedding_model_name}...")
    g_embedding_model = load_embedding_model(config.embedding_model_name)

    st.write(f"T·∫£i reranker model: {config.reranking_model_name}...")
    g_reranking_model = load_reranker_model(config.reranking_model_name)

    st.write(f"T·∫£i/C·∫•u h√¨nh Gemini model: {config.gemini_model_name}...")
    g_gemini_model = load_gemini_model(config.gemini_model_name)

    models_loaded = all([g_embedding_model, g_reranking_model, g_gemini_model])

    st.write("Chu·∫©n b·ªã c∆° s·ªü d·ªØ li·ªáu v√† retriever...")
    g_vector_db, g_hybrid_retriever = None, None
    print(models_loaded)
    if models_loaded: 
        g_vector_db, g_hybrid_retriever = cached_load_or_create_components(g_embedding_model)
    retriever_ready = g_hybrid_retriever is not None
   
    if models_loaded and retriever_ready:
        status.update(label="‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!", state="complete", expanded=False)
        init_ok = True
    else:
        status.update(label=" L·ªói Kh·ªüi T·∫°o!", state="error", expanded=True)
        if not models_loaded: st.error("M·ªôt ho·∫∑c nhi·ªÅu m√¥ h√¨nh AI kh√¥ng th·ªÉ t·∫£i.")
        if not retriever_ready: st.error("Kh√¥ng th·ªÉ chu·∫©n b·ªã c∆° s·ªü d·ªØ li·ªáu/retriever.")

# --- Ph·∫ßn t∆∞∆°ng t√°c ---
if init_ok:
    # S·ª≠ d·ª•ng form ƒë·ªÉ nh√≥m input v√† button
    with st.form("query_form"):
        user_query = st.text_area("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:", height=100, placeholder="V√≠ d·ª•: M·ª©c ph·∫°t khi kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm?")
        submitted = st.form_submit_button("Tra c·ª©u üöÄ")

    if submitted and user_query:
        st.markdown("---")
        with st.spinner("‚è≥ ƒêang x·ª≠ l√Ω..."):
            start_time = time.time()

            # 1. Query Augmentation
            st.write(f"*{time.time() - start_time:.2f}s: M·ªü r·ªông c√¢u h·ªèi...*")
            all_queries, summarizing_q = utils.generate_query_variations(
                user_query, g_gemini_model, num_variations=config.NUM_QUERY_VARIATIONS
            )

            # 2. Hybrid Search
            st.write(f"*{time.time() - start_time:.2f}s: T√¨m ki·∫øm t√†i li·ªáu li√™n quan...*")
            collected_docs_data = {}
            for q_idx, query_variant in enumerate(all_queries):
                variant_results = g_hybrid_retriever.hybrid_search(
                    query_variant, g_embedding_model,
                    vector_search_k=config.VECTOR_K_PER_QUERY,
                    final_k=config.HYBRID_K_PER_QUERY
                )
                for item in variant_results:
                    doc_index = item['index']
                    if doc_index not in collected_docs_data:
                        collected_docs_data[doc_index] = {'doc': item['doc']}
            num_unique_docs = len(collected_docs_data)
            st.write(f"*{time.time() - start_time:.2f}s: T√¨m th·∫•y {num_unique_docs} t√†i li·ªáu ·ª©ng vi√™n.*")

            # Chu·∫©n b·ªã cho Re-rank
            unique_docs_for_reranking_input = []
            if num_unique_docs > 0:
                unique_docs_for_reranking_input = [{'doc': data['doc'], 'index': idx}
                                                  for idx, data in collected_docs_data.items()]
                if len(unique_docs_for_reranking_input) > config.MAX_DOCS_FOR_RERANK:
                    unique_docs_for_reranking_input = unique_docs_for_reranking_input[:config.MAX_DOCS_FOR_RERANK]

            # 3. Re-ranking
            final_relevant_documents = []
            if unique_docs_for_reranking_input:
                st.write(f"*{time.time() - start_time:.2f}s: ƒê√°nh gi√° v√† x·∫øp h·∫°ng l·∫°i {len(unique_docs_for_reranking_input)} t√†i li·ªáu...*")
                reranked_results = utils.rerank_documents(
                    summarizing_q,
                    unique_docs_for_reranking_input,
                    g_reranking_model
                )
                final_relevant_documents = reranked_results[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
                st.write(f"*{time.time() - start_time:.2f}s: Ch·ªçn l·ªçc top {len(final_relevant_documents)} t√†i li·ªáu.*")
            # else: st.write("   * Kh√¥ng c√≥ t√†i li·ªáu ƒë·ªÉ re-rank.") # Kh√¥ng c·∫ßn hi·ªÉn th·ªã

            # 4. Generate Answer
            final_answer = "..."
            if final_relevant_documents:
                st.write(f"*{time.time() - start_time:.2f}s: T·ªïng h·ª£p c√¢u tr·∫£ l·ªùi...*")
                final_answer = utils.generate_answer_with_gemini(
                    user_query,
                    final_relevant_documents,
                    g_gemini_model
                )
            else:
                st.write(f"*{time.time() - start_time:.2f}s: Kh√¥ng ƒë·ªß ng·ªØ c·∫£nh, ƒëang t·∫°o c√¢u tr·∫£ l·ªùi chung...*")
                # V·∫´n g·ªçi generate nh∆∞ng kh√¥ng c√≥ context
                final_answer = utils.generate_answer_with_gemini(user_query, [], g_gemini_model)

            end_time = time.time()
            st.write(f"*{end_time - start_time:.2f}s: Ho√†n t·∫•t!*")

        # --- Hi·ªÉn th·ªã K·∫øt qu·∫£ ---
        st.markdown("---")
        st.header("üìñ C√¢u tr·∫£ l·ªùi:")
        st.markdown(final_answer) # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi t·ª´ LLM

        # --- Hi·ªÉn th·ªã ·∫¢nh (n·∫øu c√≥) ---
        if final_relevant_documents:
            st.markdown("---")
            # S·ª≠ d·ª•ng expander ƒë·ªÉ kh√¥ng chi·∫øm nhi·ªÅu di·ªán t√≠ch n·∫øu kh√¥ng c·∫ßn
            with st.expander("Xem H√¨nh ·∫¢nh Bi·ªÉn B√°o Li√™n Quan (N·∫øu c√≥)"):
                displayed_images = set()
                image_found_in_context = False
                cols = st.columns(5) # Hi·ªÉn th·ªã t·ªëi ƒëa 5 ·∫£nh/h√†ng
                col_idx = 0
                for item in final_relevant_documents:
                    doc = item.get('doc')
                    if doc:
                        metadata = doc.get('metadata', {})
                        image_path = metadata.get('sign_image_path') # L·∫•y ƒë∆∞·ªùng d·∫´n ·∫£nh
                        sign_code = metadata.get('sign_code') # L·∫•y m√£ hi·ªáu bi·ªÉn b√°o

                        if image_path and image_path not in displayed_images:
                            # Quan tr·ªçng: ƒêi·ªÅu ch·ªânh ƒë∆∞·ªùng d·∫´n ·∫£nh n√†y cho ƒë√∫ng v·ªõi m√¥i tr∆∞·ªùng deploy
                            # V√≠ d·ª•: N·∫øu ·∫£nh n·∫±m trong th∆∞ m·ª•c 'images' c√πng c·∫•p app.py
                            full_image_path = image_path # Gi·∫£ s·ª≠ ƒë∆∞·ªùng d·∫´n ƒë√£ ƒë√∫ng
                            # Ho·∫∑c full_image_path = os.path.join("images", os.path.basename(image_path))

                            if os.path.exists(full_image_path):
                                with cols[col_idx % 5]:
                                    st.image(full_image_path, caption=f"{sign_code}" if sign_code else None, use_column_width=True)
                                displayed_images.add(image_path)
                                image_found_in_context = True
                                col_idx += 1
                            # else: print(f"·∫¢nh kh√¥ng t·ªìn t·∫°i: {full_image_path}") # Debug

                if not image_found_in_context:
                    st.write("_Kh√¥ng t√¨m th·∫•y h√¨nh ·∫£nh bi·ªÉn b√°o trong c√°c t√†i li·ªáu tham kh·∫£o._")

    elif submitted and not user_query:
        st.warning("ü§î Vui l√≤ng nh·∫≠p c√¢u h·ªèi!")

elif not init_ok:
    st.error("‚ö†Ô∏è H·ªá th·ªëng ch∆∞a th·ªÉ kh·ªüi ƒë·ªông do l·ªói. Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u h√¨nh v√† ƒë·∫£m b·∫£o c√≥ k·∫øt n·ªëi m·∫°ng ƒë·ªÉ t·∫£i model l·∫ßn ƒë·∫ßu.")