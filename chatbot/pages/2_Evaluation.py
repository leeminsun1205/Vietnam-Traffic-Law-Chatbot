# # pages/2_Evaluation.py

# import streamlit as st
# import pandas as pd
# import json
# import time
# import math
# import os
# import logging
# from datetime import datetime
# import sys
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# import config
# import utils
# import data_loader
# from vector_db import SimpleVectorDatabase # C·∫ßn l·ªõp n√†y ƒë·ªÉ ki·ªÉm tra ki·ªÉu
# from retriever import HybridRetriever     # C·∫ßn l·ªõp n√†y ƒë·ªÉ ki·ªÉm tra ki·ªÉu

# # --- C·∫•u h√¨nh Logging ---
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# # --- C√°c h√†m t√≠nh Metrics (Gi·ªëng trong notebook) ---
# def precision_at_k(retrieved_ids, relevant_ids, k):
#     if k <= 0: return 0.0
#     retrieved_at_k = retrieved_ids[:k]; relevant_set = set(relevant_ids)
#     if not relevant_set: return 0.0
#     intersect = set(retrieved_at_k) & relevant_set
#     return len(intersect) / k

# def recall_at_k(retrieved_ids, relevant_ids, k):
#     relevant_set = set(relevant_ids)
#     if not relevant_set: return 1.0
#     retrieved_at_k = retrieved_ids[:k]
#     intersect = set(retrieved_at_k) & relevant_set
#     return len(intersect) / len(relevant_set)

# def f1_at_k(retrieved_ids, relevant_ids, k):
#     prec = precision_at_k(retrieved_ids, relevant_ids, k); rec = recall_at_k(retrieved_ids, relevant_ids, k)
#     return 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0

# def mrr_at_k(retrieved_ids, relevant_ids, k):
#     relevant_set = set(relevant_ids);
#     if not relevant_set: return 0.0
#     for rank, doc_id in enumerate(retrieved_ids[:k], 1):
#         if doc_id in relevant_set: return 1.0 / rank
#     return 0.0

# def ndcg_at_k(retrieved_ids, relevant_ids, k):
#     relevant_set = set(relevant_ids);
#     if not relevant_set: return 1.0
#     retrieved_at_k = retrieved_ids[:k]; dcg = 0.0; idcg = 0.0
#     for i, doc_id in enumerate(retrieved_at_k):
#         if doc_id in relevant_set: dcg += 1.0 / math.log2(i + 2)
#     for i in range(min(k, len(relevant_set))): idcg += 1.0 / math.log2(i + 2)
#     return dcg / idcg if idcg > 0 else 0.0

# # --- H√†m th·ª±c thi l√µi ƒë√°nh gi√° Retrieval ---
# def run_retrieval_evaluation(
#     eval_data: list,
#     hybrid_retriever: HybridRetriever,
#     embedding_model, # Model ƒë√£ load
#     reranking_model, # Model ƒë√£ load
#     gemini_model,    # Model ƒë√£ load (c√≥ th·ªÉ None)
#     eval_config: dict # Ch·ª©a c√°c t√πy ch·ªçn nh∆∞ retrieval_mode, use_history,...
#     ):
    
#     results_list = []
#     k_values = [3, 5, 10] # K values for evaluation metrics

#     # L·∫•y c·∫•u h√¨nh t·ª´ dict
#     retrieval_mode = eval_config.get('retrieval_mode', 'ƒê∆°n gi·∫£n') # Default n·∫øu thi·∫øu
#     use_history = eval_config.get('use_history_for_llm1', False)
#     dummy_history = [{"role": "user", "content": "..."}] if use_history else None # Simple dummy history

#     # Placeholder cho progress bar
#     progress_bar = st.progress(0)
#     status_text = st.empty()

#     total_items = len(eval_data)
#     for i, item in enumerate(eval_data):
#         query_id = item.get("query_id"); original_query = item.get("query")
#         relevant_chunk_ids = set(item.get("relevant_chunk_ids", []))
#         if not query_id or not original_query: continue # B·ªè qua item kh√¥ng h·ª£p l·ªá

#         status_text.text(f"ƒêang x·ª≠ l√Ω query {i+1}/{total_items}: {query_id}")
#         logging.info(f"Eval - Processing QID: {query_id}")

#         start_time = time.time()
#         query_metrics = {
#             "query_id": query_id, "query": original_query, "retrieval_mode": retrieval_mode,
#             "status": "error", "retrieved_ids": [], "relevant_ids": list(relevant_chunk_ids),
#             "processing_time": 0.0
#         }
#         # Init metrics columns
#         for k in k_values: query_metrics[f'precision@{k}'] = 0.0; query_metrics[f'recall@{k}'] = 0.0; query_metrics[f'f1@{k}'] = 0.0; query_metrics[f'mrr@{k}'] = 0.0; query_metrics[f'ndcg@{k}'] = 0.0
#         timing_keys = ['variation_time', 'search_time', 'rerank_time']
#         count_keys = ['num_variations_generated', 'num_unique_docs_found', 'num_docs_reranked']
#         for k in timing_keys + count_keys: query_metrics[k] = 0.0

#         try:
#             # 1. Generate Variations / Check Relevance
#             variation_start = time.time()
#             relevance_status, _, all_queries, summarizing_query = utils.generate_query_variations(
#                 original_query=original_query,
#                 gemini_model=gemini_model,
#                 chat_history=dummy_history
#             )
#             query_metrics["variation_time"] = time.time() - variation_start
#             query_metrics["summarizing_query"] = summarizing_query
#             query_metrics["num_variations_generated"] = len(all_queries)

#             if relevance_status == 'invalid':
#                 query_metrics["status"] = "skipped_irrelevant"
#                 query_metrics["processing_time"] = time.time() - start_time
#                 results_list.append(query_metrics)
#                 progress_bar.progress((i + 1) / total_items) # C·∫≠p nh·∫≠t progress bar
#                 continue

#             # 2. Retrieval based on mode
#             collected_docs_data = {}
#             search_start = time.time()
#             if retrieval_mode == 'ƒê∆°n gi·∫£n':
#                 variant_results = hybrid_retriever.hybrid_search(
#                     summarizing_query, embedding_model,
#                     vector_search_k=config.VECTOR_K_PER_QUERY, final_k=config.HYBRID_K_PER_QUERY
#                 )
#                 for res_item in variant_results:
#                     idx = res_item.get('index');
#                     if isinstance(idx, int) and idx >= 0: collected_docs_data[idx] = res_item
#             elif retrieval_mode == 'S√¢u':
#                 for q_variant in all_queries:
#                     variant_results = hybrid_retriever.hybrid_search(
#                         q_variant, embedding_model,
#                         vector_search_k=config.VECTOR_K_PER_QUERY, final_k=config.HYBRID_K_PER_QUERY
#                     )
#                     for res_item in variant_results:
#                         idx = res_item.get('index')
#                         if isinstance(idx, int) and idx >= 0 and idx not in collected_docs_data:
#                              collected_docs_data[idx] = res_item
#             query_metrics["search_time"] = time.time() - search_start
#             query_metrics["num_unique_docs_found"] = len(collected_docs_data)

#             # 3. Prepare for Reranking
#             unique_docs_list = list(collected_docs_data.values())
#             unique_docs_list.sort(key=lambda x: x.get('hybrid_score', 0.0), reverse=True)
#             docs_for_reranking_input = unique_docs_list[:config.MAX_DOCS_FOR_RERANK]
#             query_metrics["num_docs_reranked"] = len(docs_for_reranking_input)

#             # 4. Reranking
#             rerank_start = time.time()
#             reranked_results = utils.rerank_documents(
#                 summarizing_query, docs_for_reranking_input, reranking_model
#             )
#             query_metrics["rerank_time"] = time.time() - rerank_start

#             # 5. Get Final Retrieved IDs
#             final_retrieved_docs = reranked_results[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
#             retrieved_ids = []
#             for res in final_retrieved_docs:
#                 doc_data = res.get('doc', {}); chunk_id = None
#                 if isinstance(doc_data, dict):
#                     chunk_id = doc_data.get('id') or doc_data.get('metadata', {}).get('id') or doc_data.get('metadata', {}).get('chunk_id')
#                 if chunk_id: retrieved_ids.append(str(chunk_id))
#             query_metrics["retrieved_ids"] = retrieved_ids

#             # 6. Calculate Metrics
#             query_metrics["status"] = "evaluated"
#             for k in k_values:
#                 query_metrics[f'precision@{k}'] = precision_at_k(retrieved_ids, relevant_chunk_ids, k)
#                 query_metrics[f'recall@{k}'] = recall_at_k(retrieved_ids, relevant_chunk_ids, k)
#                 query_metrics[f'f1@{k}'] = f1_at_k(retrieved_ids, relevant_chunk_ids, k)
#                 query_metrics[f'mrr@{k}'] = mrr_at_k(retrieved_ids, relevant_chunk_ids, k)
#                 query_metrics[f'ndcg@{k}'] = ndcg_at_k(retrieved_ids, relevant_chunk_ids, k)

#         except Exception as e:
#             logging.exception(f"Error evaluating QID {query_id}: {e}")
#             query_metrics["status"] = "error_runtime"
#             query_metrics["error_message"] = str(e)
#         finally:
#             query_metrics["processing_time"] = time.time() - start_time
#             results_list.append(query_metrics)
#             progress_bar.progress((i + 1) / total_items) # C·∫≠p nh·∫≠t progress bar

#     status_text.text(f"Ho√†n th√†nh ƒë√°nh gi√° {total_items} queries!")
#     return pd.DataFrame(results_list)

# # --- H√†m t√≠nh to√°n t·ªïng h·ª£p Metrics ---
# def calculate_average_metrics(df_results: pd.DataFrame):
#     """T√≠nh to√°n metrics trung b√¨nh t·ª´ DataFrame k·∫øt qu·∫£ chi ti·∫øt."""
#     evaluated_df = df_results[df_results['status'] == 'evaluated']
#     num_evaluated = len(evaluated_df)
#     if num_evaluated == 0:
#         return None, num_evaluated, len(df_results) - num_evaluated

#     avg_metrics = {}
#     k_values = [3, 5, 10]
#     metric_keys_k = [f'{m}@{k}' for k in k_values for m in ['precision', 'recall', 'f1', 'mrr', 'ndcg']]
#     timing_keys = ['processing_time', 'variation_time', 'search_time', 'rerank_time']
#     count_keys = ['num_variations_generated', 'num_unique_docs_found', 'num_docs_reranked']

#     for key in metric_keys_k + timing_keys + count_keys:
#         # T√≠nh t·ªïng, b·ªè qua NaN n·∫øu c√≥
#         total = evaluated_df[key].sum(skipna=True)
#         avg_metrics[f'avg_{key}'] = total / num_evaluated

#     return avg_metrics, num_evaluated, len(df_results) - num_evaluated


# # --- Giao di·ªán Streamlit cho Trang ƒê√°nh gi√° ---
# st.set_page_config(page_title="ƒê√°nh gi√° Retrieval", layout="wide")
# st.title("üìä ƒê√°nh gi√° H·ªá th·ªëng Retrieval")

# st.markdown("""
# Trang n√†y cho ph√©p b·∫°n ch·∫°y ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa h·ªá th·ªëng retrieval (t√¨m ki·∫øm + x·∫øp h·∫°ng l·∫°i)
# d·ª±a tr√™n m·ªôt t·∫≠p d·ªØ li·ªáu c√≥ ch·ª©a c√°c c√¢u h·ªèi v√† c√°c chunk t√†i li·ªáu li√™n quan (ground truth).
# """)

# # --- Ki·ªÉm tra v√† Hi·ªÉn th·ªã Tr·∫°ng th√°i H·ªá th·ªëng ---
# st.subheader("Tr·∫°ng th√°i H·ªá th·ªëng C∆° b·∫£n")
# init_ok = False
# models_ready = False
# retriever_instance = None

# # C·ªë g·∫Øng l·∫•y c√°c th√†nh ph·∫ßn ƒë√£ cache t·ª´ app ch√≠nh (c√≥ th·ªÉ c·∫ßn ƒëi·ªÅu ch·ªânh c√°ch truy c·∫≠p)
# # Streamlit kh√¥ng d·ªÖ d√†ng chia s·∫ª @st.cache_resource gi·ªØa c√°c trang m·ªôt c√°ch tr·ª±c ti·∫øp.
# # C√°ch ti·∫øp c·∫≠n ƒë∆°n gi·∫£n l√† t·∫£i l·∫°i tr√™n trang n√†y n·∫øu c·∫ßn.
# # Ho·∫∑c l∆∞u tr·ªØ instance v√†o st.session_state trong app.py n·∫øu c√≥ th·ªÉ.

# # T·∫°m th·ªùi, ch√∫ng ta s·∫Ω t·∫£i l·∫°i model v√† retriever tr√™n trang n√†y khi c·∫ßn ƒë√°nh gi√°.
# # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o trang ƒë√°nh gi√° ho·∫°t ƒë·ªông ƒë·ªôc l·∫≠p h∆°n nh∆∞ng kh√¥ng t·∫≠n d·ª•ng cache t·ª´ trang ch√≠nh.

# with st.spinner("Ki·ªÉm tra v√† kh·ªüi t·∫°o t√†i nguy√™n..."):
#     # T·∫£i models (s·∫Ω d√πng cache n·ªôi b·ªô c·ªßa h√†m n·∫øu ch∆∞a force reload)
#     g_embedding_model = utils.load_embedding_model(config.embedding_model_name)
#     g_reranking_model = utils.load_reranker_model(config.reranking_model_name)

#     # T·∫£i vector DB v√† retriever
#     try:
#         # S·ª≠ d·ª•ng h√†m g·ªëc t·ª´ data_loader ƒë·ªÉ ƒë·∫£m b·∫£o nh·∫•t qu√°n
#         # H√†m n√†y c≈©ng c√≥ cache ri√™ng (@st.cache_resource trong app.py)
#         # nh∆∞ng g·ªçi l·∫°i ·ªü ƒë√¢y s·∫Ω t·∫°o instance m·ªõi n·∫øu kh√¥ng c√≥ cache session
#         # TODO: T√¨m c√°ch chia s·∫ª instance t·ª´ app.py hi·ªáu qu·∫£ h∆°n n·∫øu c·∫ßn t·ªëi ∆∞u
#         # Hi·ªán t·∫°i, t·∫£i l·∫°i ƒë·ªÉ ƒë·∫£m b·∫£o trang ƒë·ªôc l·∫≠p
#         vector_db, retriever_instance = data_loader.load_or_create_rag_components(g_embedding_model)
#         if vector_db and retriever_instance and g_embedding_model and g_reranking_model:
#             init_ok = True
#             models_ready = True # Assume Gemini load happens later
#             st.success("‚úÖ VectorDB, Retriever, Embedding, Reranker ƒë√£ s·∫µn s√†ng.")
#         else:
#             st.error("‚ö†Ô∏è L·ªói khi kh·ªüi t·∫°o VectorDB ho·∫∑c Retriever.")
#     except Exception as e:
#         st.error(f"‚ö†Ô∏è L·ªói nghi√™m tr·ªçng khi kh·ªüi t·∫°o h·ªá th·ªëng: {e}")

# if init_ok:
#     st.subheader("C·∫•u h√¨nh ƒê√°nh gi√°")
#     st.markdown("ƒê√°nh gi√° s·∫Ω ƒë∆∞·ª£c ch·∫°y v·ªõi c√°c c·∫•u h√¨nh **hi·ªán t·∫°i** ƒë∆∞·ª£c ch·ªçn trong Sidebar c·ªßa trang Chatbot ch√≠nh.")

#     # L·∫•y c·∫•u h√¨nh hi·ªán t·∫°i t·ª´ session_state (ƒë∆∞·ª£c thi·∫øt l·∫≠p ·ªü app.py)
#     current_gemini_model = st.session_state.get('selected_gemini_model', config.DEFAULT_GEMINI_MODEL)
#     current_retrieval_mode = st.session_state.get('retrieval_mode', 'ƒê∆°n gi·∫£n')
#     current_use_history = st.session_state.get('use_history_for_llm1', True)

#     col1, col2, col3 = st.columns(3)
#     with col1:
#         st.info(f"**Ch·∫ø ƒë·ªô Retrieval:** `{current_retrieval_mode}`")
#     with col2:
#         st.info(f"**Model Gemini (Variations):** `{current_gemini_model}`")
#     with col3:
#         st.info(f"**S·ª≠ d·ª•ng L·ªãch s·ª≠ (LLM1):** `{current_use_history}`")

#     eval_config_dict = {
#         'retrieval_mode': current_retrieval_mode,
#         'use_history_for_llm1': current_use_history,
#         # Th√™m c√°c config kh√°c n·∫øu c·∫ßn
#     }

#     st.subheader("T·∫£i L√™n File ƒê√°nh gi√°")
#     uploaded_file = st.file_uploader(
#         "Ch·ªçn file JSON ch·ª©a d·ªØ li·ªáu ƒë√°nh gi√° (ƒë·ªãnh d·∫°ng: [{'query_id': ..., 'query': ..., 'relevant_chunk_ids': [...]}, ...])",
#         type=["json"]
#     )

#     if uploaded_file is not None:
#         try:
#             # ƒê·ªçc n·ªôi dung file
#             eval_data = json.load(uploaded_file)
#             st.success(f"ƒê√£ t·∫£i l√™n th√†nh c√¥ng file '{uploaded_file.name}' ch·ª©a {len(eval_data)} c√¢u h·ªèi.")

#             # Hi·ªÉn th·ªã m·ªôt ph·∫ßn d·ªØ li·ªáu ƒë·ªÉ ki·ªÉm tra
#             if st.checkbox("Hi·ªÉn th·ªã d·ªØ li·ªáu m·∫´u (5 d√≤ng ƒë·∫ßu)"):
#                 st.dataframe(pd.DataFrame(eval_data).head())

#             # N√∫t b·∫Øt ƒë·∫ßu ƒë√°nh gi√°
#             if st.button("üöÄ B·∫Øt ƒë·∫ßu ƒê√°nh gi√°", key="start_eval_button"):
#                 # T·∫£i model Gemini ƒë∆∞·ª£c ch·ªçn
#                 with st.spinner(f"ƒêang t·∫£i model Gemini: {current_gemini_model}..."):
#                     g_gemini_model = utils.load_gemini_model(current_gemini_model) # D√πng h√†m g·ªëc ƒë·ªÉ t·∫£i

#                 if not g_gemini_model and config.GOOGLE_API_KEY: # Ch·ªâ b√°o l·ªói n·∫øu c√≥ key m√† t·∫£i th·∫•t b·∫°i
#                      st.error(f"L·ªói t·∫£i model Gemini '{current_gemini_model}'. Vui l√≤ng ki·ªÉm tra API Key v√† c·∫•u h√¨nh.")
#                 elif not config.GOOGLE_API_KEY:
#                      st.warning("Kh√¥ng t√¨m th·∫•y Google API Key. B∆∞·ªõc t·∫°o bi·∫øn th·ªÉ/ki·ªÉm tra li√™n quan s·∫Ω b·ªã b·ªè qua.")
#                      # ƒê√°nh gi√° v·∫´n c√≥ th·ªÉ ch·∫°y n·∫øu kh√¥ng c·∫ßn b∆∞·ªõc n√†y

#                 # Ch·∫°y ƒë√°nh gi√°
#                 with st.spinner("‚è≥ ƒêang ch·∫°y ƒë√°nh gi√°... Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t."):
#                     start_eval_time = time.time()
#                     detailed_results_df = run_retrieval_evaluation(
#                         eval_data=eval_data,
#                         hybrid_retriever=retriever_instance,
#                         embedding_model=g_embedding_model,
#                         reranking_model=g_reranking_model,
#                         gemini_model=g_gemini_model,
#                         eval_config=eval_config_dict
#                     )
#                     total_eval_time = time.time() - start_eval_time
#                     st.info(f"Ho√†n th√†nh ƒë√°nh gi√° sau {total_eval_time:.2f} gi√¢y.")

#                 # --- Hi·ªÉn th·ªã K·∫øt qu·∫£ ---
#                 st.subheader("K·∫øt qu·∫£ ƒê√°nh gi√°")

#                 # T√≠nh to√°n metrics trung b√¨nh
#                 avg_metrics, num_eval, num_skipped_error = calculate_average_metrics(detailed_results_df)

#                 st.metric("T·ªïng s·ªë Queries", len(detailed_results_df))
#                 col_res1, col_res2, col_res3 = st.columns(3)
#                 col_res1.metric("Queries ƒê√°nh gi√° H·ª£p l·ªá", num_eval)
#                 col_res2.metric("Queries B·ªè qua/L·ªói", num_skipped_error)

#                 if avg_metrics:
#                     st.markdown("#### Metrics Trung b√¨nh (tr√™n c√°c queries h·ª£p l·ªá)")
#                     k_values_display = [3, 5, 10]
#                     cols_k = st.columns(len(k_values_display))
#                     for idx, k in enumerate(k_values_display):
#                         with cols_k[idx]:
#                             st.markdown(f"**K = {k}**")
#                             st.text(f"Precision: {avg_metrics.get(f'avg_precision@{k}', 0.0):.4f}")
#                             st.text(f"Recall:    {avg_metrics.get(f'avg_recall@{k}', 0.0):.4f}")
#                             st.text(f"F1:        {avg_metrics.get(f'avg_f1@{k}', 0.0):.4f}")
#                             st.text(f"MRR:       {avg_metrics.get(f'avg_mrr@{k}', 0.0):.4f}")
#                             st.text(f"NDCG:      {avg_metrics.get(f'avg_ndcg@{k}', 0.0):.4f}")

#                     st.markdown("#### Th√¥ng tin Hi·ªáu nƒÉng Trung b√¨nh")
#                     col_perf1, col_perf2, col_perf3 = st.columns(3)
#                     col_perf1.metric("Avg Total Time/Query (s)", f"{avg_metrics.get('avg_processing_time', 0.0):.3f}")
#                     col_perf2.metric("Avg Variation Time (s)", f"{avg_metrics.get('avg_variation_time', 0.0):.3f}")
#                     col_perf3.metric("Avg Search Time (s)", f"{avg_metrics.get('avg_search_time', 0.0):.3f}")
#                     # C√≥ th·ªÉ th√™m c√°c timing kh√°c n·∫øu mu·ªën

#                 # Hi·ªÉn th·ªã b·∫£ng k·∫øt qu·∫£ chi ti·∫øt
#                 with st.expander("Xem K·∫øt qu·∫£ Chi ti·∫øt cho t·ª´ng Query"):
#                     # Ch·ªçn c√°c c·ªôt c·∫ßn hi·ªÉn th·ªã ƒë·ªÉ b·∫£ng g·ªçn h∆°n
#                     display_columns = [
#                         'query_id', 'query', 'status', 'retrieval_mode',
#                         'precision@3', 'recall@10', 'mrr@10', 'ndcg@10',
#                         'processing_time', 'retrieved_ids' # Gi·ªØ l·∫°i retrieved_ids ƒë·ªÉ tham kh·∫£o
#                     ]
#                     # L·ªçc c√°c c·ªôt t·ªìn t·∫°i trong df
#                     existing_display_columns = [col for col in display_columns if col in detailed_results_df.columns]
#                     st.dataframe(detailed_results_df[existing_display_columns])

#                 # --- L∆∞u K·∫øt qu·∫£ ---
#                 st.subheader("L∆∞u K·∫øt qu·∫£ Chi ti·∫øt")
#                 try:
#                     # Chuy·ªÉn DataFrame th√†nh JSON lines ho·∫∑c CSV ƒë·ªÉ t·∫£i v·ªÅ
#                     results_json = detailed_results_df.to_json(orient='records', indent=2, force_ascii=False)
#                     results_csv = detailed_results_df.to_csv(index=False).encode('utf-8')

#                     # T·∫°o t√™n file ƒë·ªông
#                     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#                     mode_suffix = current_retrieval_mode.lower().replace(' ', '_')
#                     fname_json = f"evaluation_results_{mode_suffix}_{timestamp}.json"
#                     fname_csv = f"evaluation_results_{mode_suffix}_{timestamp}.csv"

#                     col_dl1, col_dl2 = st.columns(2)
#                     with col_dl1:
#                         st.download_button(
#                             label="üíæ T·∫£i v·ªÅ JSON",
#                             data=results_json,
#                             file_name=fname_json,
#                             mime="application/json",
#                         )
#                     with col_dl2:
#                         st.download_button(
#                             label="üíæ T·∫£i v·ªÅ CSV",
#                             data=results_csv,
#                             file_name=fname_csv,
#                             mime="text/csv",
#                         )
#                 except Exception as e:
#                     st.error(f"L·ªói khi chu·∫©n b·ªã file ƒë·ªÉ t·∫£i v·ªÅ: {e}")

#         except json.JSONDecodeError:
#             st.error("L·ªói: File t·∫£i l√™n kh√¥ng ph·∫£i l√† ƒë·ªãnh d·∫°ng JSON h·ª£p l·ªá.")
#         except Exception as e:
#             st.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi x·ª≠ l√Ω file ho·∫∑c ch·∫°y ƒë√°nh gi√°: {e}")
#             logging.exception("Unhandled error during evaluation page processing.")

# else:
#     st.warning("‚ö†Ô∏è H·ªá th·ªëng c∆° b·∫£n ch∆∞a s·∫µn s√†ng. Vui l√≤ng ki·ªÉm tra l·∫°i trang Chatbot ch√≠nh ho·∫∑c kh·ªüi ƒë·ªông l·∫°i ·ª©ng d·ª•ng.")