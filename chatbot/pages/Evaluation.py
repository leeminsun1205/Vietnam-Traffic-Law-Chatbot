# pages/2_Evaluation.py
import time
import streamlit as st
import pandas as pd
import json
import time
import math
import os
import logging
from datetime import datetime
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import config
import utils
import data_loader
from retriever import HybridRetriever

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- C√°c h√†m t√≠nh to√°n metrics (gi·ªØ nguy√™n) ---
def precision_at_k(retrieved_ids, relevant_ids, k):
    if k <= 0: return 0.0
    retrieved_at_k = retrieved_ids[:k]; relevant_set = set(relevant_ids)
    if not relevant_set: return 0.0 # N·∫øu kh√¥ng c√≥ relevant th√¨ precision l√† 0
    intersect = set(retrieved_at_k) & relevant_set
    return len(intersect) / k

def recall_at_k(retrieved_ids, relevant_ids, k):
    relevant_set = set(relevant_ids)
    if not relevant_set: return 1.0 # N·∫øu kh√¥ng c√≥ relevant th√¨ coi nh∆∞ ƒë√£ t√¨m th·∫•y t·∫•t c·∫£ (recall = 1)
    retrieved_at_k = retrieved_ids[:k]
    intersect = set(retrieved_at_k) & relevant_set
    return len(intersect) / len(relevant_set)

def f1_at_k(retrieved_ids, relevant_ids, k):
    prec = precision_at_k(retrieved_ids, relevant_ids, k); rec = recall_at_k(retrieved_ids, relevant_ids, k)
    return 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0

def mrr_at_k(retrieved_ids, relevant_ids, k):
    relevant_set = set(relevant_ids);
    if not relevant_set: return 0.0 # N·∫øu kh√¥ng c√≥ relevant th√¨ MRR l√† 0
    retrieved_at_k = retrieved_ids[:k] # Ch·ªâ x√©t top K
    for rank, doc_id in enumerate(retrieved_at_k, 1):
        if doc_id in relevant_set: return 1.0 / rank
    return 0.0

def ndcg_at_k(retrieved_ids, relevant_ids, k):
    relevant_set = set(relevant_ids);
    if not relevant_set: return 1.0 # N·∫øu kh√¥ng c√≥ relevant th√¨ coi nh∆∞ list tr·∫£ v·ªÅ l√† ho√†n h·∫£o (NDCG=1)
    retrieved_at_k = retrieved_ids[:k]; dcg = 0.0; idcg = 0.0
    # Calculate DCG@k
    for i, doc_id in enumerate(retrieved_at_k):
        # Gi·∫£ s·ª≠ relevancy l√† 1 n·∫øu doc_id n·∫±m trong relevant_set, ng∆∞·ª£c l·∫°i l√† 0
        relevance = 1.0 if doc_id in relevant_set else 0.0
        dcg += relevance / math.log2(i + 2) # i+2 v√¨ rank b·∫Øt ƒë·∫ßu t·ª´ 1 (log2(1+1))
    # Calculate IDCG@k
    num_relevant_in_total = len(relevant_set)
    # IDCG ƒë∆∞·ª£c t√≠nh b·∫±ng c√°ch gi·∫£ s·ª≠ c√°c t√†i li·ªáu relevant nh·∫•t n·∫±m ·ªü ƒë·∫ßu danh s√°ch
    for i in range(min(k, num_relevant_in_total)):
        idcg += 1.0 / math.log2(i + 2)
    return dcg / idcg if idcg > 0 else 0.0


def run_retrieval_evaluation(
    eval_data: list,
    hybrid_retriever: HybridRetriever,
    embedding_model,
    reranking_model, # C√≥ th·ªÉ l√† None n·∫øu kh√¥ng d√πng rerank
    gemini_model,
    eval_config: dict # Ch·ª©a retrieval_query_mode, retrieval_method, use_reranker, use_history_for_llm1
    ):

    results_list = []
    k_values = [1, 3, 5, 10] # C√°c gi√° tr·ªã K ƒë·ªÉ t√≠nh metrics

    # --- L·∫•y c·∫•u h√¨nh t·ª´ eval_config ---
    retrieval_query_mode = eval_config.get('retrieval_query_mode', 'T·ªïng qu√°t')
    retrieval_method = eval_config.get('retrieval_method', 'hybrid')
    use_reranker = eval_config.get('use_reranker', True)
    use_history_llm1 = eval_config.get('use_history_for_llm1', True) # S·ª≠a key
    dummy_history = [{"role": "user", "content": "..."}] if use_history_llm1 else None

    progress_bar = st.progress(0)
    status_text = st.empty()

    total_items = len(eval_data)
    queries_per_batch = 15 # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng query tr∆∞·ªõc khi t·∫°m d·ª´ng
    wait_time_seconds = 60 # Th·ªùi gian t·∫°m d·ª´ng

    for i, item in enumerate(eval_data):
        # T·∫°m d·ª´ng sau m·ªói batch
        if i > 0 and i % queries_per_batch == 0:
            pause_msg = f"ƒê√£ x·ª≠ l√Ω {i}/{total_items} queries. T·∫°m d·ª´ng {wait_time_seconds} gi√¢y..."
            logging.info(pause_msg)
            status_text.text(pause_msg)
            time.sleep(wait_time_seconds)
            status_text.text(f"Ti·∫øp t·ª•c x·ª≠ l√Ω query {i+1}/{total_items}...")

        query_id = item.get("query_id"); original_query = item.get("query")
        relevant_chunk_ids = set(item.get("relevant_chunk_ids", []))
        if not query_id or not original_query:
            logging.warning(f"B·ªè qua m·ª•c {i} do thi·∫øu query_id ho·∫∑c query.")
            continue

        status_text.text(f"ƒêang x·ª≠ l√Ω query {i+1}/{total_items}: {query_id} (QueryMode: {retrieval_query_mode}, Method: {retrieval_method}, Rerank: {use_reranker})")
        logging.info(f"Eval - Processing QID: {query_id} (QueryMode: {retrieval_query_mode}, Method: {retrieval_method}, Rerank: {use_reranker})")

        start_time = time.time()
        # --- Kh·ªüi t·∫°o query_metrics v·ªõi c√°c tr∆∞·ªùng c·∫•u h√¨nh ---
        query_metrics = {
            "query_id": query_id, "query": original_query,
            "retrieval_query_mode": retrieval_query_mode,
            "retrieval_method": retrieval_method,
            "use_reranker": use_reranker,
            "use_history_llm1": use_history_llm1,
            "status": "error", "retrieved_ids": [], "relevant_ids": list(relevant_chunk_ids),
            "processing_time": 0.0, 'summarizing_query': '',
            'variation_time': 0.0, 'search_time': 0.0, 'rerank_time': 0.0,
            'num_variations_generated': 0, 'num_unique_docs_found': 0, 'num_docs_reranked': 0,
            'num_retrieved_before_rerank': 0, 'num_retrieved_after_rerank': 0
        }
        for k in k_values:
            query_metrics[f'precision@{k}'] = 0.0; query_metrics[f'recall@{k}'] = 0.0
            query_metrics[f'f1@{k}'] = 0.0; query_metrics[f'mrr@{k}'] = 0.0; query_metrics[f'ndcg@{k}'] = 0.0

        try:
            # B∆∞·ªõc 1: T·∫°o variations/summarizing query (lu√¥n ch·∫°y)
            variation_start = time.time()
            relevance_status, _, all_queries, summarizing_query = utils.generate_query_variations(
                original_query=original_query, gemini_model=gemini_model,
                chat_history=dummy_history, num_variations=config.NUM_QUERY_VARIATIONS
            )
            query_metrics["variation_time"] = time.time() - variation_start
            query_metrics["summarizing_query"] = summarizing_query
            query_metrics["num_variations_generated"] = len(all_queries) - 1

            if relevance_status == 'invalid':
                query_metrics["status"] = "skipped_irrelevant"
                # C√°c metrics kh√°c gi·ªØ nguy√™n gi√° tr·ªã 0
                query_metrics["processing_time"] = time.time() - start_time
                results_list.append(query_metrics)
                progress_bar.progress((i + 1) / total_items)
                logging.info(f"QID {query_id} skipped as irrelevant.")
                continue

            # --- B∆∞·ªõc 2: X√°c ƒë·ªãnh query(s) ƒë·ªÉ t√¨m ki·∫øm ---
            queries_to_search = []
            st.write(queries_to_search)
            if retrieval_query_mode == 'ƒê∆°n gi·∫£n': queries_to_search = [original_query]
            elif retrieval_query_mode == 'T·ªïng qu√°t': queries_to_search = [summarizing_query]
            elif retrieval_query_mode == 'S√¢u': queries_to_search = all_queries

            # --- B∆∞·ªõc 3: Th·ª±c hi·ªán Retrieval ---
            collected_docs_data = {}
            search_start = time.time()
            st.write(queries_to_search)
            for q_variant in queries_to_search:
                if not q_variant: continue # B·ªè qua n·∫øu query r·ªóng
                # G·ªçi h√†m search m·ªõi c·ªßa retriever
                search_results = hybrid_retriever.search(
                    q_variant, embedding_model,
                    method=retrieval_method,
                    k=config.VECTOR_K_PER_QUERY # L·∫•y K ƒë·ªß l·ªõn cho b∆∞·ªõc sau
                )
                for item in search_results:
                    doc_index = item.get('index')
                    if isinstance(doc_index, int) and doc_index >= 0 and doc_index not in collected_docs_data:
                        collected_docs_data[doc_index] = item
            query_metrics["search_time"] = time.time() - search_start
            query_metrics["num_unique_docs_found"] = len(collected_docs_data)
            logging.debug(f"QID {query_id}: Retrieval found {len(collected_docs_data)} unique docs.")

            # --- Chu·∫©n b·ªã danh s√°ch k·∫øt qu·∫£ retrieval ---
            retrieved_docs_list = list(collected_docs_data.values())
            sort_reverse = (retrieval_method != 'dense') # Dense s·∫Øp x·∫øp score (distance) tƒÉng d·∫ßn
            retrieved_docs_list.sort(key=lambda x: x.get('score', 0 if sort_reverse else float('inf')), reverse=sort_reverse)
            query_metrics["num_retrieved_before_rerank"] = len(retrieved_docs_list)


            # --- B∆∞·ªõc 4: Re-ranking (N·∫øu b·∫≠t) ---
            final_docs_for_metrics = [] # Danh s√°ch k·∫øt qu·∫£ cu·ªëi c√πng ƒë·ªÉ t√≠nh metrics
            rerank_start = time.time()

            if use_reranker and retrieved_docs_list:
                query_for_reranking = summarizing_query if summarizing_query else original_query
                docs_to_rerank = retrieved_docs_list[:config.MAX_DOCS_FOR_RERANK]
                query_metrics["num_docs_reranked"] = len(docs_to_rerank)
                logging.debug(f"QID {query_id}: Reranking {len(docs_to_rerank)} docs with query: '{query_for_reranking[:50]}...'")

                # ƒê·∫£m b·∫£o input cho rerank ƒë√∫ng ƒë·ªãnh d·∫°ng list of dicts {'doc': ..., 'index': ...}
                rerank_input = [{'doc': item['doc'], 'index': item['index']} for item in docs_to_rerank]

                reranked_results = utils.rerank_documents(
                    query_for_reranking, rerank_input, reranking_model
                )
                # L·∫•y top K k·∫øt qu·∫£ sau rerank
                final_docs_for_metrics = reranked_results[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
                query_metrics["rerank_time"] = time.time() - rerank_start
                logging.debug(f"QID {query_id}: Reranking finished, selected {len(final_docs_for_metrics)} docs.")

            elif retrieved_docs_list: # Kh√¥ng rerank, l·∫•y tr·ª±c ti·∫øp t·ª´ retrieval
                final_docs_for_metrics = retrieved_docs_list[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
                query_metrics["rerank_time"] = 0.0 # Kh√¥ng t·ªën th·ªùi gian rerank
                query_metrics["num_docs_reranked"] = 0 # Kh√¥ng c√≥ docs n√†o ƒë∆∞·ª£c rerank
                logging.debug(f"QID {query_id}: Skipped reranking, taking top {len(final_docs_for_metrics)} retrieval results.")
            else: # Kh√¥ng c√≥ k·∫øt qu·∫£ retrieval
                 query_metrics["rerank_time"] = 0.0
                 query_metrics["num_docs_reranked"] = 0
                 logging.debug(f"QID {query_id}: No docs to rerank or select.")

            query_metrics["num_retrieved_after_rerank"] = len(final_docs_for_metrics)

            # --- B∆∞·ªõc 5: L·∫•y IDs v√† T√≠nh Metrics ---
            retrieved_ids = []
            # C·∫ßn l·∫•y 'id' ho·∫∑c 'chunk_id' t·ª´ final_docs_for_metrics
            for res in final_docs_for_metrics:
                doc_data = res.get('doc', {})
                chunk_id = None
                if isinstance(doc_data, dict):
                    chunk_id = doc_data.get('id') # ∆Øu ti√™n key 'id'
                    if not chunk_id:
                        metadata = doc_data.get('metadata', {})
                        if isinstance(metadata, dict):
                            chunk_id = metadata.get('id') or metadata.get('chunk_id')
                if chunk_id:
                    retrieved_ids.append(str(chunk_id)) # ƒê·∫£m b·∫£o l√† string

            query_metrics["retrieved_ids"] = retrieved_ids
            logging.debug(f"QID {query_id}: Final retrieved IDs for metrics (top {len(retrieved_ids)}): {retrieved_ids}")

            query_metrics["status"] = "evaluated"
            # T√≠nh to√°n metrics
            for k in k_values:
                query_metrics[f'precision@{k}'] = precision_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'recall@{k}'] = recall_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'f1@{k}'] = f1_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'mrr@{k}'] = mrr_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'ndcg@{k}'] = ndcg_at_k(retrieved_ids, relevant_chunk_ids, k)
                # logging.debug(f"  Metrics @{k}: P={query_metrics[f'precision@{k}']:.4f}, R={query_metrics[f'recall@{k}']:.4f}, F1={query_metrics[f'f1@{k}']:.4f}, MRR={query_metrics[f'mrr@{k}']:.4f}, NDCG={query_metrics[f'ndcg@{k}']:.4f}")


        except Exception as e:
            logging.exception(f"Error evaluating QID {query_id}: {e}")
            query_metrics["status"] = "error_runtime"
            query_metrics["error_message"] = str(e)
        finally:
            query_metrics["processing_time"] = time.time() - start_time
            results_list.append(query_metrics)
            progress_bar.progress((i + 1) / total_items)

    status_text.text(f"Ho√†n th√†nh ƒë√°nh gi√° {total_items} queries!")
    logging.info(f"Finished evaluation for {total_items} queries.")
    return pd.DataFrame(results_list)


def calculate_average_metrics(df_results: pd.DataFrame):
    evaluated_df = df_results[df_results['status'] == 'evaluated'].copy()
    num_evaluated = len(evaluated_df)
    num_skipped_error = len(df_results) - num_evaluated

    if num_evaluated == 0:
        logging.warning("No queries were successfully evaluated. Cannot calculate average metrics.")
        return None, num_evaluated, num_skipped_error

    avg_metrics = {}
    k_values = [1, 3, 5, 10]
    metric_keys_k = [f'{m}@{k}' for k in k_values for m in ['precision', 'recall', 'f1', 'mrr', 'ndcg']]
    timing_keys = ['processing_time', 'variation_time', 'search_time', 'rerank_time']
    # Th√™m c√°c keys s·ªë l∆∞·ª£ng m·ªõi
    count_keys = ['num_variations_generated', 'num_unique_docs_found', 'num_docs_reranked', 'num_retrieved_before_rerank', 'num_retrieved_after_rerank']

    all_keys_to_average = metric_keys_k + timing_keys + count_keys

    for key in all_keys_to_average:
        if key in evaluated_df.columns:
            evaluated_df[key] = pd.to_numeric(evaluated_df[key], errors='coerce')
            total = evaluated_df[key].sum(skipna=True)
            valid_count = evaluated_df[key].notna().sum()
            avg_metrics[f'avg_{key}'] = total / valid_count if valid_count > 0 else 0.0
        else:
            logging.warning(f"Metric key '{key}' not found in results DataFrame for averaging.")
            avg_metrics[f'avg_{key}'] = 0.0

    logging.info(f"Calculated average metrics over {num_evaluated} evaluated queries.")
    return avg_metrics, num_evaluated, num_skipped_error


# --- Giao di·ªán Streamlit ---
st.set_page_config(page_title="ƒê√°nh gi√° Retrieval", layout="wide")
st.title("üìä ƒê√°nh gi√° H·ªá th·ªëng Retrieval")

st.markdown("""
Trang n√†y cho ph√©p b·∫°n ch·∫°y ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa h·ªá th·ªëng retrieval v√† reranking
d·ª±a tr√™n m·ªôt t·∫≠p d·ªØ li·ªáu c√¢u h·ªèi v√† c√°c chunk t√†i li·ªáu li√™n quan (ground truth).
S·ª≠ d·ª•ng c·∫•u h√¨nh hi·ªán t·∫°i t·ª´ trang Chatbot ch√≠nh.
""")

# --- Kh·ªüi t·∫°o ho·∫∑c ki·ªÉm tra Session State ---
if 'eval_data' not in st.session_state: st.session_state.eval_data = None
if 'eval_results_df' not in st.session_state: st.session_state.eval_results_df = None
if 'eval_run_completed' not in st.session_state: st.session_state.eval_run_completed = False
if 'eval_uploaded_filename' not in st.session_state: st.session_state.eval_uploaded_filename = ""
if 'last_eval_config' not in st.session_state: st.session_state.last_eval_config = {}

st.subheader("Tr·∫°ng th√°i H·ªá th·ªëng C∆° b·∫£n")
init_ok = False
retriever_instance = None
g_embedding_model = None
g_reranking_model = None

with st.spinner("Ki·ªÉm tra v√† kh·ªüi t·∫°o t√†i nguy√™n c·ªët l√µi..."):
    try:
        g_embedding_model = utils.load_embedding_model(config.embedding_model_name)
        g_reranking_model = utils.load_reranker_model(config.reranking_model_name)
        _, retriever_instance = data_loader.load_or_create_rag_components(g_embedding_model)

        if retriever_instance and g_embedding_model: # Reranker c√≥ th·ªÉ kh√¥ng c·∫ßn n·∫øu use_reranker=False
            init_ok = True
            st.success("‚úÖ VectorDB, Retriever, Embedding Model, Reranker Model ƒë√£ s·∫µn s√†ng.")
            logging.info("Core components initialized successfully for evaluation.")
            # Ghi ch√∫ v·ªÅ reranker model n·∫øu kh√¥ng t·∫£i ƒë∆∞·ª£c nh∆∞ng init v·∫´n ok
            if not g_reranking_model:
                 st.warning("‚ö†Ô∏è Kh√¥ng t·∫£i ƒë∆∞·ª£c Reranker Model. Ch·ª©c nƒÉng rerank s·∫Ω kh√¥ng ho·∫°t ƒë·ªông.")
                 logging.warning("Reranker model failed to load, reranking will be disabled if attempted.")
        else:
            missing = [comp for comp, loaded in [("Retriever/VectorDB", retriever_instance), ("Embedding Model", g_embedding_model)] if not loaded]
            st.error(f"‚ö†Ô∏è L·ªói kh·ªüi t·∫°o: {', '.join(missing)}.")
            logging.error(f"Failed to initialize components: {', '.join(missing)}.")

    except Exception as e:
        st.error(f"‚ö†Ô∏è L·ªói nghi√™m tr·ªçng khi kh·ªüi t·∫°o h·ªá th·ªëng: {e}")
        logging.exception("Critical error during system initialization for evaluation.")

if init_ok:
    st.subheader("C·∫•u h√¨nh ƒê√°nh gi√°")
    st.markdown("ƒê√°nh gi√° s·∫Ω s·ª≠ d·ª•ng c·∫•u h√¨nh **hi·ªán t·∫°i** t·ª´ **Sidebar c·ªßa trang Chatbot**.")

    # --- L·∫•y c·∫•u h√¨nh hi·ªán t·∫°i t·ª´ session state c·ªßa trang Chatbot ---
    current_gemini_model = st.session_state.get('selected_gemini_model', config.DEFAULT_GEMINI_MODEL)
    current_retrieval_query_mode = st.session_state.get('retrieval_query_mode', 'T·ªïng qu√°t')
    current_use_history_llm1 = st.session_state.get('use_history_for_llm1', True)
    current_retrieval_method = st.session_state.get('retrieval_method', 'hybrid')
    current_use_reranker = st.session_state.get('use_reranker', True)

    # --- Hi·ªÉn th·ªã c·∫•u h√¨nh s·∫Ω s·ª≠ d·ª•ng ---
    st.markdown("**C·∫•u h√¨nh s·∫Ω s·ª≠ d·ª•ng:**")
    cfg_col1, cfg_col2, cfg_col3 = st.columns(3)
    with cfg_col1:
        st.info(f"**Ngu·ªìn Query:** `{current_retrieval_query_mode}`")
        st.info(f"**Ret. Method:** `{current_retrieval_method}`")
    with cfg_col2:
        st.info(f"**Reranker:** `{'B·∫≠t' if current_use_reranker else 'T·∫Øt'}`")
        st.info(f"**History LLM1:** `{'B·∫≠t' if current_use_history_llm1 else 'T·∫Øt'}`")
    with cfg_col3:
        st.info(f"**Gemini Model:** `{current_gemini_model}`")


    # T·∫°o dict c·∫•u h√¨nh cho h√†m ƒë√°nh gi√°
    eval_config_dict = {
        'retrieval_query_mode': current_retrieval_query_mode,
        'retrieval_method': current_retrieval_method,
        'use_reranker': current_use_reranker,
        'use_history_for_llm1': current_use_history_llm1,
        'gemini_model_name': current_gemini_model,
        # Th√™m t√™n model kh√°c n·∫øu c·∫ßn l∆∞u v√†o k·∫øt qu·∫£
        'embedding_model_name': config.embedding_model_name,
        'reranker_model_name': config.reranking_model_name if current_use_reranker else None,
    }
    # Ki·ªÉm tra n·∫øu reranker b·ªã t·∫Øt nh∆∞ng model kh√¥ng t·∫£i ƒë∆∞·ª£c
    reranker_model_to_pass = g_reranking_model if current_use_reranker else None
    if current_use_reranker and not g_reranking_model:
         st.warning("Reranker ƒëang ƒë∆∞·ª£c b·∫≠t trong c·∫•u h√¨nh nh∆∞ng model reranker kh√¥ng t·∫£i ƒë∆∞·ª£c. Reranking s·∫Ω b·ªã b·ªè qua.")
         eval_config_dict['use_reranker'] = False # Ghi ƒë√® config n·∫øu model kh√¥ng c√≥
         eval_config_dict['reranker_model_name'] = "FAILED_TO_LOAD"
         reranker_model_to_pass = None


    st.subheader("T·∫£i L√™n File ƒê√°nh gi√°")
    uploaded_file = st.file_uploader(
        "Ch·ªçn file JSON d·ªØ li·ªáu ƒë√°nh gi√°...", type=["json"], key="eval_file_uploader"
    )

    if uploaded_file is not None:
        if uploaded_file.name != st.session_state.eval_uploaded_filename:
            try:
                eval_data_list = json.loads(uploaded_file.getvalue().decode('utf-8'))
                # Th√™m ki·ªÉm tra ƒë·ªãnh d·∫°ng c∆° b·∫£n ·ªü ƒë√¢y n·∫øu c·∫ßn
                st.session_state.eval_data = eval_data_list
                st.session_state.eval_uploaded_filename = uploaded_file.name
                st.session_state.eval_run_completed = False
                st.session_state.eval_results_df = None
                st.session_state.last_eval_config = {}
                st.success(f"ƒê√£ t·∫£i file '{uploaded_file.name}' ({len(eval_data_list)} c√¢u h·ªèi).")
                logging.info(f"Loaded evaluation file: {uploaded_file.name}")
            except Exception as e:
                st.error(f"L·ªói x·ª≠ l√Ω file JSON: {e}")
                logging.exception("Error processing uploaded JSON file.")
                st.session_state.eval_data = None; st.session_state.eval_uploaded_filename = ""
                st.session_state.eval_run_completed = False; st.session_state.eval_results_df = None

    if st.session_state.eval_data is not None:
        st.info(f"S·∫µn s√†ng ƒë√°nh gi√° v·ªõi d·ªØ li·ªáu t·ª´: **{st.session_state.eval_uploaded_filename}**.")

        if st.checkbox("Hi·ªÉn th·ªã d·ªØ li·ªáu m·∫´u (5 d√≤ng)", key="show_eval_data_preview"):
            st.dataframe(pd.DataFrame(st.session_state.eval_data).head())

        if st.button("üöÄ B·∫Øt ƒë·∫ßu ƒê√°nh gi√°", key="start_eval_button"):
            with st.spinner(f"ƒêang t·∫£i model Gemini: {current_gemini_model}..."):
                g_gemini_model = utils.load_gemini_model(current_gemini_model)

            if g_gemini_model:
                st.info(f"Model Gemini '{current_gemini_model}' ƒë√£ s·∫µn s√†ng.")
                with st.spinner("‚è≥ ƒêang ch·∫°y ƒë√°nh gi√°..."):
                    start_eval_time = time.time()
                    results_df = run_retrieval_evaluation(
                        eval_data=st.session_state.eval_data,
                        hybrid_retriever=retriever_instance,
                        embedding_model=g_embedding_model,
                        reranking_model=reranker_model_to_pass, # Truy·ªÅn model reranker (ho·∫∑c None)
                        gemini_model=g_gemini_model,
                        eval_config=eval_config_dict # Truy·ªÅn dict config
                    )
                    st.write('HHAHAH')
                    total_eval_time = time.time() - start_eval_time
                    st.success(f"Ho√†n th√†nh ƒë√°nh gi√° sau {total_eval_time:.2f} gi√¢y.")
                    logging.info(f"Evaluation completed in {total_eval_time:.2f} seconds.")

                    st.session_state.eval_results_df = results_df
                    st.session_state.eval_run_completed = True
                    st.session_state.last_eval_config = eval_config_dict # L∆∞u config ƒë√£ ch·∫°y
                    st.rerun()
            else:
                st.error(f"Kh√¥ng th·ªÉ t·∫£i model Gemini: {current_gemini_model}.")
                logging.error(f"Failed to load Gemini model '{current_gemini_model}'.")

    # --- Hi·ªÉn th·ªã K·∫øt qu·∫£ ---
    if st.session_state.eval_run_completed and st.session_state.eval_results_df is not None:
        st.subheader("K·∫øt qu·∫£ ƒê√°nh gi√°")
        detailed_results_df = st.session_state.eval_results_df
        last_config = st.session_state.last_eval_config

        # --- Hi·ªÉn th·ªã l·∫°i c·∫•u h√¨nh ƒë√£ ch·∫°y ---
        st.markdown("**C·∫•u h√¨nh ƒë√£ s·ª≠ d·ª•ng:**")
        cfg_col1, cfg_col2, cfg_col3, cfg_col4 = st.columns(4)
        cfg_col1.metric("Ngu·ªìn Query", last_config.get('retrieval_query_mode', 'N/A'))
        cfg_col2.metric("Ret. Method", last_config.get('retrieval_method', 'N/A'))
        cfg_col3.metric("Reranker", "B·∫≠t" if last_config.get('use_reranker', False) else "T·∫Øt")
        cfg_col4.metric("History LLM1", "B·∫≠t" if last_config.get('use_history_for_llm1', False) else "T·∫Øt")
        # Th√™m th√¥ng tin model n·∫øu c√≥ trong config
        st.caption(f"Gemini: `{last_config.get('gemini_model_name', 'N/A')}`, Embedding: `{last_config.get('embedding_model_name', 'N/A')}`, Reranker: `{last_config.get('reranker_model_name', 'N/A')}`")


        avg_metrics, num_eval, num_skipped_error = calculate_average_metrics(detailed_results_df)

        st.metric("T·ªïng s·ªë Queries", len(detailed_results_df))
        col_res1, col_res2 = st.columns(2)
        col_res1.metric("Queries ƒê√°nh gi√° H·ª£p l·ªá", num_eval)
        col_res2.metric("Queries B·ªè qua / L·ªói", num_skipped_error)

        if avg_metrics:
            st.markdown("#### Metrics Trung b√¨nh @K (tr√™n c√°c queries h·ª£p l·ªá)")
            k_values_display = [1, 3, 5, 10]
            cols_k = st.columns(len(k_values_display))
            for idx, k in enumerate(k_values_display):
                with cols_k[idx]:
                    st.markdown(f"**K = {k}**")
                    st.text(f"Precision: {avg_metrics.get(f'avg_precision@{k}', 0.0):.4f}")
                    st.text(f"Recall:    {avg_metrics.get(f'avg_recall@{k}', 0.0):.4f}")
                    st.text(f"F1:        {avg_metrics.get(f'avg_f1@{k}', 0.0):.4f}")
                    st.text(f"MRR:       {avg_metrics.get(f'avg_mrr@{k}', 0.0):.4f}")
                    st.text(f"NDCG:      {avg_metrics.get(f'avg_ndcg@{k}', 0.0):.4f}")

            st.markdown("#### Th√¥ng tin Hi·ªáu nƒÉng & S·ªë l∆∞·ª£ng Trung b√¨nh")
            col_perf1, col_perf2, col_perf3, col_perf4 = st.columns(4)
            col_perf1.metric("Avg Total Time (s)", f"{avg_metrics.get('avg_processing_time', 0.0):.3f}")
            col_perf2.metric("Avg Variation Time (s)", f"{avg_metrics.get('avg_variation_time', 0.0):.3f}")
            col_perf3.metric("Avg Search Time (s)", f"{avg_metrics.get('avg_search_time', 0.0):.3f}")
            col_perf4.metric("Avg Rerank Time (s)", f"{avg_metrics.get('avg_rerank_time', 0.0):.3f}")

            col_count1, col_count2, col_count3, col_count4 = st.columns(4)
            col_count1.metric("Avg Variations Gen", f"{avg_metrics.get('avg_num_variations_generated', 0.0):.1f}")
            col_count2.metric("Avg Docs Found", f"{avg_metrics.get('avg_num_unique_docs_found', 0.0):.1f}")
            col_count3.metric("Avg Docs Reranked", f"{avg_metrics.get('avg_num_docs_reranked', 0.0):.1f}")
            col_count4.metric("Avg Final Docs", f"{avg_metrics.get('avg_num_retrieved_after_rerank', 0.0):.1f}")


        else:
            st.warning("Kh√¥ng th·ªÉ t√≠nh metrics trung b√¨nh (kh√¥ng c√≥ query h·ª£p l·ªá).")


        with st.expander("Xem K·∫øt qu·∫£ Chi ti·∫øt cho t·ª´ng Query"):
            # --- C·∫≠p nh·∫≠t c√°c c·ªôt hi·ªÉn th·ªã ---
            display_columns = [
                'query_id', 'query', 'status',
                'retrieval_query_mode','retrieval_method', 'use_reranker', 'use_history_llm1', # C·∫•u h√¨nh
                'precision@1', 'recall@1', 'f1@1','mrr@1', 'ndcg@1',
                'precision@3', 'recall@3', 'f1@3', 'mrr@3', 'ndcg@3',
                'precision@5', 'recall@5', 'f1@5', 'mrr@5', 'ndcg@5',
                'precision@10', 'recall@10', 'f1@10', 'mrr@10', 'ndcg@10', # Th√™m @10
                'processing_time', 'variation_time', 'search_time', 'rerank_time', # Th·ªùi gian
                'num_variations_generated','num_unique_docs_found', 'num_retrieved_before_rerank','num_docs_reranked', 'num_retrieved_after_rerank', # S·ªë l∆∞·ª£ng
                'retrieved_ids', 'relevant_ids', 'summarizing_query', 'error_message' # Th√¥ng tin kh√°c
            ]
            existing_display_columns = [col for col in display_columns if col in detailed_results_df.columns]
            st.dataframe(detailed_results_df[existing_display_columns])

        st.subheader("L∆∞u K·∫øt qu·∫£ Chi ti·∫øt")
        try:
            results_json = detailed_results_df.to_json(orient='records', indent=2, force_ascii=False)
            results_csv = detailed_results_df.to_csv(index=False).encode('utf-8')

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # --- C·∫≠p nh·∫≠t t√™n file ƒë·ªÉ bao g·ªìm c·∫•u h√¨nh m·ªõi ---
            qmode_suffix = last_config.get('retrieval_query_mode', 'na').lower()[:3] # L·∫•y 3 ch·ªØ c√°i ƒë·∫ßu
            method_suffix = last_config.get('retrieval_method', 'na').lower()
            rerank_suffix = "rr" if last_config.get('use_reranker', False) else "norr"
            hist_suffix = "hist" if last_config.get('use_history_for_llm1', False) else "nohist"
            model_suffix = last_config.get('gemini_model_name', 'gemini').split('/')[-1].replace('.','-')[:15] # Gi·ªõi h·∫°n ƒë·ªô d√†i t√™n model

            base_filename = f"eval_{qmode_suffix}_{method_suffix}_{rerank_suffix}_{hist_suffix}_{model_suffix}_{timestamp}"
            fname_json = f"{base_filename}.json"
            fname_csv = f"{base_filename}.csv"

            col_dl1, col_dl2 = st.columns(2)
            with col_dl1:
                st.download_button("üíæ T·∫£i v·ªÅ JSON", results_json, fname_json, "application/json", key="dl_json")
            with col_dl2:
                st.download_button("üíæ T·∫£i v·ªÅ CSV", results_csv, fname_csv, "text/csv", key="dl_csv")
        except Exception as e:
            st.error(f"L·ªói khi chu·∫©n b·ªã file k·∫øt qu·∫£: {e}")
            logging.exception("Error preparing evaluation results for download.")

    # --- Qu·∫£n l√Ω Tr·∫°ng th√°i ƒê√°nh gi√° ---
    st.markdown("---")
    st.subheader("Qu·∫£n l√Ω Tr·∫°ng th√°i ƒê√°nh gi√°")
    if st.button("X√≥a File ƒê√£ T·∫£i v√† K·∫øt Qu·∫£", key="clear_eval_state"):
        st.session_state.eval_data = None
        st.session_state.eval_uploaded_filename = ""
        st.session_state.eval_run_completed = False
        st.session_state.eval_results_df = None
        st.session_state.last_eval_config = {}
        st.success("ƒê√£ x√≥a tr·∫°ng th√°i ƒë√°nh gi√°.")
        logging.info("Evaluation state cleared.")
        time.sleep(1); st.rerun()

else:
    st.warning("‚ö†Ô∏è H·ªá th·ªëng c∆° b·∫£n ch∆∞a s·∫µn s√†ng. Vui l√≤ng ki·ªÉm tra l·ªói v√† kh·ªüi ƒë·ªông l·∫°i.")
    logging.warning("Evaluation page cannot proceed as core components are not ready.")