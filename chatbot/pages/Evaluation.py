# pages/2_Evaluation.py
import time
import streamlit as st

# --- Debug: Ki·ªÉm tra tr·∫°ng th√°i ngay khi script t·∫£i (gi·ªØ l·∫°i ho·∫∑c x√≥a t√πy √Ω) ---
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logging.info("Evaluation Page Script Started (Removing K=1 metrics).")
logging.info(f"State on load - Gemini Model: {st.session_state.get('selected_gemini_model', 'NOT_FOUND')}")
logging.info(f"State on load - Query Mode: {st.session_state.get('retrieval_query_mode', 'NOT_FOUND')}")
logging.info(f"State on load - Retrieval Method: {st.session_state.get('retrieval_method', 'NOT_FOUND')}")
logging.info(f"State on load - Use Reranker: {st.session_state.get('use_reranker', 'NOT_FOUND')}")
# ƒê√£ b·ªè log History LLM1 ·ªü ƒë√¢y
logging.info("--------------------------------------")
# --- K·∫øt th√∫c Debug ---


# ... Ti·∫øp t·ª•c c√°c l·ªánh import kh√°c
import pandas as pd
import json
import math
import os
from datetime import datetime
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Import config, utils, data_loader, retriever sau khi ƒëi·ªÅu ch·ªânh path
import config
import utils
import data_loader
from retriever import HybridRetriever

# --- C√°c h√†m t√≠nh to√°n metrics (gi·ªØ nguy√™n) ---
def precision_at_k(retrieved_ids, relevant_ids, k):
    if k <= 0: return 0.0
    retrieved_at_k = retrieved_ids[:k]; relevant_set = set(relevant_ids)
    if not relevant_set: return 0.0
    intersect = set(retrieved_at_k) & relevant_set
    return len(intersect) / k

def recall_at_k(retrieved_ids, relevant_ids, k):
    relevant_set = set(relevant_ids)
    if not relevant_set: return 1.0
    retrieved_at_k = retrieved_ids[:k]
    intersect = set(retrieved_at_k) & relevant_set
    return len(intersect) / len(relevant_set)

def f1_at_k(retrieved_ids, relevant_ids, k):
    prec = precision_at_k(retrieved_ids, relevant_ids, k); rec = recall_at_k(retrieved_ids, relevant_ids, k)
    return 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0

def mrr_at_k(retrieved_ids, relevant_ids, k):
    relevant_set = set(relevant_ids);
    if not relevant_set: return 0.0
    retrieved_at_k = retrieved_ids[:k]
    for rank, doc_id in enumerate(retrieved_at_k, 1):
        if doc_id in relevant_set: return 1.0 / rank
    return 0.0

def ndcg_at_k(retrieved_ids, relevant_ids, k):
    relevant_set = set(relevant_ids);
    if not relevant_set: return 1.0
    retrieved_at_k = retrieved_ids[:k]; dcg = 0.0; idcg = 0.0
    for i, doc_id in enumerate(retrieved_at_k):
        relevance = 1.0 if doc_id in relevant_set else 0.0
        dcg += relevance / math.log2(i + 2)
    num_relevant_in_total = len(relevant_set)
    for i in range(min(k, num_relevant_in_total)):
        idcg += 1.0 / math.log2(i + 2)
    return dcg / idcg if idcg > 0 else 0.0


def run_retrieval_evaluation(
    eval_data: list,
    hybrid_retriever: HybridRetriever,
    embedding_model,
    reranking_model, # C√≥ th·ªÉ l√† None n·∫øu kh√¥ng d√πng rerank
    gemini_model,
    eval_config: dict # Ch·ª©a retrieval_query_mode, retrieval_method, use_reranker
    ):

    results_list = []
    # ƒê√£ b·ªè K=1
    k_values = [3, 5, 10] # C√°c gi√° tr·ªã K ƒë·ªÉ t√≠nh metrics

    # --- L·∫•y c·∫•u h√¨nh t·ª´ eval_config ---
    retrieval_query_mode = eval_config.get('retrieval_query_mode', 'T·ªïng qu√°t')
    retrieval_method = eval_config.get('retrieval_method', 'hybrid')
    use_reranker = eval_config.get('use_reranker', True)
    dummy_history = None # Lu√¥n l√† None v√¨ kh√¥ng d√πng history trong evaluation

    progress_bar = st.progress(0)
    status_text = st.empty()

    total_items = len(eval_data)
    queries_per_batch = 15 # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng query tr∆∞·ªõc khi t·∫°m d·ª´ng
    wait_time_seconds = 60 # Th·ªùi gian t·∫°m d·ª´ng

    for i, item in enumerate(eval_data):
        # T·∫°m d·ª´ng sau m·ªói batch
        if i > 0 and i % queries_per_batch == 0:
            pause_msg = f"ƒê√£ x·ª≠ l√Ω {i}/{total_items} queries. T·∫°m d·ª´ng {wait_time_seconds} gi√¢y..."
            logging.info(pause_msg)
            status_text.text(pause_msg)
            time.sleep(wait_time_seconds)
            status_text.text(f"Ti·∫øp t·ª•c x·ª≠ l√Ω query {i+1}/{total_items}...")

        query_id = item.get("query_id"); original_query = item.get("query")
        relevant_chunk_ids = set(item.get("relevant_chunk_ids", []))
        if not query_id or not original_query:
            logging.warning(f"B·ªè qua m·ª•c {i} do thi·∫øu query_id ho·∫∑c query.")
            continue

        status_text.text(f"ƒêang x·ª≠ l√Ω query {i+1}/{total_items}: {query_id} (QueryMode: {retrieval_query_mode}, Method: {retrieval_method}, Rerank: {use_reranker})")
        logging.info(f"Eval - Processing QID: {query_id} (QueryMode: {retrieval_query_mode}, Method: {retrieval_method}, Rerank: {use_reranker})")

        start_time = time.time()
        # --- Kh·ªüi t·∫°o query_metrics v·ªõi c√°c tr∆∞·ªùng c·∫•u h√¨nh ---
        query_metrics = {
            "query_id": query_id, "query": original_query,
            "retrieval_query_mode": retrieval_query_mode,
            "retrieval_method": retrieval_method,
            "use_reranker": use_reranker,
            "status": "error", "retrieved_ids": [], "relevant_ids": list(relevant_chunk_ids),
            "processing_time": 0.0, 'summarizing_query': '',
            'variation_time': 0.0, 'search_time': 0.0, 'rerank_time': 0.0,
            'num_variations_generated': 0, 'num_unique_docs_found': 0, 'num_docs_reranked': 0,
            'num_retrieved_before_rerank': 0, 'num_retrieved_after_rerank': 0
        }
        # V√≤ng l·∫∑p kh·ªüi t·∫°o metrics, t·ª± ƒë·ªông d√πng k_values m·ªõi
        for k in k_values:
            query_metrics[f'precision@{k}'] = 0.0; query_metrics[f'recall@{k}'] = 0.0
            query_metrics[f'f1@{k}'] = 0.0; query_metrics[f'mrr@{k}'] = 0.0; query_metrics[f'ndcg@{k}'] = 0.0

        try:
            # B∆∞·ªõc 1: T·∫°o variations/summarizing query (lu√¥n ch·∫°y)
            # dummy_history gi·ªù lu√¥n l√† None
            variation_start = time.time()
            relevance_status, _, all_queries, summarizing_query = utils.generate_query_variations(
                original_query=original_query, gemini_model=gemini_model,
                chat_history=dummy_history, # S·ª≠ d·ª•ng bi·∫øn n√†y (lu√¥n None)
                num_variations=config.NUM_QUERY_VARIATIONS
            )
            query_metrics["variation_time"] = time.time() - variation_start
            query_metrics["summarizing_query"] = summarizing_query
            query_metrics["num_variations_generated"] = len(all_queries) - 1

            if relevance_status == 'invalid':
                query_metrics["status"] = "skipped_irrelevant"
                query_metrics["processing_time"] = time.time() - start_time
                results_list.append(query_metrics)
                progress_bar.progress((i + 1) / total_items)
                logging.info(f"QID {query_id} skipped as irrelevant.")
                continue

            # --- B∆∞·ªõc 2: X√°c ƒë·ªãnh query(s) ƒë·ªÉ t√¨m ki·∫øm ---
            queries_to_search = []
            if retrieval_query_mode == 'ƒê∆°n gi·∫£n': queries_to_search = [original_query]
            elif retrieval_query_mode == 'T·ªïng qu√°t': queries_to_search = [summarizing_query]
            elif retrieval_query_mode == 'S√¢u': queries_to_search = all_queries

            # --- B∆∞·ªõc 3: Th·ª±c hi·ªán Retrieval ---
            collected_docs_data = {}
            search_start = time.time()
            for q_variant in queries_to_search:
                if not q_variant: continue
                search_results = hybrid_retriever.search(
                    q_variant, embedding_model,
                    method=retrieval_method,
                    k=config.VECTOR_K_PER_QUERY
                )
                for item in search_results:
                    doc_index = item.get('index')
                    if isinstance(doc_index, int) and doc_index >= 0 and doc_index not in collected_docs_data:
                        collected_docs_data[doc_index] = item
            query_metrics["search_time"] = time.time() - search_start
            query_metrics["num_unique_docs_found"] = len(collected_docs_data)
            logging.debug(f"QID {query_id}: Retrieval found {len(collected_docs_data)} unique docs.")

            # --- Chu·∫©n b·ªã danh s√°ch k·∫øt qu·∫£ retrieval ---
            retrieved_docs_list = list(collected_docs_data.values())
            sort_reverse = (retrieval_method != 'dense')
            retrieved_docs_list.sort(key=lambda x: x.get('score', 0 if sort_reverse else float('inf')), reverse=sort_reverse)
            query_metrics["num_retrieved_before_rerank"] = len(retrieved_docs_list)


            # --- B∆∞·ªõc 4: Re-ranking (N·∫øu b·∫≠t) ---
            final_docs_for_metrics = []
            rerank_start = time.time()

            if use_reranker and retrieved_docs_list:
                query_for_reranking = summarizing_query if summarizing_query else original_query
                docs_to_rerank = retrieved_docs_list[:config.MAX_DOCS_FOR_RERANK]
                query_metrics["num_docs_reranked"] = len(docs_to_rerank)
                logging.debug(f"QID {query_id}: Reranking {len(docs_to_rerank)} docs with query: '{query_for_reranking[:50]}...'")

                rerank_input = [{'doc': item['doc'], 'index': item['index']} for item in docs_to_rerank]

                reranked_results = utils.rerank_documents(
                    query_for_reranking, rerank_input, reranking_model
                )
                final_docs_for_metrics = reranked_results[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
                query_metrics["rerank_time"] = time.time() - rerank_start
                logging.debug(f"QID {query_id}: Reranking finished, selected {len(final_docs_for_metrics)} docs.")

            elif retrieved_docs_list:
                final_docs_for_metrics = retrieved_docs_list[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
                query_metrics["rerank_time"] = 0.0
                query_metrics["num_docs_reranked"] = 0
                logging.debug(f"QID {query_id}: Skipped reranking, taking top {len(final_docs_for_metrics)} retrieval results.")
            else:
                 query_metrics["rerank_time"] = 0.0
                 query_metrics["num_docs_reranked"] = 0
                 logging.debug(f"QID {query_id}: No docs to rerank or select.")

            query_metrics["num_retrieved_after_rerank"] = len(final_docs_for_metrics)

            # --- B∆∞·ªõc 5: L·∫•y IDs v√† T√≠nh Metrics ---
            retrieved_ids = []
            for res in final_docs_for_metrics:
                doc_data = res.get('doc', {})
                chunk_id = None
                if isinstance(doc_data, dict):
                    chunk_id = doc_data.get('id')
                    if not chunk_id:
                        metadata = doc_data.get('metadata', {})
                        if isinstance(metadata, dict):
                            chunk_id = metadata.get('id') or metadata.get('chunk_id')
                if chunk_id:
                    retrieved_ids.append(str(chunk_id))

            query_metrics["retrieved_ids"] = retrieved_ids
            logging.debug(f"QID {query_id}: Final retrieved IDs for metrics (top {len(retrieved_ids)}): {retrieved_ids}")

            query_metrics["status"] = "evaluated"
            # V√≤ng l·∫∑p t√≠nh metrics, t·ª± ƒë·ªông d√πng k_values m·ªõi
            for k in k_values:
                query_metrics[f'precision@{k}'] = precision_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'recall@{k}'] = recall_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'f1@{k}'] = f1_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'mrr@{k}'] = mrr_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'ndcg@{k}'] = ndcg_at_k(retrieved_ids, relevant_chunk_ids, k)


        except Exception as e:
            logging.exception(f"Error evaluating QID {query_id}: {e}")
            query_metrics["status"] = "error_runtime"
            query_metrics["error_message"] = str(e)
        finally:
            query_metrics["processing_time"] = time.time() - start_time
            results_list.append(query_metrics)
            progress_bar.progress((i + 1) / total_items)

    status_text.text(f"Ho√†n th√†nh ƒë√°nh gi√° {total_items} queries!")
    logging.info(f"Finished evaluation for {total_items} queries.")
    return pd.DataFrame(results_list)


def calculate_average_metrics(df_results: pd.DataFrame):
    evaluated_df = df_results[df_results['status'] == 'evaluated'].copy()
    num_evaluated = len(evaluated_df)
    num_skipped_error = len(df_results) - num_evaluated

    if num_evaluated == 0:
        logging.warning("No queries were successfully evaluated. Cannot calculate average metrics.")
        return None, num_evaluated, num_skipped_error

    avg_metrics = {}
    # ƒê√£ b·ªè K=1
    k_values = [3, 5, 10]
    metric_keys_k = [f'{m}@{k}' for k in k_values for m in ['precision', 'recall', 'f1', 'mrr', 'ndcg']]
    timing_keys = ['processing_time', 'variation_time', 'search_time', 'rerank_time']
    count_keys = ['num_variations_generated', 'num_unique_docs_found', 'num_docs_reranked', 'num_retrieved_before_rerank', 'num_retrieved_after_rerank']

    all_keys_to_average = metric_keys_k + timing_keys + count_keys

    for key in all_keys_to_average:
        if key in evaluated_df.columns:
            evaluated_df[key] = pd.to_numeric(evaluated_df[key], errors='coerce')
            total = evaluated_df[key].sum(skipna=True)
            valid_count = evaluated_df[key].notna().sum()
            avg_metrics[f'avg_{key}'] = total / valid_count if valid_count > 0 else 0.0
        else:
            logging.warning(f"Metric key '{key}' not found in results DataFrame for averaging.")
            avg_metrics[f'avg_{key}'] = 0.0

    logging.info(f"Calculated average metrics over {num_evaluated} evaluated queries.")
    return avg_metrics, num_evaluated, num_skipped_error


# --- Giao di·ªán Streamlit ---
st.set_page_config(page_title="ƒê√°nh gi√° Retrieval", layout="wide")
st.title("üìä ƒê√°nh gi√° H·ªá th·ªëng Retrieval")

st.markdown("""
Trang n√†y cho ph√©p b·∫°n ch·∫°y ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa h·ªá th·ªëng retrieval v√† reranking
d·ª±a tr√™n m·ªôt t·∫≠p d·ªØ li·ªáu c√¢u h·ªèi v√† c√°c chunk t√†i li·ªáu li√™n quan (ground truth).
S·ª≠ d·ª•ng c·∫•u h√¨nh **hi·ªán t·∫°i ƒë∆∞·ª£c ch·ªçn tr√™n sidebar c·ªßa trang n√†y**.
""")

# --- sidebar ---
with st.sidebar:
    st.title("T√πy ch·ªçn ƒê√°nh gi√°")

    # --- Initialize session state keys for sidebar widgets if they don't exist ---
    # Attempt to get initial state from Chatbot state if it exists, otherwise use defaults
    DEFAULT_EVAL_CONFIG_STATE = {
        "selected_gemini_model": st.session_state.get("selected_gemini_model", config.DEFAULT_GEMINI_MODEL),
        "retrieval_query_mode": st.session_state.get("retrieval_query_mode", 'T·ªïng qu√°t'),
        "retrieval_method": st.session_state.get("retrieval_method", 'hybrid'),
        "use_reranker": st.session_state.get("use_reranker", True),
    }

    for key, default_value in DEFAULT_EVAL_CONFIG_STATE.items():
        if key not in st.session_state:
            st.session_state[key] = default_value
            # logging.info(f"Initialized missing key '{key}' in Evaluation sidebar state with default: {default_value}") # Optional logging

    # ƒê·∫£m b·∫£o key use_history_llm1 t·ªìn t·∫°i v·ªõi gi√° tr·ªã False m·∫∑c ƒë·ªãnh cho Evaluation n·∫øu n√≥ b·ªã thi·∫øu
    # Kh√¥ng c√≥ widget ƒëi·ªÅu khi·ªÉn cho n√≥ ·ªü ƒë√¢y.
    if 'use_history_for_llm1' not in st.session_state:
        st.session_state.use_history_for_llm1 = False


    st.header("M√¥ h√¨nh")
    # Widget ƒë·ªçc v√† ghi v√†o st.session_state['selected_gemini_model']
    st.selectbox(
        "Ch·ªçn m√¥ h√¨nh Gemini (ƒë·ªÉ t·∫°o query variations):",
        options=config.AVAILABLE_GEMINI_MODELS,
        index=config.AVAILABLE_GEMINI_MODELS.index(st.session_state.get('selected_gemini_model', config.DEFAULT_GEMINI_MODEL)), # ƒê·ªçc t·ª´ state
        key="selected_gemini_model", # Ghi v√†o state khi thay ƒë·ªïi
        help="Ch·ªçn m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn ƒë·ªÉ ph√¢n t√≠ch v√† t·∫°o bi·∫øn th·ªÉ c√¢u h·ªèi cho Retrieval."
    )


    st.header("C·∫•u h√¨nh Retrieval")

    # Widget ƒë·ªçc v√† ghi v√†o st.session_state['retrieval_query_mode']
    st.radio(
        "Ngu·ªìn c√¢u h·ªèi cho Retrieval:",
        options=['ƒê∆°n gi·∫£n', 'T·ªïng qu√°t', 'S√¢u'],
        index=['ƒê∆°n gi·∫£n', 'T·ªïng qu√°t', 'S√¢u'].index(st.session_state.get('retrieval_query_mode', 'T·ªïng qu√°t')), # ƒê·ªçc t·ª´ state
        key="retrieval_query_mode", # Ghi v√†o state khi thay ƒë·ªïi
        horizontal=True,
        help=(
            "**ƒê∆°n gi·∫£n:** Ch·ªâ d√πng c√¢u h·ªèi g·ªëc.\n"
            "**T·ªïng qu√°t:** Ch·ªâ d√πng c√¢u h·ªèi t√≥m t·∫Øt (do AI t·∫°o).\n"
            "**S√¢u:** D√πng c·∫£ c√¢u h·ªèi g·ªëc v√† c√°c bi·∫øn th·ªÉ (do AI t·∫°o)."
        )
    )

    # Widget ƒë·ªçc v√† ghi v√†o st.session_state['retrieval_method']
    st.radio(
        "Ph∆∞∆°ng th·ª©c Retrieval:",
        options=['dense', 'sparse', 'hybrid'],
        index=['dense', 'sparse', 'hybrid'].index(st.session_state.get('retrieval_method', 'hybrid')), # ƒê·ªçc t·ª´ state
        key="retrieval_method", # Ghi v√†o state khi thay ƒë·ªïi
        horizontal=True,
        help=(
            "**dense:** T√¨m ki·∫øm d·ª±a tr√™n vector ng·ªØ nghƒ©a.\n"
            "**sparse:** T√¨m ki·∫øm d·ª±a tr√™n t·ª´ kh√≥a (BM25).\n"
            "**hybrid:** K·∫øt h·ª£p c·∫£ dense v√† sparse."
        )
    )

    # Widget ƒë·ªçc v√† ghi v√†o st.session_state['use_reranker']
    st.toggle(
        "S·ª≠ d·ª•ng Reranker",
        value=st.session_state.get('use_reranker', True), # ƒê·ªçc t·ª´ state
        key="use_reranker", # Ghi v√†o state khi thay ƒë·ªïi
        help="B·∫≠t ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh CrossEncoder x·∫øp h·∫°ng l·∫°i k·∫øt qu·∫£ t√¨m ki·∫øm."
    )

    # ƒê√£ b·ªè c√†i ƒë·∫∑t cho History LLM1 ·ªü sidebar

# --- Kh·ªüi t·∫°o ho·∫∑c ki·ªÉm tra Session State (Ti·∫øp t·ª•c) ---
# Ph·∫ßn kh·ªüi t·∫°o state ri√™ng c·ªßa Evaluation (gi·ªØ nguy√™n)
if 'eval_data' not in st.session_state: st.session_state.eval_data = None
if 'eval_results_df' not in st.session_state: st.session_state.eval_results_df = None
if 'eval_run_completed' not in st.session_state: st.session_state.eval_run_completed = False
if 'eval_uploaded_filename' not in st.session_state: st.session_state.eval_uploaded_filename = ""
# last_eval_config kh√¥ng c·∫ßn kh·ªüi t·∫°o ·ªü ƒë√¢y v√¨ n√≥ ch·ªâ ƒë∆∞·ª£c set khi b·∫Øt ƒë·∫ßu ƒë√°nh gi√°


st.subheader("Tr·∫°ng th√°i H·ªá th·ªëng C∆° b·∫£n")
init_ok = False
retriever_instance = None
g_embedding_model = None
g_reranking_model_loaded = None # ƒê·ªïi t√™n bi·∫øn ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n

with st.spinner("Ki·ªÉm tra v√† kh·ªüi t·∫°o t√†i nguy√™n c·ªët l√µi..."):
    try:
        g_embedding_model = utils.load_embedding_model(config.embedding_model_name)
        # T·∫£i reranker model nh∆∞ng ch·ªâ d√πng n·∫øu use_reranker_eval l√† True (ƒë·ªçc t·ª´ state)
        g_reranking_model_loaded = utils.load_reranker_model(config.reranking_model_name)

        _, retriever_instance = data_loader.load_or_create_rag_components(g_embedding_model)

        # ƒê·ªçc gi√° tr·ªã use_reranker t·ª´ session state (ƒë∆∞·ª£c qu·∫£n l√Ω b·ªüi sidebar)
        use_reranker_current = st.session_state.get('use_reranker', True)

        if retriever_instance and g_embedding_model:
            init_ok = True
            st.success("‚úÖ VectorDB, Retriever, Embedding Model ƒë√£ s·∫µn s√†ng.")
            logging.info("Core components initialized successfully for evaluation.")
            # Th√¥ng b√°o v·ªÅ reranker model n·∫øu kh√¥ng t·∫£i ƒë∆∞·ª£c ho·∫∑c b·ªã t·∫Øt
            if not g_reranking_model_loaded:
                 st.warning("‚ö†Ô∏è Kh√¥ng t·∫£i ƒë∆∞·ª£c Reranker Model. Ch·ª©c nƒÉng rerank s·∫Ω kh√¥ng ho·∫°t ƒë·ªông.")
                 logging.warning("Reranker model failed to load, reranking will be disabled if attempted.")
            elif not use_reranker_current: # D√πng bi·∫øn m·ªõi ƒë·ªçc t·ª´ state
                 st.info("Reranker Model ƒë√£ t·∫£i, nh∆∞ng ch·ª©c nƒÉng Rerank ƒëang **T·∫Øt** trong c·∫•u h√¨nh sidebar.")

        else:
            missing = [comp for comp, loaded in [("Retriever/VectorDB", retriever_instance), ("Embedding Model", g_embedding_model)] if not loaded]
            st.error(f"‚ö†Ô∏è L·ªói kh·ªüi t·∫°o: {', '.join(missing)}.")
            logging.error(f"Failed to initialize components: {', '.join(missing)}.")

    except Exception as e:
        st.error(f"‚ö†Ô∏è L·ªói nghi√™m tr·ªçng khi kh·ªüi t·∫°o h·ªá th·ªëng: {e}")
        logging.exception("Critical error during system initialization for evaluation.")

if init_ok:
    # --- Hi·ªÉn th·ªã C·∫•u h√¨nh ƒê√°nh gi√° s·∫Ω s·ª≠ d·ª•ng (ƒë·ªçc t·ª´ session state, gi·ªù do sidebar qu·∫£n l√Ω) ---
    st.caption(f"M√¥ h√¨nh: `{st.session_state.get('selected_gemini_model', 'N/A')}` | Ngu·ªìn Query: `{st.session_state.get('retrieval_query_mode', 'N/A')}` | Retrieval: `{st.session_state.get('retrieval_method', 'N/A')}` | Reranker: `{'B·∫≠t' if st.session_state.get('use_reranker', False) else 'T·∫Øt'}`")

    # T·∫°o dict c·∫•u h√¨nh cho h√†m ƒë√°nh gi√° - ƒê·ªçc tr·ª±c ti·∫øp t·ª´ st.session_state
    # C√°c gi√° tr·ªã n√†y gi·ªù ƒë∆∞·ª£c ƒë·∫£m b·∫£o t·ªìn t·∫°i do sidebar ho·∫∑c kh·ªüi t·∫°o s·ªõm
    eval_config_dict = {
        'retrieval_query_mode': st.session_state.get('retrieval_query_mode', 'T·ªïng qu√°t'),
        'retrieval_method': st.session_state.get('retrieval_method', 'hybrid'),
        'use_reranker': st.session_state.get('use_reranker', True),
        'use_history_llm1': False, # Gi√° tr·ªã n√†y lu√¥n l√† False cho evaluation
        'gemini_model_name': st.session_state.get('selected_gemini_model', config.DEFAULT_GEMINI_MODEL),
        'embedding_model_name': config.embedding_model_name,
        # C·∫≠p nh·∫≠t t√™n reranker model d·ª±a tr√™n tr·∫°ng th√°i t·∫£i v√† c·∫•u h√¨nh
        'reranker_model_name': config.reranking_model_name if st.session_state.get('use_reranker', True) and g_reranking_model_loaded else ("DISABLED_BY_CONFIG" if st.session_state.get('use_reranker', True) else "DISABLED_BY_CONFIG"),
    }
    # Ki·ªÉm tra cu·ªëi c√πng cho reranker model ƒë·ªÉ truy·ªÅn v√†o h√†m run_retrieval_evaluation
    reranker_model_for_run = g_reranking_model_loaded if st.session_state.get('use_reranker', True) and g_reranking_model_loaded else None


    st.subheader("T·∫£i L√™n File ƒê√°nh gi√°")
    uploaded_file = st.file_uploader(
        "Ch·ªçn file JSON d·ªØ li·ªáu ƒë√°nh gi√°...", type=["json"], key="eval_file_uploader"
    )

    if uploaded_file is not None:
        if uploaded_file.name != st.session_state.eval_uploaded_filename:
            try:
                eval_data_list = json.loads(uploaded_file.getvalue().decode('utf-8'))
                st.session_state.eval_data = eval_data_list
                st.session_state.eval_uploaded_filename = uploaded_file.name
                st.session_state.eval_run_completed = False
                # Reset last_eval_config khi t·∫£i file m·ªõi ƒë·ªÉ tr√°nh hi·ªÉn th·ªã k·∫øt qu·∫£ c≈© v·ªõi c·∫•u h√¨nh sai
                st.session_state.last_eval_config = {}
                st.success(f"ƒê√£ t·∫£i file '{uploaded_file.name}' ({len(eval_data_list)} c√¢u h·ªèi).")
                logging.info(f"Loaded evaluation file: {uploaded_file.name}")
            except Exception as e:
                st.error(f"L·ªói x·ª≠ l√Ω file JSON: {e}")
                logging.exception("Error processing uploaded JSON file.")
                st.session_state.eval_data = None; st.session_state.eval_uploaded_filename = ""
                st.session_state.eval_run_completed = False

    if st.session_state.eval_data is not None:
        st.info(f"S·∫µn s√†ng ƒë√°nh gi√° v·ªõi d·ªØ li·ªáu t·ª´: **{st.session_state.eval_uploaded_filename}**.")

        if st.checkbox("Hi·ªÉn th·ªã d·ªØ li·ªáu m·∫´u (5 d√≤ng)", key="show_eval_data_preview"):
            st.dataframe(pd.DataFrame(st.session_state.eval_data).head())

        # N√∫t b·∫Øt ƒë·∫ßu ƒë√°nh gi√°
        if st.button("üöÄ B·∫Øt ƒë·∫ßu ƒê√°nh gi√°", key="start_eval_button"):
             # L∆∞u c·∫•u h√¨nh hi·ªán t·∫°i t·ª´ st.session_state v√†o last_eval_config tr∆∞·ªõc khi ch·∫°y
             # ƒê√¢y l√† c·∫•u h√¨nh m√† ng∆∞·ªùi d√πng ƒë√£ ch·ªçn tr√™n sidebar c·ªßa trang Evaluation
             current_config_for_save = {
                'retrieval_query_mode': st.session_state.get('retrieval_query_mode', 'T·ªïng qu√°t'),
                'retrieval_method': st.session_state.get('retrieval_method', 'hybrid'),
                'use_reranker': st.session_state.get('use_reranker', True),
                'use_history_llm1': False, # Gi√° tr·ªã n√†y lu√¥n l√† False cho evaluation
                'gemini_model_name': st.session_state.get('selected_gemini_model', config.DEFAULT_GEMINI_MODEL),
                'embedding_model_name': config.embedding_model_name,
                'reranker_model_name': config.reranking_model_name if st.session_state.get('use_reranker', True) and g_reranking_model_loaded else ("DISABLED_BY_CONFIG" if st.session_state.get('use_reranker', True) else "DISABLED_BY_CONFIG"),
             }
             st.session_state.last_eval_config = current_config_for_save.copy() # L∆∞u b·∫£n sao

             with st.spinner(f"ƒêang t·∫£i model Gemini: {st.session_state.get('selected_gemini_model', config.DEFAULT_GEMINI_MODEL)}..."):
                 # T·∫£i Gemini model d·ª±a tr√™n l·ª±a ch·ªçn m·ªõi nh·∫•t t·ª´ sidebar (ƒë√£ c√≥ trong session state)
                 g_gemini_model_eval = utils.load_gemini_model(st.session_state.get('selected_gemini_model', config.DEFAULT_GEMINI_MODEL))


             if g_gemini_model_eval:
                st.info(f"Model Gemini '{st.session_state.get('selected_gemini_model', config.DEFAULT_GEMINI_MODEL)}' ƒë√£ s·∫µn s√†ng.")
                with st.spinner("‚è≥ ƒêang ch·∫°y ƒë√°nh gi√°..."):
                    start_eval_time = time.time()
                    results_df = run_retrieval_evaluation(
                        eval_data=st.session_state.eval_data,
                        hybrid_retriever=retriever_instance,
                        embedding_model=g_embedding_model,
                        reranking_model=reranker_model_for_run, # Truy·ªÅn model (ho·∫∑c None)
                        gemini_model=g_gemini_model_eval, # Truy·ªÅn Gemini model ƒë√£ t·∫£i
                        eval_config=st.session_state.last_eval_config # Truy·ªÅn dict config ƒë√£ l∆∞u (ƒë·∫£m b·∫£o nh·∫•t)
                    )
                    total_eval_time = time.time() - start_eval_time
                    st.success(f"Ho√†n th√†nh ƒë√°nh gi√° sau {total_eval_time:.2f} gi√¢y.")
                    logging.info(f"Evaluation completed in {total_eval_time:.2f} seconds.")

                    st.session_state.eval_results_df = results_df
                    st.session_state.eval_run_completed = True
                    st.rerun() # Rerun ƒë·ªÉ hi·ªÉn th·ªã k·∫øt qu·∫£


    # --- Hi·ªÉn th·ªã K·∫øt qu·∫£ ---
    if st.session_state.eval_run_completed and st.session_state.eval_results_df is not None:
        st.subheader("K·∫øt qu·∫£ ƒê√°nh gi√°")
        detailed_results_df = st.session_state.eval_results_df
        last_config = st.session_state.last_eval_config # ƒê·ªçc config ƒë√£ ch·∫°y

        # --- Hi·ªÉn th·ªã l·∫°i c·∫•u h√¨nh ƒë√£ ch·∫°y ---
        st.markdown("**C·∫•u h√¨nh ƒë√£ s·ª≠ d·ª•ng cho l·∫ßn ch·∫°y cu·ªëi:**")
        cfg_col1, cfg_col2, cfg_col3, cfg_col4 = st.columns(4)
        cfg_col1.metric("Ngu·ªìn Query", last_config.get('retrieval_query_mode', 'N/A'))
        cfg_col2.metric("Ret. Method", last_config.get('retrieval_method', 'N/A'))
        cfg_col3.metric("Reranker", "B·∫≠t" if last_config.get('use_reranker', False) else "T·∫Øt")
        cfg_col4.metric("History LLM1", "T·∫Øt") # Lu√¥n hi·ªÉn th·ªã T·∫Øt v√¨ kh√¥ng d√πng history
        st.caption(f"Gemini: `{last_config.get('gemini_model_name', 'N/A')}`, Embedding: `{last_config.get('embedding_model_name', 'N/A')}`, Reranker: `{last_config.get('reranker_model_name', 'N/A')}`")


        avg_metrics, num_eval, num_skipped_error = calculate_average_metrics(detailed_results_df)

        st.metric("T·ªïng s·ªë Queries", len(detailed_results_df))
        col_res1, col_res2 = st.columns(2)
        col_res1.metric("Queries ƒê√°nh gi√° H·ª£p l·ªá", num_eval)
        col_res2.metric("Queries B·ªè qua / L·ªói", num_skipped_error)

        if avg_metrics:
            st.markdown("#### Metrics Trung b√¨nh @K (tr√™n c√°c queries h·ª£p l·ªá)")
            # ƒê√£ b·ªè K=1
            k_values_display = [3, 5, 10]
            cols_k = st.columns(len(k_values_display))
            for idx, k in enumerate(k_values_display):
                with cols_k[idx]:
                    st.markdown(f"**K = {k}**")
                    st.text(f"Precision: {avg_metrics.get(f'avg_precision@{k}', 0.0):.4f}")
                    st.text(f"Recall:    {avg_metrics.get(f'avg_recall@{k}', 0.0):.4f}")
                    st.text(f"F1:        {avg_metrics.get(f'avg_f1@{k}', 0.0):.4f}")
                    st.text(f"MRR:       {avg_metrics.get(f'avg_mrr@{k}', 0.0):.4f}")
                    st.text(f"NDCG:      {avg_metrics.get(f'avg_ndcg@{k}', 0.0):.4f}")

            st.markdown("#### Th√¥ng tin Hi·ªáu nƒÉng & S·ªë l∆∞·ª£ng Trung b√¨nh")
            col_perf1, col_perf2, col_perf3, col_perf4 = st.columns(4)
            col_perf1.metric("Avg Total Time (s)", f"{avg_metrics.get('avg_processing_time', 0.0):.3f}")
            col_perf2.metric("Avg Variation Time (s)", f"{avg_metrics.get('avg_variation_time', 0.0):.3f}")
            col_perf3.metric("Avg Search Time (s)", f"{avg_metrics.get('avg_search_time', 0.0):.3f}")
            col_perf4.metric("Avg Rerank Time (s)", f"{avg_metrics.get('avg_rerank_time', 0.0):.3f}")

            col_count1, col_count2, col_count3, col_count4 = st.columns(4)
            col_count1.metric("Avg Variations Gen", f"{avg_metrics.get('avg_num_variations_generated', 0.0):.1f}")
            col_count2.metric("Avg Docs Found", f"{avg_metrics.get('avg_num_unique_docs_found', 0.0):.1f}")
            col_count3.metric("Avg Docs Reranked", f"{avg_metrics.get('avg_num_docs_reranked', 0.0):.1f}")
            col_count4.metric("Avg Final Docs", f"{avg_metrics.get('avg_num_retrieved_after_rerank', 0.0):.1f}")


        else:
            st.warning("Kh√¥ng th·ªÉ t√≠nh metrics trung b√¨nh (kh√¥ng c√≥ query h·ª£p l·ªá).")


        with st.expander("Xem K·∫øt qu·∫£ Chi ti·∫øt cho t·ª´ng Query"):
            display_columns = [
                'query_id', 'query', 'status',
                'retrieval_query_mode','retrieval_method', 'use_reranker', # ƒê√£ b·ªè use_history_llm1 kh·ªèi c·ªôt hi·ªÉn th·ªã
                'precision@3', 'recall@3', 'f1@3', 'mrr@3', 'ndcg@3', # Ch·ªâ gi·ªØ K=3, 5, 10
                'precision@5', 'recall@5', 'f1@5', 'mrr@5', 'ndcg@5',
                'precision@10', 'recall@10', 'f1@10', 'mrr@10', 'ndcg@10',
                'processing_time', 'variation_time', 'search_time', 'rerank_time',
                'num_variations_generated','num_unique_docs_found', 'num_retrieved_before_rerank','num_docs_reranked', 'num_retrieved_after_rerank',
                'retrieved_ids', 'relevant_ids', 'summarizing_query', 'error_message'
            ]
            # L·ªçc l·∫°i c√°c c·ªôt hi·ªÉn th·ªã ƒë·ªÉ ch·ªâ gi·ªØ l·∫°i c√°c c·ªôt th·ª±c s·ª± c√≥ trong DataFrame
            # ƒêi·ªÅu n√†y quan tr·ªçng v√¨ c√°c metrics @1 kh√¥ng c√≤n ƒë∆∞·ª£c t√≠nh
            existing_display_columns = [col for col in display_columns if col in detailed_results_df.columns]
            st.dataframe(detailed_results_df[existing_display_columns])


        st.subheader("L∆∞u K·∫øt qu·∫£ Chi ti·∫øt")
        try:
            results_json = detailed_results_df.to_json(orient='records', indent=2, force_ascii=False)
            results_csv = detailed_results_df.to_csv(index=False).encode('utf-8')

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            # S·ª≠ d·ª•ng config ƒë√£ ch·∫°y ƒë·ªÉ t·∫°o t√™n file
            qmode_suffix = last_config.get('retrieval_query_mode', 'na').lower()[:3]
            method_suffix = last_config.get('retrieval_method', 'na').lower()
            rerank_suffix = "rr" if last_config.get('use_reranker', False) else "norr"
            # ƒê√£ b·ªè hist_suffix kh·ªèi t√™n file
            model_suffix = last_config.get('gemini_model_name', 'gemini').split('/')[-1].replace('.','-')[:15]

            # T√™n file kh√¥ng c√≤n ch·ª©a th√¥ng tin history
            base_filename = f"eval_{qmode_suffix}_{method_suffix}_{rerank_suffix}_{model_suffix}_{timestamp}"
            fname_json = f"{base_filename}.json"
            fname_csv = f"{base_filename}.csv"

            col_dl1, col_dl2 = st.columns(2)
            with col_dl1:
                st.download_button("üíæ T·∫£i v·ªÅ JSON", results_json, fname_json, "application/json", key="dl_json")
            with col_dl2:
                st.download_button("üíæ T·∫£i v·ªÅ CSV", results_csv, fname_csv, "text/csv", key="dl_csv")
        except Exception as e:
            st.error(f"L·ªói khi chu·∫©n b·ªã file k·∫øt qu·∫£: {e}")
            logging.exception("Error preparing evaluation results for download.")

    # --- Qu·∫£n l√Ω Tr·∫°ng th√°i ƒê√°nh gi√° ---
    st.markdown("---")
    st.subheader("Qu·∫£n l√Ω Tr·∫°ng th√°i ƒê√°nh gi√°")
    if st.button("X√≥a File ƒê√£ T·∫£i v√† K·∫øt Qu·∫£", key="clear_eval_state"):
        st.session_state.eval_data = None
        st.session_state.eval_uploaded_filename = ""
        st.session_state.eval_run_completed = False
        st.session_state.eval_results_df = None
        st.session_state.last_eval_config = {}
        # Reset c√°c c√†i ƒë·∫∑t sidebar v·ªÅ m·∫∑c ƒë·ªãnh khi x√≥a tr·∫°ng th√°i
        st.session_state.selected_gemini_model = config.DEFAULT_GEMINI_MODEL
        st.session_state.retrieval_query_mode = 'T·ªïng qu√°t'
        st.session_state.retrieval_method = 'hybrid'
        st.session_state.use_reranker = True
        st.session_state.use_history_llm1 = False # Lu√¥n reset use_history_llm1 v·ªÅ False cho Evaluation

        st.success("ƒê√£ x√≥a tr·∫°ng th√°i ƒë√°nh gi√°.")
        logging.info("Evaluation state cleared.")
        time.sleep(1); st.rerun()

else:
    st.warning("‚ö†Ô∏è H·ªá th·ªëng c∆° b·∫£n ch∆∞a s·∫µn s√†ng. Vui l√≤ng ki·ªÉm tra l·ªói v√† kh·ªüi ƒë·ªông l·∫°i.")
    logging.warning("Evaluation page cannot proceed as core components are not ready.")