# pages/2_Evaluation.py
import time
import streamlit as st
import pandas as pd
import json
import time
import math
import os
import logging
from datetime import datetime
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import config
import utils
import data_loader
from vector_db import SimpleVectorDatabase
from retriever import HybridRetriever

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def precision_at_k(retrieved_ids, relevant_ids, k):
    if k <= 0: return 0.0
    retrieved_at_k = retrieved_ids[:k]; relevant_set = set(relevant_ids)
    if not relevant_set: return 0.0
    intersect = set(retrieved_at_k) & relevant_set
    return len(intersect) / k

def recall_at_k(retrieved_ids, relevant_ids, k):
    relevant_set = set(relevant_ids)
    if not relevant_set: return 1.0
    retrieved_at_k = retrieved_ids[:k]
    intersect = set(retrieved_at_k) & relevant_set
    return len(intersect) / len(relevant_set)

def f1_at_k(retrieved_ids, relevant_ids, k):
    prec = precision_at_k(retrieved_ids, relevant_ids, k); rec = recall_at_k(retrieved_ids, relevant_ids, k)
    return 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0

def mrr_at_k(retrieved_ids, relevant_ids, k):
    relevant_set = set(relevant_ids);
    if not relevant_set: return 0.0
    for rank, doc_id in enumerate(retrieved_ids[:k], 1):
        if doc_id in relevant_set: return 1.0 / rank
    return 0.0

def ndcg_at_k(retrieved_ids, relevant_ids, k):
    relevant_set = set(relevant_ids);
    if not relevant_set: return 1.0
    retrieved_at_k = retrieved_ids[:k]; dcg = 0.0; idcg = 0.0
    for i, doc_id in enumerate(retrieved_at_k):
        if doc_id in relevant_set: dcg += 1.0 / math.log2(i + 2)
    for i in range(min(k, len(relevant_set))): idcg += 1.0 / math.log2(i + 2)
    return dcg / idcg if idcg > 0 else 0.0


def run_retrieval_evaluation(
    eval_data: list,
    hybrid_retriever: HybridRetriever,
    embedding_model,
    reranking_model,
    gemini_model,
    eval_config: dict
    ):

    results_list = []
    k_values = [1, 3, 5, 10] # <<< THAY ƒê·ªîI >>> Add K=1

    retrieval_mode = eval_config.get('retrieval_mode', 'ƒê∆°n gi·∫£n')
    use_history = eval_config.get('use_history_for_llm1', False)
    dummy_history = [{"role": "user", "content": "..."}] if use_history else None

    progress_bar = st.progress(0)
    status_text = st.empty()

    total_items = len(eval_data)
    queries_per_batch = 15
    wait_time_seconds = 60

    for i, item in enumerate(eval_data):
        if i > 0 and i % queries_per_batch == 0:
            pause_msg = f"ƒê√£ x·ª≠ l√Ω {i}/{total_items} queries. T·∫°m d·ª´ng {wait_time_seconds} gi√¢y..."
            logging.info(pause_msg)
            status_text.text(pause_msg)
            time.sleep(wait_time_seconds)
            status_text.text(f"Ti·∫øp t·ª•c x·ª≠ l√Ω query {i+1}/{total_items}...")

        query_id = item.get("query_id"); original_query = item.get("query")
        relevant_chunk_ids = set(item.get("relevant_chunk_ids", []))
        if not query_id or not original_query: continue

        status_text.text(f"ƒêang x·ª≠ l√Ω query {i+1}/{total_items}: {query_id}")
        logging.info(f"Eval - Processing QID: {query_id}")

        start_time = time.time()
        query_metrics = {
            "query_id": query_id, "query": original_query, "retrieval_mode": retrieval_mode,
            "status": "error", "retrieved_ids": [], "relevant_ids": list(relevant_chunk_ids),
            "processing_time": 0.0
        }
        # Initialize metrics for all k values, including k=1
        for k in k_values: query_metrics[f'precision@{k}'] = 0.0; query_metrics[f'recall@{k}'] = 0.0; query_metrics[f'f1@{k}'] = 0.0; query_metrics[f'mrr@{k}'] = 0.0; query_metrics[f'ndcg@{k}'] = 0.0
        timing_keys = ['variation_time', 'search_time', 'rerank_time']
        count_keys = ['num_variations_generated', 'num_unique_docs_found', 'num_docs_reranked']
        for k in timing_keys + count_keys: query_metrics[k] = 0.0

        try:
            variation_start = time.time()
            relevance_status, _, all_queries, summarizing_query = utils.generate_query_variations(
                original_query=original_query,
                gemini_model=gemini_model,
                chat_history=dummy_history
            )
            query_metrics["variation_time"] = time.time() - variation_start
            query_metrics["summarizing_query"] = summarizing_query
            query_metrics["num_variations_generated"] = len(all_queries)

            if relevance_status == 'invalid':
                query_metrics["status"] = "skipped_irrelevant"
                query_metrics["processing_time"] = time.time() - start_time
                results_list.append(query_metrics)
                progress_bar.progress((i + 1) / total_items)
                continue

            collected_docs_data = {}
            search_start = time.time()
            if retrieval_mode == 'ƒê∆°n gi·∫£n':
                variant_results = hybrid_retriever.hybrid_search(
                    summarizing_query, embedding_model,
                    vector_search_k=config.VECTOR_K_PER_QUERY, final_k=config.HYBRID_K_PER_QUERY
                )
                for res_item in variant_results:
                    idx = res_item.get('index');
                    if isinstance(idx, int) and idx >= 0: collected_docs_data[idx] = res_item
            elif retrieval_mode == 'S√¢u':
                for q_variant in all_queries:
                    variant_results = hybrid_retriever.hybrid_search(
                        q_variant, embedding_model,
                        vector_search_k=config.VECTOR_K_PER_QUERY, final_k=config.HYBRID_K_PER_QUERY
                    )
                    for res_item in variant_results:
                        idx = res_item.get('index')
                        if isinstance(idx, int) and idx >= 0 and idx not in collected_docs_data:
                             collected_docs_data[idx] = res_item
            query_metrics["search_time"] = time.time() - search_start
            query_metrics["num_unique_docs_found"] = len(collected_docs_data)

            unique_docs_list = list(collected_docs_data.values())
            unique_docs_list.sort(key=lambda x: x.get('hybrid_score', 0.0), reverse=True)
            docs_for_reranking_input = unique_docs_list[:config.MAX_DOCS_FOR_RERANK]
            query_metrics["num_docs_reranked"] = len(docs_for_reranking_input)

            rerank_start = time.time()
            reranked_results = utils.rerank_documents(
                summarizing_query, docs_for_reranking_input, reranking_model
            )
            query_metrics["rerank_time"] = time.time() - rerank_start

            final_retrieved_docs = reranked_results[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
            retrieved_ids = []
            for res in final_retrieved_docs:
                doc_data = res.get('doc', {}); chunk_id = None
                if isinstance(doc_data, dict):
                    chunk_id = doc_data.get('id') or doc_data.get('metadata', {}).get('id') or doc_data.get('metadata', {}).get('chunk_id')
                if chunk_id: retrieved_ids.append(str(chunk_id))
            query_metrics["retrieved_ids"] = retrieved_ids

            query_metrics["status"] = "evaluated"
            # Calculate metrics for all k values, including k=1
            for k in k_values:
                query_metrics[f'precision@{k}'] = precision_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'recall@{k}'] = recall_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'f1@{k}'] = f1_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'mrr@{k}'] = mrr_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'ndcg@{k}'] = ndcg_at_k(retrieved_ids, relevant_chunk_ids, k)

        except Exception as e:
            logging.exception(f"Error evaluating QID {query_id}: {e}")
            query_metrics["status"] = "error_runtime"
            query_metrics["error_message"] = str(e)
        finally:
            query_metrics["processing_time"] = time.time() - start_time
            results_list.append(query_metrics)
            progress_bar.progress((i + 1) / total_items)

    status_text.text(f"Ho√†n th√†nh ƒë√°nh gi√° {total_items} queries!")
    return pd.DataFrame(results_list)


def calculate_average_metrics(df_results: pd.DataFrame):
    evaluated_df = df_results[df_results['status'] == 'evaluated']
    num_evaluated = len(evaluated_df)
    if num_evaluated == 0:
        return None, num_evaluated, len(df_results) - num_evaluated

    avg_metrics = {}
    k_values = [1, 3, 5, 10] # <<< THAY ƒê·ªîI >>> Add K=1
    metric_keys_k = [f'{m}@{k}' for k in k_values for m in ['precision', 'recall', 'f1', 'mrr', 'ndcg']]
    timing_keys = ['processing_time', 'variation_time', 'search_time', 'rerank_time']
    count_keys = ['num_variations_generated', 'num_unique_docs_found', 'num_docs_reranked']

    for key in metric_keys_k + timing_keys + count_keys:
        total = evaluated_df[key].sum(skipna=True)
        avg_metrics[f'avg_{key}'] = total / num_evaluated if num_evaluated > 0 else 0.0

    return avg_metrics, num_evaluated, len(df_results) - num_evaluated


st.set_page_config(page_title="ƒê√°nh gi√° Retrieval", layout="wide")
st.title("üìä ƒê√°nh gi√° H·ªá th·ªëng Retrieval")

st.markdown("""
Trang n√†y cho ph√©p b·∫°n ch·∫°y ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa h·ªá th·ªëng retrieval (t√¨m ki·∫øm + x·∫øp h·∫°ng l·∫°i)
d·ª±a tr√™n m·ªôt t·∫≠p d·ªØ li·ªáu c√≥ ch·ª©a c√°c c√¢u h·ªèi v√† c√°c chunk t√†i li·ªáu li√™n quan (ground truth).
K·∫øt qu·∫£ s·∫Ω ƒë∆∞·ª£c l∆∞u tr·ªØ trong phi√™n l√†m vi·ªác n√†y ngay c·∫£ khi b·∫°n chuy·ªÉn tab.
""")

if 'eval_data' not in st.session_state:
    st.session_state.eval_data = None
if 'eval_results_df' not in st.session_state:
    st.session_state.eval_results_df = None
if 'eval_run_completed' not in st.session_state:
    st.session_state.eval_run_completed = False
if 'eval_uploaded_filename' not in st.session_state:
    st.session_state.eval_uploaded_filename = ""

st.subheader("Tr·∫°ng th√°i H·ªá th·ªëng C∆° b·∫£n")
init_ok = False
models_ready = False
retriever_instance = None

with st.spinner("Ki·ªÉm tra v√† kh·ªüi t·∫°o t√†i nguy√™n..."):
    g_embedding_model = utils.load_embedding_model(config.embedding_model_name)
    g_reranking_model = utils.load_reranker_model(config.reranking_model_name)
    try:
        vector_db, retriever_instance = data_loader.load_or_create_rag_components(g_embedding_model)
        if vector_db and retriever_instance and g_embedding_model and g_reranking_model:
            init_ok = True
            models_ready = True
            st.success("‚úÖ VectorDB, Retriever, Embedding, Reranker ƒë√£ s·∫µn s√†ng.")
        else:
            st.error("‚ö†Ô∏è L·ªói khi kh·ªüi t·∫°o VectorDB ho·∫∑c Retriever.")
    except Exception as e:
        st.error(f"‚ö†Ô∏è L·ªói nghi√™m tr·ªçng khi kh·ªüi t·∫°o h·ªá th·ªëng: {e}")

if init_ok:
    st.subheader("C·∫•u h√¨nh ƒê√°nh gi√°")
    st.markdown("ƒê√°nh gi√° s·∫Ω ƒë∆∞·ª£c ch·∫°y v·ªõi c√°c c·∫•u h√¨nh **hi·ªán t·∫°i** ƒë∆∞·ª£c ch·ªçn trong Sidebar c·ªßa trang Chatbot ch√≠nh.")

    current_gemini_model = st.session_state.get('selected_gemini_model', config.DEFAULT_GEMINI_MODEL)
    current_retrieval_mode = st.session_state.get('retrieval_mode', 'ƒê∆°n gi·∫£n')
    current_use_history = st.session_state.get('use_history_for_llm1', False) # Changed default to False to align with previous discussion

    col1, col2, col3 = st.columns(3)
    with col1: st.info(f"**Ch·∫ø ƒë·ªô Retrieval:** `{current_retrieval_mode}`")
    with col2: st.info(f"**Model Gemini (Variations):** `{current_gemini_model}`")
    with col3: st.info(f"**S·ª≠ d·ª•ng L·ªãch s·ª≠ (LLM1):** `{current_use_history}`")

    eval_config_dict = {
        'retrieval_mode': current_retrieval_mode,
        'use_history_for_llm1': current_use_history,
    }

    st.subheader("T·∫£i L√™n File ƒê√°nh gi√°")
    uploaded_file = st.file_uploader(
        "Ch·ªçn file JSON ch·ª©a d·ªØ li·ªáu ƒë√°nh gi√° (ƒë·ªãnh d·∫°ng: [{'query_id': ..., 'query': ..., 'relevant_chunk_ids': [...]}, ...])",
        type=["json"],
        key="eval_file_uploader"
    )

    if uploaded_file is not None:
        if uploaded_file.name != st.session_state.eval_uploaded_filename:
            try:
                file_content_bytes = uploaded_file.getvalue()
                eval_data_list = json.loads(file_content_bytes.decode('utf-8'))

                st.session_state.eval_data = eval_data_list
                st.session_state.eval_uploaded_filename = uploaded_file.name
                st.session_state.eval_run_completed = False
                st.session_state.eval_results_df = None
                st.success(f"ƒê√£ t·∫£i v√† l∆∞u tr·ªØ file '{uploaded_file.name}' ch·ª©a {len(eval_data_list)} c√¢u h·ªèi.")
            except json.JSONDecodeError:
                st.error("L·ªói: File t·∫£i l√™n kh√¥ng ph·∫£i l√† ƒë·ªãnh d·∫°ng JSON h·ª£p l·ªá.")
                st.session_state.eval_data = None
                st.session_state.eval_uploaded_filename = ""
                st.session_state.eval_run_completed = False
                st.session_state.eval_results_df = None
            except Exception as e:
                st.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi x·ª≠ l√Ω file: {e}")
                logging.exception("Unhandled error during file processing.")
                st.session_state.eval_data = None
                st.session_state.eval_uploaded_filename = ""
                st.session_state.eval_run_completed = False
                st.session_state.eval_results_df = None

    if st.session_state.eval_data is not None:
        st.info(f"ƒêang s·ª≠ d·ª•ng d·ªØ li·ªáu t·ª´ file: **{st.session_state.eval_uploaded_filename}** ({len(st.session_state.eval_data)} c√¢u h·ªèi).")

        if st.checkbox("Hi·ªÉn th·ªã d·ªØ li·ªáu m·∫´u (5 d√≤ng ƒë·∫ßu)", key="show_eval_data_preview"):
            st.dataframe(pd.DataFrame(st.session_state.eval_data).head())

        if st.button("üöÄ B·∫Øt ƒë·∫ßu ƒê√°nh gi√°", key="start_eval_button"):
            with st.spinner(f"ƒêang t·∫£i model Gemini: {current_gemini_model}..."):
                g_gemini_model = utils.load_gemini_model(current_gemini_model)

            if g_gemini_model:
                with st.spinner("‚è≥ ƒêang ch·∫°y ƒë√°nh gi√°... Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t."):
                    start_eval_time = time.time()
                    results_df = run_retrieval_evaluation(
                        eval_data=st.session_state.eval_data,
                        hybrid_retriever=retriever_instance,
                        embedding_model=g_embedding_model,
                        reranking_model=g_reranking_model,
                        gemini_model=g_gemini_model,
                        eval_config=eval_config_dict
                    )
                    total_eval_time = time.time() - start_eval_time
                    st.info(f"Ho√†n th√†nh ƒë√°nh gi√° sau {total_eval_time:.2f} gi√¢y.")

                    st.session_state.eval_results_df = results_df
                    st.session_state.eval_run_completed = True
                    st.rerun()
            else:
                st.error(f"Kh√¥ng th·ªÉ t·∫£i model Gemini: {current_gemini_model}. Kh√¥ng th·ªÉ ch·∫°y ƒë√°nh gi√°.")

    if st.session_state.eval_run_completed and st.session_state.eval_results_df is not None:
        st.subheader("K·∫øt qu·∫£ ƒê√°nh gi√°")
        detailed_results_df = st.session_state.eval_results_df

        avg_metrics, num_eval, num_skipped_error = calculate_average_metrics(detailed_results_df)

        st.metric("T·ªïng s·ªë Queries", len(detailed_results_df))
        col_res1, col_res2 = st.columns(2)
        col_res1.metric("Queries ƒê√°nh gi√° H·ª£p l·ªá", num_eval)
        col_res2.metric("Queries B·ªè qua/L·ªói", num_skipped_error)

        if avg_metrics:
            st.markdown("#### Metrics Trung b√¨nh (tr√™n c√°c queries h·ª£p l·ªá)")
            k_values_display = [1, 3, 5, 10] # <<< THAY ƒê·ªîI >>> Add K=1
            cols_k = st.columns(len(k_values_display)) # Adjust columns number if needed
            for idx, k in enumerate(k_values_display):
                with cols_k[idx]:
                    st.markdown(f"**K = {k}**")
                    st.text(f"Precision: {avg_metrics.get(f'avg_precision@{k}', 0.0):.4f}")
                    st.text(f"Recall:    {avg_metrics.get(f'avg_recall@{k}', 0.0):.4f}")
                    st.text(f"F1:        {avg_metrics.get(f'avg_f1@{k}', 0.0):.4f}")
                    st.text(f"MRR:       {avg_metrics.get(f'avg_mrr@{k}', 0.0):.4f}")
                    st.text(f"NDCG:      {avg_metrics.get(f'avg_ndcg@{k}', 0.0):.4f}")

            st.markdown("#### Th√¥ng tin Hi·ªáu nƒÉng Trung b√¨nh")
            col_perf1, col_perf2, col_perf3 = st.columns(3)
            col_perf1.metric("Avg Total Time/Query (s)", f"{avg_metrics.get('avg_processing_time', 0.0):.3f}")
            col_perf2.metric("Avg Variation Time (s)", f"{avg_metrics.get('avg_variation_time', 0.0):.3f}")
            col_perf3.metric("Avg Search Time (s)", f"{avg_metrics.get('avg_search_time', 0.0):.3f}")


        with st.expander("Xem K·∫øt qu·∫£ Chi ti·∫øt cho t·ª´ng Query"):
            # <<< THAY ƒê·ªîI >>> Add @1 metrics
            display_columns = [
                'query_id', 'query', 'status', 'retrieval_mode',
                'precision@1','mrr@1', 'precision@3', 'recall@10', 'mrr@10', 'ndcg@10',
                'processing_time', 'retrieved_ids'
            ]
            existing_display_columns = [col for col in display_columns if col in detailed_results_df.columns]
            st.dataframe(detailed_results_df[existing_display_columns])

        st.subheader("L∆∞u K·∫øt qu·∫£ Chi ti·∫øt")
        try:
            results_json = detailed_results_df.to_json(orient='records', indent=2, force_ascii=False)
            results_csv = detailed_results_df.to_csv(index=False).encode('utf-8')

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            mode_suffix = current_retrieval_mode.lower().replace(' ', '_')
            fname_json = f"evaluation_results_{mode_suffix}_{timestamp}.json"
            fname_csv = f"evaluation_results_{mode_suffix}_{timestamp}.csv"

            col_dl1, col_dl2 = st.columns(2)
            with col_dl1:
                st.download_button(
                    label="üíæ T·∫£i v·ªÅ JSON", data=results_json, file_name=fname_json, mime="application/json",
                    key="download_json_eval"
                )
            with col_dl2:
                st.download_button(
                    label="üíæ T·∫£i v·ªÅ CSV", data=results_csv, file_name=fname_csv, mime="text/csv",
                    key="download_csv_eval"
                )
        except Exception as e:
            st.error(f"L·ªói khi chu·∫©n b·ªã file ƒë·ªÉ t·∫£i v·ªÅ: {e}")

    st.markdown("---")
    st.subheader("Qu·∫£n l√Ω Tr·∫°ng th√°i ƒê√°nh gi√°")
    if st.button("X√≥a File ƒê√£ T·∫£i v√† K·∫øt Qu·∫£ ƒê√°nh Gi√°", key="clear_eval_state"):
        st.session_state.eval_data = None
        st.session_state.eval_uploaded_filename = ""
        st.session_state.eval_run_completed = False
        st.session_state.eval_results_df = None
        st.success("ƒê√£ x√≥a tr·∫°ng th√°i ƒë√°nh gi√°. Vui l√≤ng t·∫£i l·∫°i file n·∫øu mu·ªën ch·∫°y l·∫°i.")
        time.sleep(1)
        st.rerun()

else:
    st.warning("‚ö†Ô∏è H·ªá th·ªëng c∆° b·∫£n ch∆∞a s·∫µn s√†ng. Vui l√≤ng ki·ªÉm tra l·∫°i trang Chatbot ch√≠nh ho·∫∑c kh·ªüi ƒë·ªông l·∫°i ·ª©ng d·ª•ng.")