# pages/2_Evaluation.py
import time
import streamlit as st
import pandas as pd
import json
import time
import math
import os
import logging
from datetime import datetime
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import config
import utils
import data_loader
# B·ªè import SimpleVectorDatabase v√¨ kh√¥ng d√πng tr·ª±c ti·∫øp
from retriever import HybridRetriever # Ch·ªâ c·∫ßn import HybridRetriever

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- C√°c h√†m t√≠nh to√°n metrics (gi·ªØ nguy√™n) ---
def precision_at_k(retrieved_ids, relevant_ids, k):
    if k <= 0: return 0.0
    retrieved_at_k = retrieved_ids[:k]; relevant_set = set(relevant_ids)
    if not relevant_set: return 0.0
    intersect = set(retrieved_at_k) & relevant_set
    return len(intersect) / k

def recall_at_k(retrieved_ids, relevant_ids, k):
    relevant_set = set(relevant_ids)
    if not relevant_set: return 1.0 # N·∫øu kh√¥ng c√≥ relevant th√¨ recall l√† 1? Ho·∫∑c 0? Xem l·∫°i logic n·∫øu c·∫ßn
    retrieved_at_k = retrieved_ids[:k]
    intersect = set(retrieved_at_k) & relevant_set
    return len(intersect) / len(relevant_set)

def f1_at_k(retrieved_ids, relevant_ids, k):
    prec = precision_at_k(retrieved_ids, relevant_ids, k); rec = recall_at_k(retrieved_ids, relevant_ids, k)
    return 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0

def mrr_at_k(retrieved_ids, relevant_ids, k):
    relevant_set = set(relevant_ids);
    if not relevant_set: return 0.0
    for rank, doc_id in enumerate(retrieved_ids[:k], 1):
        if doc_id in relevant_set: return 1.0 / rank
    return 0.0

def ndcg_at_k(retrieved_ids, relevant_ids, k):
    relevant_set = set(relevant_ids);
    if not relevant_set: return 1.0 # N·∫øu kh√¥ng c√≥ relevant th√¨ NDCG l√† 1? Ho·∫∑c 0? Xem l·∫°i logic
    retrieved_at_k = retrieved_ids[:k]; dcg = 0.0; idcg = 0.0
    for i, doc_id in enumerate(retrieved_at_k):
        if doc_id in relevant_set: dcg += 1.0 / math.log2(i + 2)
    # IDCG t√≠nh d·ª±a tr√™n s·ªë l∆∞·ª£ng relevant th·ª±c t·∫ø, t·ªëi ƒëa l√† k
    for i in range(min(k, len(relevant_set))): idcg += 1.0 / math.log2(i + 2)
    return dcg / idcg if idcg > 0 else 0.0


def run_retrieval_evaluation(
    eval_data: list,
    hybrid_retriever: HybridRetriever,
    embedding_model,
    reranking_model,
    gemini_model,
    eval_config: dict # Ch·ª©a retrieval_mode v√† use_history_for_llm1
    ):

    results_list = []
    k_values = [1, 3, 5, 10] # C√°c gi√° tr·ªã K ƒë·ªÉ t√≠nh metrics

    # L·∫•y c·∫•u h√¨nh t·ª´ eval_config
    retrieval_mode = eval_config.get('retrieval_mode', 'T·ªïng qu√°t') # M·∫∑c ƒë·ªãnh l√† T·ªïng qu√°t
    use_history = eval_config.get('use_history_for_llm1', False)
    # T·∫°o l·ªãch s·ª≠ gi·∫£ n·∫øu c·∫ßn cho b∆∞·ªõc t·∫°o variation
    dummy_history = [{"role": "user", "content": "C√¢u h·ªèi tr∆∞·ªõc ƒë√≥ (n·∫øu c√≥)"}] if use_history else None

    progress_bar = st.progress(0)
    status_text = st.empty()

    total_items = len(eval_data)
    queries_per_batch = 15 # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng query tr∆∞·ªõc khi t·∫°m d·ª´ng (tr√°nh l·ªói API)
    wait_time_seconds = 60 # Th·ªùi gian t·∫°m d·ª´ng (gi√¢y)

    for i, item in enumerate(eval_data):
        # T·∫°m d·ª´ng sau m·ªói batch ƒë·ªÉ tr√°nh rate limit c·ªßa API (n·∫øu c·∫ßn)
        if i > 0 and i % queries_per_batch == 0:
            pause_msg = f"ƒê√£ x·ª≠ l√Ω {i}/{total_items} queries. T·∫°m d·ª´ng {wait_time_seconds} gi√¢y ƒë·ªÉ tr√°nh l·ªói API..."
            logging.info(pause_msg)
            status_text.text(pause_msg)
            time.sleep(wait_time_seconds)
            status_text.text(f"Ti·∫øp t·ª•c x·ª≠ l√Ω query {i+1}/{total_items}...")

        query_id = item.get("query_id"); original_query = item.get("query")
        relevant_chunk_ids = set(item.get("relevant_chunk_ids", [])) # D√πng set ƒë·ªÉ ki·ªÉm tra nhanh h∆°n
        if not query_id or not original_query:
            logging.warning(f"B·ªè qua m·ª•c {i} do thi·∫øu query_id ho·∫∑c query.")
            continue # B·ªè qua n·∫øu thi·∫øu th√¥ng tin c∆° b·∫£n

        status_text.text(f"ƒêang x·ª≠ l√Ω query {i+1}/{total_items}: {query_id} (Mode: {retrieval_mode})")
        logging.info(f"Eval - Processing QID: {query_id} with Mode: {retrieval_mode}")

        start_time = time.time()
        # Kh·ªüi t·∫°o dictionary ch·ª©a k·∫øt qu·∫£ cho query n√†y
        query_metrics = {
            "query_id": query_id, "query": original_query,
            "retrieval_mode": retrieval_mode, # L∆∞u l·∫°i ch·∫ø ƒë·ªô ƒëang d√πng
            "use_history_llm1": use_history, # L∆∞u l·∫°i tr·∫°ng th√°i d√πng history
            "status": "error", # Tr·∫°ng th√°i m·∫∑c ƒë·ªãnh l√† l·ªói
            "retrieved_ids": [], "relevant_ids": list(relevant_chunk_ids), # L∆∞u c·∫£ relevant ids ƒë·ªÉ ƒë·ªëi chi·∫øu
            "processing_time": 0.0
        }
        # Kh·ªüi t·∫°o t·∫•t c·∫£ c√°c metrics v·ªÅ 0.0
        for k in k_values:
            query_metrics[f'precision@{k}'] = 0.0
            query_metrics[f'recall@{k}'] = 0.0
            query_metrics[f'f1@{k}'] = 0.0
            query_metrics[f'mrr@{k}'] = 0.0
            query_metrics[f'ndcg@{k}'] = 0.0
        # Kh·ªüi t·∫°o c√°c th√¥ng s·ªë th·ªùi gian v√† s·ªë l∆∞·ª£ng
        timing_keys = ['variation_time', 'search_time', 'rerank_time']
        count_keys = ['num_variations_generated', 'num_unique_docs_found', 'num_docs_reranked']
        for k in timing_keys + count_keys: query_metrics[k] = 0.0
        query_metrics['summarizing_query'] = '' # Kh·ªüi t·∫°o summarizing_query

        try:
            # B∆∞·ªõc 1: T·∫°o variations v√† summarizing query (lu√¥n ch·∫°y b∆∞·ªõc n√†y)
            variation_start = time.time()
            relevance_status, _, all_queries, summarizing_query = utils.generate_query_variations(
                original_query=original_query,
                gemini_model=gemini_model,
                chat_history=dummy_history, # Truy·ªÅn dummy_history
                num_variations=config.NUM_QUERY_VARIATIONS # ƒê·∫£m b·∫£o d√πng config
            )
            query_metrics["variation_time"] = time.time() - variation_start
            query_metrics["summarizing_query"] = summarizing_query
            # all_queries ƒë√£ bao g·ªìm original_query trong h√†m generate_query_variations
            query_metrics["num_variations_generated"] = len(all_queries) -1 # S·ªë bi·∫øn th·ªÉ = t·ªïng - 1 (c√¢u g·ªëc)


            # Ki·ªÉm tra relevancy (n·∫øu c·∫ßn b·ªè qua c√°c query kh√¥ng li√™n quan)
            if relevance_status == 'invalid':
                query_metrics["status"] = "skipped_irrelevant"
                query_metrics["processing_time"] = time.time() - start_time
                results_list.append(query_metrics)
                progress_bar.progress((i + 1) / total_items)
                logging.info(f"QID {query_id} skipped as irrelevant.")
                continue # Chuy·ªÉn sang query ti·∫øp theo

            # B∆∞·ªõc 2: Retrieval d·ª±a tr√™n retrieval_mode
            collected_docs_data = {} # Dict ƒë·ªÉ l∆∞u c√°c docs t√¨m ƒë∆∞·ª£c {index: {'doc': ..., 'hybrid_score': ...}}
            search_start = time.time()
            query_for_reranking = original_query # Query m·∫∑c ƒë·ªãnh ƒë·ªÉ rerank

            if retrieval_mode == 'ƒê∆°n gi·∫£n':
                logging.debug(f"QID {query_id}: Running Simple Search with: '{original_query}'")
                variant_results = hybrid_retriever.hybrid_search(
                    original_query, embedding_model, # <<< D√πng c√¢u g·ªëc
                    vector_search_k=config.VECTOR_K_PER_QUERY,
                    final_k=config.HYBRID_K_PER_QUERY
                )
                for res_item in variant_results:
                    idx = res_item.get('index')
                    # Ch·ªâ th√™m n·∫øu l√† s·ªë nguy√™n v√† h·ª£p l·ªá
                    if isinstance(idx, int) and idx >= 0:
                        collected_docs_data[idx] = res_item # Ghi ƒë√® n·∫øu ƒë√£ c√≥ (kh√¥ng c·∫ßn thi·∫øt v√¨ ch·ªâ search 1 l·∫ßn)
                query_for_reranking = original_query

            elif retrieval_mode == 'T·ªïng qu√°t':
                logging.debug(f"QID {query_id}: Running General Search with: '{summarizing_query}'")
                variant_results = hybrid_retriever.hybrid_search(
                    summarizing_query, embedding_model, # <<< D√πng c√¢u t√≥m t·∫Øt
                    vector_search_k=config.VECTOR_K_PER_QUERY,
                    final_k=config.HYBRID_K_PER_QUERY
                )
                for res_item in variant_results:
                    idx = res_item.get('index')
                    if isinstance(idx, int) and idx >= 0:
                        collected_docs_data[idx] = res_item
                query_for_reranking = summarizing_query

            elif retrieval_mode == 'S√¢u':
                logging.debug(f"QID {query_id}: Running Deep Search with {len(all_queries)} queries...")
                # all_queries ƒë√£ ch·ª©a original_query
                for q_variant in all_queries:
                    logging.debug(f"  - Deep searching with variant: '{q_variant[:100]}...'")
                    variant_results = hybrid_retriever.hybrid_search(
                        q_variant, embedding_model, # <<< D√πng t·ª´ng query trong all_queries
                        vector_search_k=config.VECTOR_K_PER_QUERY,
                        final_k=config.HYBRID_K_PER_QUERY # L·∫•y K k·∫øt qu·∫£ cho m·ªói variant
                    )
                    for res_item in variant_results:
                        idx = res_item.get('index')
                        # Ch·ªâ th√™m n·∫øu ch∆∞a c√≥ ƒë·ªÉ tr√°nh tr√πng l·∫∑p t·ª´ c√°c variant kh√°c nhau
                        if isinstance(idx, int) and idx >= 0 and idx not in collected_docs_data:
                             collected_docs_data[idx] = res_item
                # Cho ch·∫ø ƒë·ªô S√¢u, rerank b·∫±ng c√¢u t√≥m t·∫Øt (ho·∫∑c c√≥ th·ªÉ ƒë·ªïi th√†nh c√¢u g·ªëc n·∫øu mu·ªën)
                query_for_reranking = summarizing_query

            query_metrics["search_time"] = time.time() - search_start
            query_metrics["num_unique_docs_found"] = len(collected_docs_data)
            logging.debug(f"QID {query_id}: Found {len(collected_docs_data)} unique docs.")

            # Chu·∫©n b·ªã danh s√°ch docs ƒë·ªÉ rerank
            unique_docs_list = list(collected_docs_data.values())
            # S·∫Øp x·∫øp theo hybrid_score gi·∫£m d·∫ßn (n·∫øu c√≥) tr∆∞·ªõc khi c·∫Øt
            unique_docs_list.sort(key=lambda x: x.get('hybrid_score', 0.0), reverse=True)
            docs_for_reranking_input = unique_docs_list[:config.MAX_DOCS_FOR_RERANK]
            query_metrics["num_docs_reranked"] = len(docs_for_reranking_input)
            logging.debug(f"QID {query_id}: Prepared {len(docs_for_reranking_input)} docs for reranking.")


            # B∆∞·ªõc 3: Re-ranking
            rerank_start = time.time()
            reranked_results = [] # Kh·ªüi t·∫°o list r·ªóng
            if docs_for_reranking_input: # Ch·ªâ rerank n·∫øu c√≥ t√†i li·ªáu
                logging.debug(f"QID {query_id}: Reranking with query: '{query_for_reranking[:100]}...'")
                reranked_results = utils.rerank_documents(
                    query_for_reranking, # Query d√πng ƒë·ªÉ rerank ƒë√£ x√°c ƒë·ªãnh ·ªü b∆∞·ªõc tr∆∞·ªõc
                    docs_for_reranking_input, # Danh s√°ch c√°c dict {'doc': ..., 'index': ...}
                    reranking_model
                )
            else:
                 logging.debug(f"QID {query_id}: No documents to rerank.")
            query_metrics["rerank_time"] = time.time() - rerank_start

            # B∆∞·ªõc 4: L·∫•y IDs v√† T√≠nh Metrics
            # L·∫•y top K k·∫øt qu·∫£ cu·ªëi c√πng sau rerank
            final_retrieved_docs = reranked_results[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
            retrieved_ids = []
            for res in final_retrieved_docs:
                # C·ªë g·∫Øng l·∫•y 'id' ho·∫∑c 'chunk_id' t·ª´ metadata ho·∫∑c t·ª´ key 'id' tr·ª±c ti·∫øp
                doc_data = res.get('doc', {})
                chunk_id = None
                if isinstance(doc_data, dict):
                    # ∆Øu ti√™n l·∫•y t·ª´ key 'id' n·∫øu c√≥, sau ƒë√≥ m·ªõi ƒë·∫øn metadata
                    chunk_id = doc_data.get('id')
                    if not chunk_id:
                        metadata = doc_data.get('metadata', {})
                        if isinstance(metadata, dict):
                             chunk_id = metadata.get('id') or metadata.get('chunk_id')

                if chunk_id:
                    retrieved_ids.append(str(chunk_id)) # ƒê·∫£m b·∫£o ID l√† string

            query_metrics["retrieved_ids"] = retrieved_ids
            logging.debug(f"QID {query_id}: Final retrieved IDs (top {len(retrieved_ids)}): {retrieved_ids}")


            query_metrics["status"] = "evaluated" # ƒê√°nh d·∫•u l√† ƒë√£ ƒë√°nh gi√° th√†nh c√¥ng
            # T√≠nh to√°n t·∫•t c·∫£ c√°c metrics v·ªõi c√°c gi√° tr·ªã K
            for k in k_values:
                query_metrics[f'precision@{k}'] = precision_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'recall@{k}'] = recall_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'f1@{k}'] = f1_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'mrr@{k}'] = mrr_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'ndcg@{k}'] = ndcg_at_k(retrieved_ids, relevant_chunk_ids, k)
                logging.debug(f"  Metrics @{k}: P={query_metrics[f'precision@{k}']:.4f}, R={query_metrics[f'recall@{k}']:.4f}, F1={query_metrics[f'f1@{k}']:.4f}, MRR={query_metrics[f'mrr@{k}']:.4f}, NDCG={query_metrics[f'ndcg@{k}']:.4f}")


        except Exception as e:
            logging.exception(f"Error evaluating QID {query_id}: {e}") # Log c·∫£ traceback
            query_metrics["status"] = "error_runtime"
            query_metrics["error_message"] = str(e) # Ghi l·∫°i th√¥ng b√°o l·ªói
        finally:
            query_metrics["processing_time"] = time.time() - start_time
            results_list.append(query_metrics) # Th√™m k·∫øt qu·∫£ (k·ªÉ c·∫£ l·ªói) v√†o danh s√°ch
            progress_bar.progress((i + 1) / total_items) # C·∫≠p nh·∫≠t progress bar

    status_text.text(f"Ho√†n th√†nh ƒë√°nh gi√° {total_items} queries!")
    logging.info(f"Finished evaluation for {total_items} queries.")
    return pd.DataFrame(results_list)


def calculate_average_metrics(df_results: pd.DataFrame):
    # Ch·ªâ t√≠nh trung b√¨nh tr√™n c√°c query ƒë∆∞·ª£c ƒë√°nh gi√° th√†nh c√¥ng
    evaluated_df = df_results[df_results['status'] == 'evaluated'].copy() # T·∫°o b·∫£n sao ƒë·ªÉ tr√°nh SettingWithCopyWarning
    num_evaluated = len(evaluated_df)
    num_skipped_error = len(df_results) - num_evaluated

    if num_evaluated == 0:
        logging.warning("No queries were successfully evaluated. Cannot calculate average metrics.")
        return None, num_evaluated, num_skipped_error

    avg_metrics = {}
    k_values = [1, 3, 5, 10] # C√°c gi√° tr·ªã K ƒë√£ t√≠nh
    # Danh s√°ch c√°c c·ªôt metrics c·∫ßn t√≠nh trung b√¨nh
    metric_keys_k = [f'{m}@{k}' for k in k_values for m in ['precision', 'recall', 'f1', 'mrr', 'ndcg']]
    timing_keys = ['processing_time', 'variation_time', 'search_time', 'rerank_time']
    count_keys = ['num_variations_generated', 'num_unique_docs_found', 'num_docs_reranked']

    all_keys_to_average = metric_keys_k + timing_keys + count_keys

    for key in all_keys_to_average:
        if key in evaluated_df.columns: # Ki·ªÉm tra xem c·ªôt c√≥ t·ªìn t·∫°i kh√¥ng
             # Chuy·ªÉn ƒë·ªïi c·ªôt sang d·∫°ng s·ªë, l·ªói s·∫Ω th√†nh NaN
             evaluated_df[key] = pd.to_numeric(evaluated_df[key], errors='coerce')
             # T√≠nh t·ªïng b·ªè qua NaN
             total = evaluated_df[key].sum(skipna=True)
             # ƒê·∫øm s·ªë l∆∞·ª£ng gi√° tr·ªã kh√¥ng ph·∫£i NaN ƒë·ªÉ chia trung b√¨nh
             valid_count = evaluated_df[key].notna().sum()
             # T√≠nh trung b√¨nh, tr√°nh chia cho 0
             avg_metrics[f'avg_{key}'] = total / valid_count if valid_count > 0 else 0.0
        else:
             logging.warning(f"Metric key '{key}' not found in results DataFrame for averaging.")
             avg_metrics[f'avg_{key}'] = 0.0 # ƒê·∫∑t gi√° tr·ªã m·∫∑c ƒë·ªãnh n·∫øu c·ªôt kh√¥ng t·ªìn t·∫°i

    logging.info(f"Calculated average metrics over {num_evaluated} evaluated queries.")
    return avg_metrics, num_evaluated, num_skipped_error


# --- Giao di·ªán Streamlit ---
st.set_page_config(page_title="ƒê√°nh gi√° Retrieval", layout="wide")
st.title("üìä ƒê√°nh gi√° H·ªá th·ªëng Retrieval")

st.markdown("""
Trang n√†y cho ph√©p b·∫°n ch·∫°y ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa h·ªá th·ªëng retrieval (t√¨m ki·∫øm + x·∫øp h·∫°ng l·∫°i)
d·ª±a tr√™n m·ªôt t·∫≠p d·ªØ li·ªáu c√≥ ch·ª©a c√°c c√¢u h·ªèi v√† c√°c chunk t√†i li·ªáu li√™n quan (ground truth).
K·∫øt qu·∫£ ƒë√°nh gi√° cho l·∫ßn ch·∫°y g·∫ßn nh·∫•t s·∫Ω ƒë∆∞·ª£c l∆∞u trong phi√™n l√†m vi·ªác n√†y.
""")

# --- Kh·ªüi t·∫°o ho·∫∑c ki·ªÉm tra Session State ---
if 'eval_data' not in st.session_state:
    st.session_state.eval_data = None # D·ªØ li·ªáu ƒë√°nh gi√° ƒë√£ t·∫£i
if 'eval_results_df' not in st.session_state:
    st.session_state.eval_results_df = None # DataFrame k·∫øt qu·∫£
if 'eval_run_completed' not in st.session_state:
    st.session_state.eval_run_completed = False # ƒê√°nh d·∫•u ƒë√£ ch·∫°y xong
if 'eval_uploaded_filename' not in st.session_state:
    st.session_state.eval_uploaded_filename = "" # T√™n file ƒë√£ t·∫£i
if 'last_eval_config' not in st.session_state:
    st.session_state.last_eval_config = {} # L∆∞u c·∫•u h√¨nh c·ªßa l·∫ßn ch·∫°y cu·ªëi

st.subheader("Tr·∫°ng th√°i H·ªá th·ªëng C∆° b·∫£n")
init_ok = False
retriever_instance = None
g_embedding_model = None
g_reranking_model = None

# S·ª≠ d·ª•ng spinner ƒë·ªÉ hi·ªÉn th·ªã qu√° tr√¨nh kh·ªüi t·∫°o
with st.spinner("Ki·ªÉm tra v√† kh·ªüi t·∫°o t√†i nguy√™n c·ªët l√µi (models, vectorDB, retriever)..."):
    try:
        # T·∫£i c√°c model c·∫ßn thi·∫øt (cache resource s·∫Ω ho·∫°t ƒë·ªông n·∫øu ƒë√£ t·∫£i tr∆∞·ªõc ƒë√≥)
        g_embedding_model = utils.load_embedding_model(config.embedding_model_name)
        g_reranking_model = utils.load_reranker_model(config.reranking_model_name)

        # T·∫£i ho·∫∑c t·∫°o VectorDB v√† Retriever (cache resource s·∫Ω ho·∫°t ƒë·ªông)
        # Gi·∫£ s·ª≠ cached_load_or_create_components tr·∫£ v·ªÅ (vector_db, hybrid_retriever)
        _, retriever_instance = data_loader.load_or_create_rag_components(g_embedding_model)

        # Ki·ªÉm tra t·∫•t c·∫£ th√†nh ph·∫ßn ƒë√£ s·∫µn s√†ng ch∆∞a
        if retriever_instance and g_embedding_model and g_reranking_model:
            init_ok = True
            st.success("‚úÖ VectorDB, Retriever, Embedding Model, Reranker Model ƒë√£ s·∫µn s√†ng.")
            logging.info("Core components initialized successfully.")
        else:
            missing_components = []
            if not retriever_instance: missing_components.append("Retriever")
            if not g_embedding_model: missing_components.append("Embedding Model")
            if not g_reranking_model: missing_components.append("Reranker Model")
            st.error(f"‚ö†Ô∏è L·ªói: Kh√¥ng th·ªÉ kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn: {', '.join(missing_components)}.")
            logging.error(f"Failed to initialize components: {', '.join(missing_components)}.")

    except Exception as e:
        st.error(f"‚ö†Ô∏è L·ªói nghi√™m tr·ªçng trong qu√° tr√¨nh kh·ªüi t·∫°o h·ªá th·ªëng: {e}")
        logging.exception("Critical error during system initialization.")

# Ch·ªâ hi·ªÉn th·ªã ph·∫ßn c√≤n l·∫°i n·∫øu h·ªá th·ªëng c∆° b·∫£n OK
if init_ok:
    st.subheader("C·∫•u h√¨nh ƒê√°nh gi√°")
    st.markdown("ƒê√°nh gi√° s·∫Ω ƒë∆∞·ª£c th·ª±c hi·ªán v·ªõi c√°c c·∫•u h√¨nh **hi·ªán t·∫°i** ƒë∆∞·ª£c ch·ªçn trong **Sidebar c·ªßa trang Chatbot ch√≠nh**.")

    # L·∫•y c·∫•u h√¨nh hi·ªán t·∫°i t·ª´ session state c·ªßa trang Chatbot
    # Cung c·∫•p gi√° tr·ªã m·∫∑c ƒë·ªãnh n·∫øu key kh√¥ng t·ªìn t·∫°i
    current_gemini_model = st.session_state.get('selected_gemini_model', config.DEFAULT_GEMINI_MODEL)
    current_retrieval_mode = st.session_state.get('retrieval_mode', 'T·ªïng qu√°t') # M·∫∑c ƒë·ªãnh 'T·ªïng qu√°t'
    current_use_history = st.session_state.get('use_history_for_llm1', False) # M·∫∑c ƒë·ªãnh True

    # Hi·ªÉn th·ªã c·∫•u h√¨nh s·∫Ω s·ª≠ d·ª•ng
    col1, col2, col3 = st.columns(3)
    with col1: st.info(f"**Ch·∫ø ƒë·ªô Retrieval:** `{current_retrieval_mode}`")
    with col2: st.info(f"**Model Gemini (Variations):** `{current_gemini_model}`")
    with col3: st.info(f"**S·ª≠ d·ª•ng L·ªãch s·ª≠ (LLM1):** `{current_use_history}`")

    # T·∫°o dict c·∫•u h√¨nh s·∫Ω truy·ªÅn v√†o h√†m ƒë√°nh gi√°
    eval_config_dict = {
        'retrieval_mode': current_retrieval_mode,
        'use_history_for_llm1': False,
        'gemini_model_name': current_gemini_model, 
    }

    st.subheader("T·∫£i L√™n File ƒê√°nh gi√°")
    uploaded_file = st.file_uploader(
        "Ch·ªçn file JSON ch·ª©a d·ªØ li·ªáu ƒë√°nh gi√° (ƒë·ªãnh d·∫°ng: [{'query_id': ..., 'query': ..., 'relevant_chunk_ids': [...]}, ...])",
        type=["json"],
        key="eval_file_uploader", # Key ƒë·ªÉ gi·ªØ tr·∫°ng th√°i c·ªßa uploader
        accept_multiple_files=False # Ch·ªâ cho ph√©p t·∫£i 1 file
    )

    # X·ª≠ l√Ω file ƒë∆∞·ª£c t·∫£i l√™n
    if uploaded_file is not None:
        # Ch·ªâ x·ª≠ l√Ω l·∫°i n·∫øu t√™n file thay ƒë·ªïi (tr√°nh load l·∫°i khi rerun kh√¥ng c·∫ßn thi·∫øt)
        if uploaded_file.name != st.session_state.eval_uploaded_filename:
            try:
                # ƒê·ªçc n·ªôi dung file
                file_content_bytes = uploaded_file.getvalue()
                # Decode UTF-8 v√† parse JSON
                eval_data_list = json.loads(file_content_bytes.decode('utf-8'))

                # Ki·ªÉm tra ƒë·ªãnh d·∫°ng c∆° b·∫£n c·ªßa d·ªØ li·ªáu
                if not isinstance(eval_data_list, list) or not all(isinstance(item, dict) for item in eval_data_list):
                     raise ValueError("D·ªØ li·ªáu kh√¥ng ph·∫£i l√† m·ªôt danh s√°ch c√°c dictionary.")
                if not all('query_id' in item and 'query' in item and 'relevant_chunk_ids' in item for item in eval_data_list):
                     raise ValueError("M·ªói m·ª•c ph·∫£i ch·ª©a 'query_id', 'query', v√† 'relevant_chunk_ids'.")

                # L∆∞u d·ªØ li·ªáu v√† tr·∫°ng th√°i v√†o session state
                st.session_state.eval_data = eval_data_list
                st.session_state.eval_uploaded_filename = uploaded_file.name
                # Reset tr·∫°ng th√°i ƒë√°nh gi√° c≈© khi c√≥ file m·ªõi
                st.session_state.eval_run_completed = False
                st.session_state.eval_results_df = None
                st.session_state.last_eval_config = {} # X√≥a config c≈©
                st.success(f"ƒê√£ t·∫£i v√† x√°c th·ª±c th√†nh c√¥ng file '{uploaded_file.name}' ch·ª©a {len(eval_data_list)} c√¢u h·ªèi.")
                logging.info(f"Successfully loaded and validated evaluation file: {uploaded_file.name}")

            except json.JSONDecodeError as e:
                st.error(f"L·ªói: File t·∫£i l√™n kh√¥ng ph·∫£i l√† ƒë·ªãnh d·∫°ng JSON h·ª£p l·ªá. Chi ti·∫øt: {e}")
                logging.error(f"JSONDecodeError while processing uploaded file: {e}")
                # Reset tr·∫°ng th√°i n·∫øu file l·ªói
                st.session_state.eval_data = None
                st.session_state.eval_uploaded_filename = ""
                st.session_state.eval_run_completed = False
                st.session_state.eval_results_df = None
            except ValueError as e:
                 st.error(f"L·ªói: D·ªØ li·ªáu trong file kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng y√™u c·∫ßu. Chi ti·∫øt: {e}")
                 logging.error(f"ValueError (Invalid data format) while processing uploaded file: {e}")
                 # Reset tr·∫°ng th√°i n·∫øu file l·ªói
                 st.session_state.eval_data = None
                 st.session_state.eval_uploaded_filename = ""
                 st.session_state.eval_run_completed = False
                 st.session_state.eval_results_df = None
            except Exception as e:
                st.error(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi x·ª≠ l√Ω file: {e}")
                logging.exception("Unhandled error during evaluation file processing.")
                # Reset tr·∫°ng th√°i n·∫øu c√≥ l·ªói kh√°c
                st.session_state.eval_data = None
                st.session_state.eval_uploaded_filename = ""
                st.session_state.eval_run_completed = False
                st.session_state.eval_results_df = None
        # else:
        #     # N·∫øu t√™n file kh√¥ng ƒë·ªïi, kh√¥ng c·∫ßn l√†m g√¨ c·∫£, d·ªØ li·ªáu ƒë√£ c√≥ trong session state
        #     pass


    # N·∫øu ƒë√£ c√≥ d·ªØ li·ªáu ƒë√°nh gi√° trong session state
    if st.session_state.eval_data is not None:
        st.info(f"ƒêang s·ª≠ d·ª•ng d·ªØ li·ªáu t·ª´ file: **{st.session_state.eval_uploaded_filename}** ({len(st.session_state.eval_data)} c√¢u h·ªèi).")

        # Cho ph√©p xem tr∆∞·ªõc 5 d√≤ng ƒë·∫ßu
        if st.checkbox("Hi·ªÉn th·ªã d·ªØ li·ªáu m·∫´u (5 d√≤ng ƒë·∫ßu)", key="show_eval_data_preview"):
            st.dataframe(pd.DataFrame(st.session_state.eval_data).head())

        # N√∫t b·∫Øt ƒë·∫ßu ƒë√°nh gi√°
        if st.button("üöÄ B·∫Øt ƒë·∫ßu ƒê√°nh gi√°", key="start_eval_button", help="Ch·∫°y ƒë√°nh gi√° v·ªõi c·∫•u h√¨nh hi·ªán t·∫°i v√† d·ªØ li·ªáu ƒë√£ t·∫£i l√™n."):
            # T·∫£i model Gemini ƒë√£ ch·ªçn (kh√¥ng cache v√¨ c√≥ th·ªÉ thay ƒë·ªïi)
            with st.spinner(f"ƒêang t·∫£i model Gemini: {current_gemini_model}..."):
                # C·∫ßn ƒë·∫£m b·∫£o h√†m load_gemini_model x·ª≠ l√Ω l·ªói API key ho·∫∑c t·∫£i model
                g_gemini_model = utils.load_gemini_model(current_gemini_model)

            if g_gemini_model:
                st.info(f"Model Gemini '{current_gemini_model}' ƒë√£ s·∫µn s√†ng.")
                logging.info(f"Gemini model '{current_gemini_model}' loaded for evaluation.")
                with st.spinner("‚è≥ ƒêang ch·∫°y ƒë√°nh gi√°... Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t ho·∫∑c l√¢u h∆°n t√πy thu·ªôc v√†o s·ªë l∆∞·ª£ng c√¢u h·ªèi v√† c·∫•u h√¨nh."):
                    start_eval_time = time.time()
                    # G·ªçi h√†m ƒë√°nh gi√°
                    results_df = run_retrieval_evaluation(
                        eval_data=st.session_state.eval_data,
                        hybrid_retriever=retriever_instance, # ƒê√£ kh·ªüi t·∫°o ·ªü tr√™n
                        embedding_model=g_embedding_model, # ƒê√£ kh·ªüi t·∫°o ·ªü tr√™n
                        reranking_model=g_reranking_model, # ƒê√£ kh·ªüi t·∫°o ·ªü tr√™n
                        gemini_model=g_gemini_model, # Model Gemini v·ª´a t·∫£i
                        eval_config=eval_config_dict # Dict c·∫•u h√¨nh ƒë√£ t·∫°o
                    )
                    total_eval_time = time.time() - start_eval_time
                    st.success(f"Ho√†n th√†nh ƒë√°nh gi√° sau {total_eval_time:.2f} gi√¢y.")
                    logging.info(f"Evaluation completed in {total_eval_time:.2f} seconds.")

                    # L∆∞u k·∫øt qu·∫£ v√† tr·∫°ng th√°i v√†o session state
                    st.session_state.eval_results_df = results_df
                    st.session_state.eval_run_completed = True
                    st.session_state.last_eval_config = eval_config_dict # L∆∞u l·∫°i c·∫•u h√¨nh ƒë√£ ch·∫°y
                    st.rerun() # Rerun ƒë·ªÉ hi·ªÉn th·ªã k·∫øt qu·∫£
            else:
                st.error(f"Kh√¥ng th·ªÉ t·∫£i model Gemini: {current_gemini_model}. Kh√¥ng th·ªÉ ch·∫°y ƒë√°nh gi√°.")
                logging.error(f"Failed to load Gemini model '{current_gemini_model}'. Evaluation cannot proceed.")

    # --- Hi·ªÉn th·ªã K·∫øt qu·∫£ (N·∫øu ƒë√£ ch·∫°y xong) ---
    if st.session_state.eval_run_completed and st.session_state.eval_results_df is not None:
        st.subheader("K·∫øt qu·∫£ ƒê√°nh gi√°")
        # L·∫•y DataFrame k·∫øt qu·∫£ t·ª´ session state
        detailed_results_df = st.session_state.eval_results_df
        # L·∫•y c·∫•u h√¨nh c·ªßa l·∫ßn ch·∫°y n√†y t·ª´ session state
        last_config = st.session_state.last_eval_config

        # Hi·ªÉn th·ªã l·∫°i c·∫•u h√¨nh ƒë√£ s·ª≠ d·ª•ng cho l·∫ßn ƒë√°nh gi√° n√†y
        st.markdown("**C·∫•u h√¨nh ƒë√£ s·ª≠ d·ª•ng cho l·∫ßn ƒë√°nh gi√° n√†y:**")
        config_cols = st.columns(3)
        config_cols[0].metric("Ch·∫ø ƒë·ªô Retrieval", last_config.get('retrieval_mode', 'N/A'))
        config_cols[1].metric("Model Gemini", last_config.get('gemini_model_name', 'N/A'))
        config_cols[2].metric("S·ª≠ d·ª•ng History LLM1", str(last_config.get('use_history_for_llm1', 'N/A')))


        # T√≠nh to√°n metrics trung b√¨nh
        avg_metrics, num_eval, num_skipped_error = calculate_average_metrics(detailed_results_df)

        st.metric("T·ªïng s·ªë Queries trong File", len(detailed_results_df))
        col_res1, col_res2 = st.columns(2)
        col_res1.metric("S·ªë Queries ƒê√°nh gi√° H·ª£p l·ªá", num_eval)
        col_res2.metric("S·ªë Queries B·ªè qua / L·ªói", num_skipped_error)

        if avg_metrics:
            st.markdown("#### Metrics Trung b√¨nh (tr√™n c√°c queries h·ª£p l·ªá)")
            k_values_display = [1, 3, 5, 10] # C√°c gi√° tr·ªã K ƒë·ªÉ hi·ªÉn th·ªã
            cols_k = st.columns(len(k_values_display)) # T·∫°o c·ªôt cho m·ªói K
            for idx, k in enumerate(k_values_display):
                with cols_k[idx]:
                    st.markdown(f"**K = {k}**")
                    # S·ª≠ d·ª•ng .get ƒë·ªÉ tr√°nh l·ªói n·∫øu key kh√¥ng t·ªìn t·∫°i, m·∫∑c ƒë·ªãnh l√† 0.0
                    st.text(f"Precision: {avg_metrics.get(f'avg_precision@{k}', 0.0):.4f}")
                    st.text(f"Recall:    {avg_metrics.get(f'avg_recall@{k}', 0.0):.4f}")
                    st.text(f"F1:        {avg_metrics.get(f'avg_f1@{k}', 0.0):.4f}")
                    st.text(f"MRR:       {avg_metrics.get(f'avg_mrr@{k}', 0.0):.4f}")
                    st.text(f"NDCG:      {avg_metrics.get(f'avg_ndcg@{k}', 0.0):.4f}")

            st.markdown("#### Th√¥ng tin Hi·ªáu nƒÉng Trung b√¨nh (tr√™n c√°c queries h·ª£p l·ªá)")
            col_perf1, col_perf2, col_perf3, col_perf4 = st.columns(4)
            col_perf1.metric("Avg Total Time/Query (s)", f"{avg_metrics.get('avg_processing_time', 0.0):.3f}")
            col_perf2.metric("Avg Variation Time (s)", f"{avg_metrics.get('avg_variation_time', 0.0):.3f}")
            col_perf3.metric("Avg Search Time (s)", f"{avg_metrics.get('avg_search_time', 0.0):.3f}")
            col_perf4.metric("Avg Rerank Time (s)", f"{avg_metrics.get('avg_rerank_time', 0.0):.3f}")

        else:
            st.warning("Kh√¥ng c√≥ query n√†o ƒë∆∞·ª£c ƒë√°nh gi√° th√†nh c√¥ng, kh√¥ng th·ªÉ t√≠nh metrics trung b√¨nh.")


        with st.expander("Xem K·∫øt qu·∫£ Chi ti·∫øt cho t·ª´ng Query"):
            # Ch·ªçn c√°c c·ªôt mu·ªën hi·ªÉn th·ªã trong b·∫£ng k·∫øt qu·∫£ chi ti·∫øt
            display_columns = [
                'query_id', 'query', 'status', 'retrieval_mode', 'use_history_llm1',
                'precision@1', 'recall@1', 'f1@1','mrr@1', 'ndcg@1', # Metrics @1
                'precision@3', 'recall@3', 'f1@3', 'mrr@3', 'ndcg@3', # Metrics @3
                'precision@5', 'recall@5', 'f1@5', 'mrr@5', 'ndcg@5', # Metrics @5
                'precision@10', 'recall@10', 'f1@10', 'mrr@10', 'ndcg@10', # Metrics @10
                'processing_time', 'variation_time', 'search_time', 'rerank_time',
                'num_variations_generated','num_unique_docs_found', 'num_docs_reranked',
                'retrieved_ids', 'relevant_ids', 'error_message' # Th√™m error_message
            ]
            # L·ªçc ra nh·ªØng c·ªôt th·ª±c s·ª± t·ªìn t·∫°i trong DataFrame ƒë·ªÉ tr√°nh l·ªói
            existing_display_columns = [col for col in display_columns if col in detailed_results_df.columns]
            # Hi·ªÉn th·ªã DataFrame v·ªõi c√°c c·ªôt ƒë√£ ch·ªçn
            st.dataframe(detailed_results_df[existing_display_columns])

        st.subheader("L∆∞u K·∫øt qu·∫£ Chi ti·∫øt")
        try:
            # Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·ªÉ t·∫£i v·ªÅ
            results_json = detailed_results_df.to_json(orient='records', indent=2, force_ascii=False)
            results_csv = detailed_results_df.to_csv(index=False).encode('utf-8') # CSV c·∫ßn encode

            # T·∫°o t√™n file ƒë·ªông d·ª±a tr√™n th·ªùi gian v√† c·∫•u h√¨nh
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            mode_suffix = last_config.get('retrieval_mode', 'unknown').lower().replace(' ', '_')
            hist_suffix = "hist" if last_config.get('use_history_for_llm1', False) else "nohist"
            model_suffix = last_config.get('gemini_model_name', 'gemini').split('/')[-1] # L·∫•y ph·∫ßn cu·ªëi t√™n model
            base_filename = f"eval_{mode_suffix}_{hist_suffix}_{model_suffix}_{timestamp}"
            fname_json = f"{base_filename}.json"
            fname_csv = f"{base_filename}.csv"

            # T·∫°o n√∫t t·∫£i v·ªÅ
            col_dl1, col_dl2 = st.columns(2)
            with col_dl1:
                st.download_button(
                    label="üíæ T·∫£i v·ªÅ JSON", data=results_json, file_name=fname_json, mime="application/json",
                    key="download_json_eval"
                )
            with col_dl2:
                st.download_button(
                    label="üíæ T·∫£i v·ªÅ CSV", data=results_csv, file_name=fname_csv, mime="text/csv",
                    key="download_csv_eval"
                )
        except Exception as e:
            st.error(f"L·ªói khi chu·∫©n b·ªã file k·∫øt qu·∫£ ƒë·ªÉ t·∫£i v·ªÅ: {e}")
            logging.exception("Error preparing evaluation results for download.")

    # --- Qu·∫£n l√Ω Tr·∫°ng th√°i ƒê√°nh gi√° ---
    st.markdown("---")
    st.subheader("Qu·∫£n l√Ω Tr·∫°ng th√°i ƒê√°nh gi√°")
    if st.button("X√≥a File ƒê√£ T·∫£i v√† K·∫øt Qu·∫£ ƒê√°nh Gi√°", key="clear_eval_state", help="X√≥a d·ªØ li·ªáu v√† k·∫øt qu·∫£ ƒë√°nh gi√° hi·ªán t·∫°i kh·ªèi b·ªô nh·ªõ phi√™n."):
        # Reset t·∫•t c·∫£ c√°c session state li√™n quan ƒë·∫øn ƒë√°nh gi√°
        st.session_state.eval_data = None
        st.session_state.eval_uploaded_filename = ""
        st.session_state.eval_run_completed = False
        st.session_state.eval_results_df = None
        st.session_state.last_eval_config = {}
        st.success("ƒê√£ x√≥a tr·∫°ng th√°i ƒë√°nh gi√°. Vui l√≤ng t·∫£i l·∫°i file n·∫øu mu·ªën ch·∫°y l·∫°i.")
        logging.info("Evaluation state cleared.")
        time.sleep(1) # Ch·ªù m·ªôt ch√∫t ƒë·ªÉ ng∆∞·ªùi d√πng ƒë·ªçc th√¥ng b√°o
        st.rerun() # Rerun ƒë·ªÉ c·∫≠p nh·∫≠t giao di·ªán

# Tr∆∞·ªùng h·ª£p h·ªá th·ªëng c∆° b·∫£n kh√¥ng kh·ªüi t·∫°o ƒë∆∞·ª£c
else:
    st.warning("‚ö†Ô∏è H·ªá th·ªëng c∆° b·∫£n ch∆∞a s·∫µn s√†ng (Models, VectorDB, Retriever). Vui l√≤ng ki·ªÉm tra l·∫°i trang Chatbot ch√≠nh ho·∫∑c kh·ªüi ƒë·ªông l·∫°i ·ª©ng d·ª•ng n·∫øu c·∫ßn.")
    logging.warning("Evaluation page cannot proceed as core components are not ready.")