# pages/2_Evaluation.py
import time
import streamlit as st
import pandas as pd
import json
import time
import math
import os
import logging
from datetime import datetime
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import config
import utils
import data_loader
from vector_db import SimpleVectorDatabase
from retriever import HybridRetriever

# --- Cáº¥u hÃ¬nh Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- CÃ¡c hÃ m tÃ­nh Metrics ---
# (CÃ¡c hÃ m precision_at_k, recall_at_k, f1_at_k, mrr_at_k, ndcg_at_k giá»¯ nguyÃªn)
def precision_at_k(retrieved_ids, relevant_ids, k):
    if k <= 0: return 0.0
    retrieved_at_k = retrieved_ids[:k]; relevant_set = set(relevant_ids)
    if not relevant_set: return 0.0
    intersect = set(retrieved_at_k) & relevant_set
    return len(intersect) / k

def recall_at_k(retrieved_ids, relevant_ids, k):
    relevant_set = set(relevant_ids)
    if not relevant_set: return 1.0
    retrieved_at_k = retrieved_ids[:k]
    intersect = set(retrieved_at_k) & relevant_set
    return len(intersect) / len(relevant_set)

def f1_at_k(retrieved_ids, relevant_ids, k):
    prec = precision_at_k(retrieved_ids, relevant_ids, k); rec = recall_at_k(retrieved_ids, relevant_ids, k)
    return 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0

def mrr_at_k(retrieved_ids, relevant_ids, k):
    relevant_set = set(relevant_ids);
    if not relevant_set: return 0.0
    for rank, doc_id in enumerate(retrieved_ids[:k], 1):
        if doc_id in relevant_set: return 1.0 / rank
    return 0.0

def ndcg_at_k(retrieved_ids, relevant_ids, k):
    relevant_set = set(relevant_ids);
    if not relevant_set: return 1.0
    retrieved_at_k = retrieved_ids[:k]; dcg = 0.0; idcg = 0.0
    for i, doc_id in enumerate(retrieved_at_k):
        if doc_id in relevant_set: dcg += 1.0 / math.log2(i + 2)
    for i in range(min(k, len(relevant_set))): idcg += 1.0 / math.log2(i + 2)
    return dcg / idcg if idcg > 0 else 0.0


# --- HÃ m thá»±c thi lÃµi Ä‘Ã¡nh giÃ¡ Retrieval ---
# (HÃ m run_retrieval_evaluation giá»¯ nguyÃªn)
def run_retrieval_evaluation(
    eval_data: list,
    hybrid_retriever: HybridRetriever,
    embedding_model, # Model Ä‘Ã£ load
    reranking_model, # Model Ä‘Ã£ load
    gemini_model,    # Model Ä‘Ã£ load (cÃ³ thá»ƒ None)
    eval_config: dict # Chá»©a cÃ¡c tÃ¹y chá»n nhÆ° retrieval_mode, use_history,...
    ):

    results_list = []
    k_values = [3, 5, 10] # K values for evaluation metrics

    retrieval_mode = eval_config.get('retrieval_mode', 'ÄÆ¡n giáº£n')
    use_history = eval_config.get('use_history_for_llm1', False)
    dummy_history = [{"role": "user", "content": "..."}] if use_history else None

    progress_bar = st.progress(0)
    status_text = st.empty()

    total_items = len(eval_data)
    queries_per_batch = 15
    wait_time_seconds = 60

    for i, item in enumerate(eval_data):
        if i > 0 and i % queries_per_batch == 0:
            pause_msg = f"ÄÃ£ xá»­ lÃ½ {i}/{total_items} queries. Táº¡m dá»«ng {wait_time_seconds} giÃ¢y..."
            logging.info(pause_msg)
            status_text.text(pause_msg)
            time.sleep(wait_time_seconds)
            status_text.text(f"Tiáº¿p tá»¥c xá»­ lÃ½ query {i+1}/{total_items}...")

        query_id = item.get("query_id"); original_query = item.get("query")
        relevant_chunk_ids = set(item.get("relevant_chunk_ids", []))
        if not query_id or not original_query: continue

        status_text.text(f"Äang xá»­ lÃ½ query {i+1}/{total_items}: {query_id}")
        logging.info(f"Eval - Processing QID: {query_id}")

        start_time = time.time()
        query_metrics = {
            "query_id": query_id, "query": original_query, "retrieval_mode": retrieval_mode,
            "status": "error", "retrieved_ids": [], "relevant_ids": list(relevant_chunk_ids),
            "processing_time": 0.0
        }
        for k in k_values: query_metrics[f'precision@{k}'] = 0.0; query_metrics[f'recall@{k}'] = 0.0; query_metrics[f'f1@{k}'] = 0.0; query_metrics[f'mrr@{k}'] = 0.0; query_metrics[f'ndcg@{k}'] = 0.0
        timing_keys = ['variation_time', 'search_time', 'rerank_time']
        count_keys = ['num_variations_generated', 'num_unique_docs_found', 'num_docs_reranked']
        for k in timing_keys + count_keys: query_metrics[k] = 0.0

        try:
            variation_start = time.time()
            relevance_status, _, all_queries, summarizing_query = utils.generate_query_variations(
                original_query=original_query,
                gemini_model=gemini_model,
                chat_history=dummy_history
            )
            query_metrics["variation_time"] = time.time() - variation_start
            query_metrics["summarizing_query"] = summarizing_query
            query_metrics["num_variations_generated"] = len(all_queries)
            # st.write(all_queries) # Bá» comment náº¿u muá»‘n debug

            if relevance_status == 'invalid':
                query_metrics["status"] = "skipped_irrelevant"
                query_metrics["processing_time"] = time.time() - start_time
                results_list.append(query_metrics)
                progress_bar.progress((i + 1) / total_items)
                continue

            collected_docs_data = {}
            search_start = time.time()
            if retrieval_mode == 'ÄÆ¡n giáº£n':
                variant_results = hybrid_retriever.hybrid_search(
                    summarizing_query, embedding_model,
                    vector_search_k=config.VECTOR_K_PER_QUERY, final_k=config.HYBRID_K_PER_QUERY
                )
                for res_item in variant_results:
                    idx = res_item.get('index');
                    if isinstance(idx, int) and idx >= 0: collected_docs_data[idx] = res_item
            elif retrieval_mode == 'SÃ¢u':
                for q_variant in all_queries:
                    variant_results = hybrid_retriever.hybrid_search(
                        q_variant, embedding_model,
                        vector_search_k=config.VECTOR_K_PER_QUERY, final_k=config.HYBRID_K_PER_QUERY
                    )
                    for res_item in variant_results:
                        idx = res_item.get('index')
                        if isinstance(idx, int) and idx >= 0 and idx not in collected_docs_data:
                             collected_docs_data[idx] = res_item
            query_metrics["search_time"] = time.time() - search_start
            query_metrics["num_unique_docs_found"] = len(collected_docs_data)

            unique_docs_list = list(collected_docs_data.values())
            unique_docs_list.sort(key=lambda x: x.get('hybrid_score', 0.0), reverse=True)
            docs_for_reranking_input = unique_docs_list[:config.MAX_DOCS_FOR_RERANK]
            query_metrics["num_docs_reranked"] = len(docs_for_reranking_input)

            rerank_start = time.time()
            reranked_results = utils.rerank_documents(
                summarizing_query, docs_for_reranking_input, reranking_model
            )
            query_metrics["rerank_time"] = time.time() - rerank_start

            final_retrieved_docs = reranked_results[:config.FINAL_NUM_RESULTS_AFTER_RERANK]
            # st.write(final_retrieved_docs)
            retrieved_ids = []
            for res in final_retrieved_docs:
                doc_data = res.get('doc', {}); chunk_id = None
                if isinstance(doc_data, dict):
                    chunk_id = doc_data.get('id') or doc_data.get('metadata', {}).get('id') or doc_data.get('metadata', {}).get('chunk_id')
                if chunk_id: retrieved_ids.append(str(chunk_id))
            query_metrics["retrieved_ids"] = retrieved_ids
            # st.write(retrieved_ids)
            query_metrics["status"] = "evaluated"
            for k in k_values:
                query_metrics[f'precision@{k}'] = precision_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'recall@{k}'] = recall_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'f1@{k}'] = f1_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'mrr@{k}'] = mrr_at_k(retrieved_ids, relevant_chunk_ids, k)
                query_metrics[f'ndcg@{k}'] = ndcg_at_k(retrieved_ids, relevant_chunk_ids, k)

        except Exception as e:
            logging.exception(f"Error evaluating QID {query_id}: {e}")
            query_metrics["status"] = "error_runtime"
            query_metrics["error_message"] = str(e)
        finally:
            query_metrics["processing_time"] = time.time() - start_time
            results_list.append(query_metrics)
            progress_bar.progress((i + 1) / total_items)

    status_text.text(f"HoÃ n thÃ nh Ä‘Ã¡nh giÃ¡ {total_items} queries!")
    return pd.DataFrame(results_list)


# --- HÃ m tÃ­nh toÃ¡n tá»•ng há»£p Metrics ---
# (HÃ m calculate_average_metrics giá»¯ nguyÃªn)
def calculate_average_metrics(df_results: pd.DataFrame):
    """TÃ­nh toÃ¡n metrics trung bÃ¬nh tá»« DataFrame káº¿t quáº£ chi tiáº¿t."""
    evaluated_df = df_results[df_results['status'] == 'evaluated']
    num_evaluated = len(evaluated_df)
    if num_evaluated == 0:
        return None, num_evaluated, len(df_results) - num_evaluated

    avg_metrics = {}
    k_values = [3, 5, 10]
    metric_keys_k = [f'{m}@{k}' for k in k_values for m in ['precision', 'recall', 'f1', 'mrr', 'ndcg']]
    timing_keys = ['processing_time', 'variation_time', 'search_time', 'rerank_time']
    count_keys = ['num_variations_generated', 'num_unique_docs_found', 'num_docs_reranked']

    for key in metric_keys_k + timing_keys + count_keys:
        # TÃ­nh tá»•ng, bá» qua NaN náº¿u cÃ³
        total = evaluated_df[key].sum(skipna=True)
        avg_metrics[f'avg_{key}'] = total / num_evaluated if num_evaluated > 0 else 0.0 # Avoid division by zero

    return avg_metrics, num_evaluated, len(df_results) - num_evaluated


# --- Giao diá»‡n Streamlit cho Trang ÄÃ¡nh giÃ¡ ---
st.set_page_config(page_title="ÄÃ¡nh giÃ¡ Retrieval", layout="wide")
st.title("ğŸ“Š ÄÃ¡nh giÃ¡ Há»‡ thá»‘ng Retrieval")

st.markdown("""
Trang nÃ y cho phÃ©p báº¡n cháº¡y Ä‘Ã¡nh giÃ¡ hiá»‡u suáº¥t cá»§a há»‡ thá»‘ng retrieval (tÃ¬m kiáº¿m + xáº¿p háº¡ng láº¡i)
dá»±a trÃªn má»™t táº­p dá»¯ liá»‡u cÃ³ chá»©a cÃ¡c cÃ¢u há»i vÃ  cÃ¡c chunk tÃ i liá»‡u liÃªn quan (ground truth).
Káº¿t quáº£ sáº½ Ä‘Æ°á»£c lÆ°u trá»¯ trong phiÃªn lÃ m viá»‡c nÃ y ngay cáº£ khi báº¡n chuyá»ƒn tab.
""")

# --- Khá»Ÿi táº¡o Session State ---
# <<< THAY Äá»”I >>>: Khá»Ÿi táº¡o cÃ¡c key cáº§n thiáº¿t trong session state
if 'eval_data' not in st.session_state:
    st.session_state.eval_data = None
if 'eval_results_df' not in st.session_state:
    st.session_state.eval_results_df = None
if 'eval_run_completed' not in st.session_state:
    st.session_state.eval_run_completed = False
if 'eval_uploaded_filename' not in st.session_state:
    st.session_state.eval_uploaded_filename = ""

# --- Kiá»ƒm tra vÃ  Hiá»ƒn thá»‹ Tráº¡ng thÃ¡i Há»‡ thá»‘ng ---
st.subheader("Tráº¡ng thÃ¡i Há»‡ thá»‘ng CÆ¡ báº£n")
init_ok = False
models_ready = False
retriever_instance = None

with st.spinner("Kiá»ƒm tra vÃ  khá»Ÿi táº¡o tÃ i nguyÃªn..."):
    g_embedding_model = utils.load_embedding_model(config.embedding_model_name)
    g_reranking_model = utils.load_reranker_model(config.reranking_model_name)
    try:
        vector_db, retriever_instance = data_loader.load_or_create_rag_components(g_embedding_model)
        if vector_db and retriever_instance and g_embedding_model and g_reranking_model:
            init_ok = True
            models_ready = True
            st.success("âœ… VectorDB, Retriever, Embedding, Reranker Ä‘Ã£ sáºµn sÃ ng.")
        else:
            st.error("âš ï¸ Lá»—i khi khá»Ÿi táº¡o VectorDB hoáº·c Retriever.")
    except Exception as e:
        st.error(f"âš ï¸ Lá»—i nghiÃªm trá»ng khi khá»Ÿi táº¡o há»‡ thá»‘ng: {e}")

if init_ok:
    st.subheader("Cáº¥u hÃ¬nh ÄÃ¡nh giÃ¡")
    st.markdown("ÄÃ¡nh giÃ¡ sáº½ Ä‘Æ°á»£c cháº¡y vá»›i cÃ¡c cáº¥u hÃ¬nh **hiá»‡n táº¡i** Ä‘Æ°á»£c chá»n trong Sidebar cá»§a trang Chatbot chÃ­nh.")

    current_gemini_model = st.session_state.get('selected_gemini_model', config.DEFAULT_GEMINI_MODEL)
    current_retrieval_mode = st.session_state.get('retrieval_mode', 'ÄÆ¡n giáº£n')
    current_use_history = st.session_state.get('use_history_for_llm1', False)

    col1, col2, col3 = st.columns(3)
    with col1: st.info(f"**Cháº¿ Ä‘á»™ Retrieval:** `{current_retrieval_mode}`")
    with col2: st.info(f"**Model Gemini (Variations):** `{current_gemini_model}`")
    with col3: st.info(f"**Sá»­ dá»¥ng Lá»‹ch sá»­ (LLM1):** `{current_use_history}`")

    eval_config_dict = {
        'retrieval_mode': current_retrieval_mode,
        'use_history_for_llm1': current_use_history,
    }

    st.subheader("Táº£i LÃªn File ÄÃ¡nh giÃ¡")
    uploaded_file = st.file_uploader(
        "Chá»n file JSON chá»©a dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ (Ä‘á»‹nh dáº¡ng: [{'query_id': ..., 'query': ..., 'relevant_chunk_ids': [...]}, ...])",
        type=["json"],
        key="eval_file_uploader" # ThÃªm key Ä‘á»ƒ quáº£n lÃ½ widget tá»‘t hÆ¡n
    )

    # <<< THAY Äá»”I >>>: Xá»­ lÃ½ file táº£i lÃªn vÃ  lÆ°u vÃ o session_state
    if uploaded_file is not None:
        # Chá»‰ xá»­ lÃ½ vÃ  lÆ°u láº¡i náº¿u file khÃ¡c vá»›i file Ä‘Ã£ lÆ°u trÆ°á»›c Ä‘Ã³
        if uploaded_file.name != st.session_state.eval_uploaded_filename:
            try:
                file_content_bytes = uploaded_file.getvalue()
                eval_data_list = json.loads(file_content_bytes.decode('utf-8'))

                # LÆ°u tráº¡ng thÃ¡i vÃ o session state
                st.session_state.eval_data = eval_data_list
                st.session_state.eval_uploaded_filename = uploaded_file.name
                st.session_state.eval_run_completed = False # Reset tráº¡ng thÃ¡i cháº¡y
                st.session_state.eval_results_df = None    # XÃ³a káº¿t quáº£ cÅ©
                st.success(f"ÄÃ£ táº£i vÃ  lÆ°u trá»¯ file '{uploaded_file.name}' chá»©a {len(eval_data_list)} cÃ¢u há»i.")
                # KhÃ´ng cáº§n rerun ngay láº­p tá»©c, code tiáº¿p theo sáº½ Ä‘á»c tá»« session_state
            except json.JSONDecodeError:
                st.error("Lá»—i: File táº£i lÃªn khÃ´ng pháº£i lÃ  Ä‘á»‹nh dáº¡ng JSON há»£p lá»‡.")
                # XÃ³a tráº¡ng thÃ¡i náº¿u lá»—i
                st.session_state.eval_data = None
                st.session_state.eval_uploaded_filename = ""
                st.session_state.eval_run_completed = False
                st.session_state.eval_results_df = None
            except Exception as e:
                st.error(f"Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh khi xá»­ lÃ½ file: {e}")
                logging.exception("Unhandled error during file processing.")
                # XÃ³a tráº¡ng thÃ¡i náº¿u lá»—i
                st.session_state.eval_data = None
                st.session_state.eval_uploaded_filename = ""
                st.session_state.eval_run_completed = False
                st.session_state.eval_results_df = None

    # <<< THAY Äá»”I >>>: Kiá»ƒm tra vÃ  xá»­ lÃ½ dá»±a trÃªn session_state
    # Hiá»ƒn thá»‹ thÃ´ng tin file Ä‘Ã£ táº£i (náº¿u cÃ³) vÃ  nÃºt cháº¡y
    if st.session_state.eval_data is not None:
        st.info(f"Äang sá»­ dá»¥ng dá»¯ liá»‡u tá»« file: **{st.session_state.eval_uploaded_filename}** ({len(st.session_state.eval_data)} cÃ¢u há»i).")

        if st.checkbox("Hiá»ƒn thá»‹ dá»¯ liá»‡u máº«u (5 dÃ²ng Ä‘áº§u)", key="show_eval_data_preview"):
            st.dataframe(pd.DataFrame(st.session_state.eval_data).head())

        # NÃºt báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡
        if st.button("ğŸš€ Báº¯t Ä‘áº§u ÄÃ¡nh giÃ¡", key="start_eval_button"):
            with st.spinner(f"Äang táº£i model Gemini: {current_gemini_model}..."):
                g_gemini_model = utils.load_gemini_model(current_gemini_model)

            if g_gemini_model:
                with st.spinner("â³ Äang cháº¡y Ä‘Ã¡nh giÃ¡... QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t vÃ i phÃºt."):
                    start_eval_time = time.time()
                    # Cháº¡y Ä‘Ã¡nh giÃ¡ vá»›i dá»¯ liá»‡u tá»« session_state
                    results_df = run_retrieval_evaluation(
                        eval_data=st.session_state.eval_data, # << Sá»­ dá»¥ng dá»¯ liá»‡u tá»« state
                        hybrid_retriever=retriever_instance,
                        embedding_model=g_embedding_model,
                        reranking_model=g_reranking_model,
                        gemini_model=g_gemini_model,
                        eval_config=eval_config_dict
                    )
                    total_eval_time = time.time() - start_eval_time
                    st.info(f"HoÃ n thÃ nh Ä‘Ã¡nh giÃ¡ sau {total_eval_time:.2f} giÃ¢y.")

                    # <<< THAY Äá»”I >>>: LÆ°u káº¿t quáº£ vÃ o session_state
                    st.session_state.eval_results_df = results_df
                    st.session_state.eval_run_completed = True
                    # YÃªu cáº§u cháº¡y láº¡i script Ä‘á»ƒ hiá»ƒn thá»‹ káº¿t quáº£ tá»« session_state
                    st.rerun()
            else:
                st.error(f"KhÃ´ng thá»ƒ táº£i model Gemini: {current_gemini_model}. KhÃ´ng thá»ƒ cháº¡y Ä‘Ã¡nh giÃ¡.")

    # <<< THAY Äá»”I >>>: Hiá»ƒn thá»‹ káº¿t quáº£ náº¿u Ä‘Ã¡nh giÃ¡ Ä‘Ã£ hoÃ n thÃ nh (Ä‘á»c tá»« session_state)
    if st.session_state.eval_run_completed and st.session_state.eval_results_df is not None:
        st.subheader("Káº¿t quáº£ ÄÃ¡nh giÃ¡")
        detailed_results_df = st.session_state.eval_results_df # << Láº¥y káº¿t quáº£ tá»« state

        avg_metrics, num_eval, num_skipped_error = calculate_average_metrics(detailed_results_df)

        st.metric("Tá»•ng sá»‘ Queries", len(detailed_results_df))
        col_res1, col_res2 = st.columns(2) # Bá» cá»™t 3 náº¿u khÃ´ng cáº§n
        col_res1.metric("Queries ÄÃ¡nh giÃ¡ Há»£p lá»‡", num_eval)
        col_res2.metric("Queries Bá» qua/Lá»—i", num_skipped_error)


        if avg_metrics:
            st.markdown("#### Metrics Trung bÃ¬nh (trÃªn cÃ¡c queries há»£p lá»‡)")
            k_values_display = [3, 5, 10]
            cols_k = st.columns(len(k_values_display))
            for idx, k in enumerate(k_values_display):
                with cols_k[idx]:
                    st.markdown(f"**K = {k}**")
                    st.text(f"Precision: {avg_metrics.get(f'avg_precision@{k}', 0.0):.4f}")
                    st.text(f"Recall:    {avg_metrics.get(f'avg_recall@{k}', 0.0):.4f}")
                    st.text(f"F1:        {avg_metrics.get(f'avg_f1@{k}', 0.0):.4f}")
                    st.text(f"MRR:       {avg_metrics.get(f'avg_mrr@{k}', 0.0):.4f}")
                    st.text(f"NDCG:      {avg_metrics.get(f'avg_ndcg@{k}', 0.0):.4f}")

            st.markdown("#### ThÃ´ng tin Hiá»‡u nÄƒng Trung bÃ¬nh")
            col_perf1, col_perf2, col_perf3 = st.columns(3)
            col_perf1.metric("Avg Total Time/Query (s)", f"{avg_metrics.get('avg_processing_time', 0.0):.3f}")
            col_perf2.metric("Avg Variation Time (s)", f"{avg_metrics.get('avg_variation_time', 0.0):.3f}")
            col_perf3.metric("Avg Search Time (s)", f"{avg_metrics.get('avg_search_time', 0.0):.3f}")


        with st.expander("Xem Káº¿t quáº£ Chi tiáº¿t cho tá»«ng Query"):
            display_columns = [
                'query_id', 'query', 'status', 'retrieval_mode',
                'precision@3', 'recall@10', 'mrr@10', 'ndcg@10',
                'processing_time', 'retrieved_ids'
            ]
            existing_display_columns = [col for col in display_columns if col in detailed_results_df.columns]
            st.dataframe(detailed_results_df[existing_display_columns])

        # --- LÆ°u Káº¿t quáº£ ---
        st.subheader("LÆ°u Káº¿t quáº£ Chi tiáº¿t")
        try:
            # Sá»­ dá»¥ng DataFrame tá»« session_state Ä‘á»ƒ táº¡o file táº£i vá»
            results_json = detailed_results_df.to_json(orient='records', indent=2, force_ascii=False)
            results_csv = detailed_results_df.to_csv(index=False).encode('utf-8')

            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            mode_suffix = current_retrieval_mode.lower().replace(' ', '_')
            fname_json = f"evaluation_results_{mode_suffix}_{timestamp}.json"
            fname_csv = f"evaluation_results_{mode_suffix}_{timestamp}.csv"

            col_dl1, col_dl2 = st.columns(2)
            with col_dl1:
                st.download_button(
                    label="ğŸ’¾ Táº£i vá» JSON", data=results_json, file_name=fname_json, mime="application/json",
                    key="download_json_eval" # ThÃªm key
                )
            with col_dl2:
                st.download_button(
                    label="ğŸ’¾ Táº£i vá» CSV", data=results_csv, file_name=fname_csv, mime="text/csv",
                    key="download_csv_eval" # ThÃªm key
                )
        except Exception as e:
            st.error(f"Lá»—i khi chuáº©n bá»‹ file Ä‘á»ƒ táº£i vá»: {e}")

    # <<< THAY Äá»”I >>>: ThÃªm nÃºt Ä‘á»ƒ xÃ³a tráº¡ng thÃ¡i Ä‘Ã¡nh giÃ¡ thá»§ cÃ´ng
    st.markdown("---")
    st.subheader("Quáº£n lÃ½ Tráº¡ng thÃ¡i ÄÃ¡nh giÃ¡")
    if st.button("XÃ³a File ÄÃ£ Táº£i vÃ  Káº¿t Quáº£ ÄÃ¡nh GiÃ¡", key="clear_eval_state"):
        st.session_state.eval_data = None
        st.session_state.eval_uploaded_filename = ""
        st.session_state.eval_run_completed = False
        st.session_state.eval_results_df = None
        # XÃ³a luÃ´n file Ä‘Ã£ chá»n trong uploader báº±ng cÃ¡ch reset key cá»§a nÃ³ (náº¿u cáº§n)
        # st.session_state.eval_file_uploader = None # CÃ³ thá»ƒ khÃ´ng cáº§n thiáº¿t náº¿u chá»‰ clear data
        st.success("ÄÃ£ xÃ³a tráº¡ng thÃ¡i Ä‘Ã¡nh giÃ¡. Vui lÃ²ng táº£i láº¡i file náº¿u muá»‘n cháº¡y láº¡i.")
        time.sleep(1)
        st.rerun()


else:
    st.warning("âš ï¸ Há»‡ thá»‘ng cÆ¡ báº£n chÆ°a sáºµn sÃ ng. Vui lÃ²ng kiá»ƒm tra láº¡i trang Chatbot chÃ­nh hoáº·c khá»Ÿi Ä‘á»™ng láº¡i á»©ng dá»¥ng.")