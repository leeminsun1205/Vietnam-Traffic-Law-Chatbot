{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ffad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate_rag.py\n",
    "import argparse\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Import các module cần thiết từ dự án của bạn\n",
    "import config\n",
    "import utils\n",
    "import data_loader\n",
    "# Giả định retriever.py và vector_db.py cũng cần thiết gián tiếp qua data_loader\n",
    "# from utils import get_chunk_id # Đảm bảo hàm này tồn tại và đúng\n",
    "\n",
    "# --- Cấu hình Logging ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Hàm lấy ID từ chunk (QUAN TRỌNG: PHẢI GIỐNG HÀM TRONG APP.PY VÀ ĐÚNG VỚI DỮ LIỆU) ---\n",
    "def get_chunk_id(doc_object, default_prefix=\"index_\"):\n",
    "    \"\"\"Lấy ID duy nhất từ object chunk. Ưu tiên metadata['id'].\"\"\"\n",
    "    # === BẠN CẦN ĐẢM BẢO LOGIC NÀY ĐÚNG VỚI DỮ LIỆU CỦA BẠN ===\n",
    "    if isinstance(doc_object, dict):\n",
    "        metadata = doc_object.get('metadata', {})\n",
    "        if 'id' in metadata: # Ưu tiên hàng đầu\n",
    "            return metadata['id']\n",
    "        # Thêm các logic lấy ID khác nếu cần (ví dụ từ key cấp cao hơn, hoặc tạo ID hash)\n",
    "        # Ví dụ fallback (không khuyến khích dùng index):\n",
    "        # if 'index' in item: return f\"index_{item['index']}\" # Nếu có index từ retrieval result\n",
    "    logging.warning(f\"Không thể lấy ID đáng tin cậy từ chunk: {str(doc_object)[:100]}...\")\n",
    "    return None # Trả về None nếu không có ID rõ ràng\n",
    "\n",
    "# --- Hàm tính toán chỉ số retrieval ---\n",
    "def calculate_retrieval_metrics(retrieved_ids_set, ground_truth_ids_set):\n",
    "    \"\"\"Tính Precision, Recall, F1 cho retrieval.\"\"\"\n",
    "    if not isinstance(retrieved_ids_set, set): retrieved_ids_set = set(retrieved_ids_set)\n",
    "    if not isinstance(ground_truth_ids_set, set): ground_truth_ids_set = set(ground_truth_ids_set)\n",
    "\n",
    "    true_positives = len(retrieved_ids_set.intersection(ground_truth_ids_set))\n",
    "    retrieved_count = len(retrieved_ids_set)\n",
    "    ground_truth_count = len(ground_truth_ids_set)\n",
    "\n",
    "    precision = true_positives / retrieved_count if retrieved_count > 0 else 0\n",
    "    recall = true_positives / ground_truth_count if ground_truth_count > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"true_positives\": true_positives, \"retrieved_count\": retrieved_count, \"ground_truth_count\": ground_truth_count}\n",
    "\n",
    "# --- Hàm chính để chạy đánh giá ---\n",
    "def run_evaluation(eval_data_path, results_output_path, retrieval_mode='Sâu'):\n",
    "    \"\"\"Chạy đánh giá RAG trên bộ dữ liệu.\"\"\"\n",
    "    logging.info(\"--- Bắt đầu quá trình đánh giá RAG ---\")\n",
    "\n",
    "    # --- 1. Tải dữ liệu đánh giá ---\n",
    "    logging.info(f\"Đang tải dữ liệu đánh giá từ: {eval_data_path}\")\n",
    "    try:\n",
    "        with open(eval_data_path, 'r', encoding='utf-8') as f:\n",
    "            evaluation_data = json.load(f)\n",
    "        logging.info(f\"Đã tải {len(evaluation_data)} mẫu đánh giá.\")\n",
    "        # Thêm query_id nếu chưa có\n",
    "        for i, item in enumerate(evaluation_data):\n",
    "            item['query_id'] = item.get('query_id', f\"query_{i+1}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Lỗi khi tải hoặc xử lý tệp đánh giá: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    # --- 2. Khởi tạo các thành phần RAG ---\n",
    "    logging.info(\"Đang khởi tạo các thành phần RAG (models, retriever)...\")\n",
    "    try:\n",
    "        # Tải models (không cần LLM nếu chỉ đánh giá retrieval/rerank)\n",
    "        embedding_model = utils.load_embedding_model(config.embedding_model_name)\n",
    "        reranker_model = utils.load_reranker_model(config.reranking_model_name)\n",
    "        # Tải retriever (bao gồm Vector DB)\n",
    "        _, hybrid_retriever = data_loader.load_or_create_rag_components(embedding_model)\n",
    "\n",
    "        # (Tùy chọn) Tải LLM nếu muốn đánh giá cả bước generation\n",
    "        # gemini_model = utils.load_gemini_model(config.DEFAULT_GEMINI_MODEL) # Hoặc model cụ thể\n",
    "\n",
    "        if not all([embedding_model, reranker_model, hybrid_retriever]):\n",
    "            raise RuntimeError(\"Không thể khởi tạo tất cả các thành phần RAG cần thiết.\")\n",
    "        logging.info(\"Khởi tạo thành phần RAG thành công.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Lỗi khi khởi tạo thành phần RAG: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    # --- 3. Chạy Pipeline và Thu thập Kết quả ---\n",
    "    all_results = []\n",
    "    start_time_total = time.time()\n",
    "\n",
    "    logging.info(f\"Bắt đầu xử lý {len(evaluation_data)} câu hỏi...\")\n",
    "    for i, eval_item in enumerate(evaluation_data):\n",
    "        query_id = eval_item['query_id']\n",
    "        query = eval_item['query']\n",
    "        ground_truth_ids = set(eval_item['relevant_chunk_ids']) # Dùng set để dễ so sánh\n",
    "        item_results = {\"query_id\": query_id, \"query\": query}\n",
    "        start_time_item = time.time()\n",
    "\n",
    "        logging.info(f\"Đang xử lý câu hỏi {i+1}/{len(evaluation_data)}: '{query_id}'\")\n",
    "\n",
    "        try:\n",
    "            # --- Bước A (Tùy chọn): Chạy LLM 1 để lấy summary/variants ---\n",
    "            # Để đơn giản, có thể bỏ qua bước này và dùng query gốc cho retrieval/rerank\n",
    "            # Hoặc chạy nó nếu muốn mô phỏng chính xác app\n",
    "            # _, _, all_queries, summarizing_q = utils.generate_query_variations(query, gemini_model, ...)\n",
    "            # Nếu bỏ qua LLM1:\n",
    "            summarizing_q = query # Dùng query gốc làm summary tạm\n",
    "            all_queries = [query] # Chỉ dùng query gốc nếu không chạy LLM1\n",
    "\n",
    "            # --- Bước B: Retrieval ---\n",
    "            collected_docs_data_by_id = {} # Dùng ID làm key\n",
    "            item_results[\"retrieval_mode_used\"] = retrieval_mode\n",
    "\n",
    "            if retrieval_mode == 'Đơn giản':\n",
    "                variant_results = hybrid_retriever.hybrid_search(\n",
    "                    summarizing_q, embedding_model,\n",
    "                    vector_search_k=config.VECTOR_K_PER_QUERY,\n",
    "                    final_k=config.HYBRID_K_PER_QUERY\n",
    "                )\n",
    "                for item in variant_results:\n",
    "                     doc = item.get('doc')\n",
    "                     doc_id = get_chunk_id(doc)\n",
    "                     if doc_id and doc_id not in collected_docs_data_by_id: collected_docs_data_by_id[doc_id] = doc\n",
    "            else: # 'Sâu'\n",
    "                for query_variant in all_queries:\n",
    "                     variant_results = hybrid_retriever.hybrid_search(\n",
    "                         query_variant, embedding_model,\n",
    "                         vector_search_k=config.VECTOR_K_PER_QUERY,\n",
    "                         final_k=config.HYBRID_K_PER_QUERY\n",
    "                     )\n",
    "                     for item in variant_results:\n",
    "                         doc = item.get('doc')\n",
    "                         doc_id = get_chunk_id(doc)\n",
    "                         if doc_id and doc_id not in collected_docs_data_by_id: collected_docs_data_by_id[doc_id] = doc\n",
    "\n",
    "            retrieved_ids = set(collected_docs_data_by_id.keys())\n",
    "            item_results[\"retrieved_ids\"] = list(retrieved_ids) # Lưu dạng list vào kết quả\n",
    "\n",
    "            # Tính retrieval metrics\n",
    "            retrieval_metrics = calculate_retrieval_metrics(retrieved_ids, ground_truth_ids)\n",
    "            item_results.update(retrieval_metrics) # Thêm các chỉ số vào results\n",
    "\n",
    "            # --- Bước C & D: Reranking ---\n",
    "            retrieved_chunks = list(collected_docs_data_by_id.values())\n",
    "            unique_docs_for_reranking_input_eval = []\n",
    "            final_relevant_documents_eval = []\n",
    "            reranked_docs_info_list = [] # Lưu thông tin rerank\n",
    "\n",
    "            if retrieved_chunks:\n",
    "                # Giới hạn số lượng rerank\n",
    "                docs_to_rerank = retrieved_chunks[:config.MAX_DOCS_FOR_RERANK]\n",
    "                # Input cho reranker cần là list các dict có key 'doc'\n",
    "                # Hàm rerank_documents cần trả về cả score và index gốc hoặc ID nếu có\n",
    "                reranker_input = [{'doc': doc} for doc in docs_to_rerank] # Cần đảm bảo rerank_documents xử lý input này\n",
    "\n",
    "                reranked_results_with_scores = utils.rerank_documents(\n",
    "                    summarizing_q, # Dùng summarizing_q để rerank\n",
    "                    reranker_input, # Chỉ truyền doc\n",
    "                    reranker_model\n",
    "                ) # Hàm này cần trả về list dict {'doc': ..., 'score': ...}\n",
    "\n",
    "                # Lấy ID và score sau rerank\n",
    "                reranked_docs_info_list = []\n",
    "                for rank, item in enumerate(reranked_results_with_scores):\n",
    "                     doc = item.get('doc')\n",
    "                     score = item.get('score')\n",
    "                     doc_id = get_chunk_id(doc)\n",
    "                     if doc_id:\n",
    "                         reranked_docs_info_list.append({'id': doc_id, 'rerank_score': score, 'rerank_pos': rank + 1})\n",
    "\n",
    "                final_relevant_documents_eval = reranked_results_with_scores[:config.FINAL_NUM_RESULTS_AFTER_RERANK]\n",
    "\n",
    "            item_results[\"reranked_ids_scores\"] = reranked_docs_info_list # Lưu list ID và score sau rerank\n",
    "            final_context_ids = {get_chunk_id(item['doc']) for item in final_relevant_documents_eval if get_chunk_id(item['doc'])}\n",
    "            item_results[\"final_context_ids\"] = list(final_context_ids)\n",
    "\n",
    "            # --- (Tùy chọn) Bước E: Generation ---\n",
    "            # if gemini_model:\n",
    "            #     generated_answer = utils.generate_answer_with_gemini(\n",
    "            #         query, final_relevant_documents_eval, gemini_model,\n",
    "            #         mode='Đầy đủ', # Hoặc mode khác\n",
    "            #         chat_history=None # Không dùng history trong eval batch\n",
    "            #     )\n",
    "            #     item_results[\"generated_answer\"] = generated_answer\n",
    "            # else:\n",
    "            #     item_results[\"generated_answer\"] = None\n",
    "\n",
    "            item_results[\"processing_time\"] = time.time() - start_time_item\n",
    "            logging.info(f\"Hoàn thành câu hỏi {i+1}. Thời gian: {item_results['processing_time']:.2f}s. Recall: {item_results['recall']:.2f}\")\n",
    "\n",
    "        except Exception as item_error:\n",
    "            logging.error(f\"Lỗi khi xử lý câu hỏi '{query_id}': {item_error}\", exc_info=True)\n",
    "            item_results[\"error\"] = str(item_error)\n",
    "            item_results[\"processing_time\"] = time.time() - start_time_item\n",
    "\n",
    "        all_results.append(item_results)\n",
    "\n",
    "    # --- 4. Tổng hợp và Báo cáo Kết quả ---\n",
    "    logging.info(\"--- Hoàn thành xử lý các câu hỏi ---\")\n",
    "    total_time = time.time() - start_time_total\n",
    "    logging.info(f\"Tổng thời gian xử lý: {total_time:.2f} giây\")\n",
    "    logging.info(f\"Thời gian trung bình mỗi câu hỏi: {total_time / len(evaluation_data):.2f} giây\")\n",
    "\n",
    "    # Tạo DataFrame từ kết quả\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "\n",
    "    # Tính toán các chỉ số trung bình\n",
    "    avg_precision = results_df['precision'].mean()\n",
    "    avg_recall = results_df['recall'].mean()\n",
    "    avg_f1 = results_df['f1'].mean()\n",
    "    # Có thể tính thêm các chỉ số khác (ví dụ: MRR từ reranked_ids_scores nếu cần)\n",
    "\n",
    "    logging.info(\"--- Kết quả Đánh giá Retrieval Trung bình ---\")\n",
    "    logging.info(f\"Precision (Trung bình): {avg_precision:.4f}\")\n",
    "    logging.info(f\"Recall (Trung bình):    {avg_recall:.4f}\")\n",
    "    logging.info(f\"F1-Score (Trung bình):  {avg_f1:.4f}\")\n",
    "\n",
    "    # --- 5. Lưu kết quả chi tiết ---\n",
    "    try:\n",
    "        results_df.to_csv(results_output_path, index=False, encoding='utf-8-sig')\n",
    "        logging.info(f\"Đã lưu kết quả chi tiết vào: {results_output_path}\")\n",
    "    except Exception as save_error:\n",
    "        logging.error(f\"Lỗi khi lưu kết quả vào tệp CSV: {save_error}\", exc_info=True)\n",
    "\n",
    "    logging.info(\"--- Quá trình đánh giá kết thúc ---\")\n",
    "\n",
    "\n",
    "# --- Entry point để chạy script ---\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Chạy đánh giá hàng loạt cho hệ thống RAG Luật GTĐB.\")\n",
    "    parser.add_argument(\"eval_data_path\", type=str, help=\"Đường dẫn đến tệp JSON chứa dữ liệu đánh giá (query, relevant_chunk_ids).\")\n",
    "    parser.add_argument(\"-o\", \"--output\", type=str, default=\"evaluation_results.csv\", help=\"Đường dẫn để lưu kết quả đánh giá chi tiết (CSV). Mặc định: evaluation_results.csv\")\n",
    "    parser.add_argument(\"-r\", \"--retrieval_mode\", type=str, default=\"Sâu\", choices=['Đơn giản', 'Sâu'], help=\"Chế độ truy vấn để đánh giá ('Đơn giản' hoặc 'Sâu'). Mặc định: Sâu\")\n",
    "    # Thêm các argument khác nếu muốn tùy chỉnh config khi chạy (ví dụ: model name, K values...)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    run_evaluation(args.eval_data_path, args.output, args.retrieval_mode)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
